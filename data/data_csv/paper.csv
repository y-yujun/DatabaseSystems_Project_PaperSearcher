cs/9308101v1,"Dynamic Backtracking","Because of their occasional need to return to shallow points in a search tree, existing backtracking methods can sometimes erase meaningful progress toward solving a search problem. In this paper, we present a method by which backtrack points can be moved deeper in the search space, thereby avoiding this difficulty. The technique developed is a variant of dependency-directed backtracking that uses only polynomial space while still providing useful control information and retaining the completeness guarantees provided by earlier approaches.",1993-08-01T00:00:00Z,http://arxiv.org/pdf/cs/9308101v1,2024-04-28,
cs/9308102v1,"A Market-Oriented Programming Environment and its Application to Distributed Multicommodity Flow Problems","Market price systems constitute a well-understood class of mechanisms that under certain conditions provide effective decentralization of decision making with minimal communication overhead. In a market-oriented programming approach to distributed problem solving, we derive the activities and resource allocations for a set of computational agents by computing the competitive equilibrium of an artificial economy. WALRAS provides basic constructs for defining computational market structures, and protocols for deriving their corresponding price equilibria. In a particular realization of this approach for a form of multicommodity flow problem, we see that careful construction of the decision process according to economic principles can lead to efficient distributed resource allocation, and that the behavior of the system can be meaningfully analyzed in economic terms.",1993-08-01T00:00:00Z,http://arxiv.org/pdf/cs/9308102v1,2024-04-28,
cs/9309101v1,"An Empirical Analysis of Search in GSAT","We describe an extensive study of search in GSAT, an approximation procedure for propositional satisfiability. GSAT performs greedy hill-climbing on the number of satisfied clauses in a truth assignment. Our experiments provide a more complete picture of GSAT's search than previous accounts. We describe in detail the two phases of search: rapid hill-climbing followed by a long plateau search. We demonstrate that when applied to randomly generated 3SAT problems, there is a very simple scaling with problem size for both the mean number of satisfied clauses and the mean branching rate. Our results allow us to make detailed numerical conjectures about the length of the hill-climbing phase, the average gradient of this phase, and to conjecture that both the average score and average branching rate decay exponentially during plateau search. We end by showing how these results can be used to direct future theoretical analysis. This work provides a case study of how computer experiments can be used to improve understanding of the theoretical properties of algorithms.",1993-09-01T00:00:00Z,http://arxiv.org/pdf/cs/9309101v1,2024-04-28,,
cs/9311101v1,"The Difficulties of Learning Logic Programs with Cut","As real logic programmers normally use cut (!), an effective learning procedure for logic programs should be able to deal with it. Because the cut predicate has only a procedural meaning, clauses containing cut cannot be learned using an extensional evaluation method, as is done in most learning systems. On the other hand, searching a space of possible programs (instead of a space of independent clauses) is unfeasible. An alternative solution is to generate first a candidate base program which covers the positive examples, and then make it consistent by inserting cut where appropriate. The problem of learning programs with cut has not been investigated before and this seems to be a natural and reasonable approach. We generalize this scheme and investigate the difficulties that arise. Some of the major shortcomings are actually caused, in general, by the need for intensional evaluation. As a conclusion, the analysis of this paper suggests, on precise and technical grounds, that learning cut is difficult, and current induction techniques should probably be restricted to purely declarative logic languages.",1993-11-01T00:00:00Z,http://arxiv.org/pdf/cs/9311101v1,2024-04-28,
cs/9311102v1,"Software Agents: Completing Patterns and Constructing User Interfaces","To support the goal of allowing users to record and retrieve information, this paper describes an interactive note-taking system for pen-based computers with two distinctive features. First, it actively predicts what the user is going to write. Second, it automatically constructs a custom, button-box user interface on request. The system is an example of a learning-apprentice software- agent. A machine learning component characterizes the syntax and semantics of the user's information. A performance system uses this learned information to generate completion strings and construct a user interface. Description of Online Appendix: People like to record information. Doing this on paper is initially efficient, but lacks flexibility. Recording information on a computer is less efficient but more powerful. In our new note taking softwre, the user records information directly on a computer. Behind the interface, an agent acts for the user. To help, it provides defaults and constructs a custom user interface. The demonstration is a QuickTime movie of the note taking agent in action. The file is a binhexed self-extracting archive. Macintosh utilities for binhex are available from mac.archive.umich.edu. QuickTime is available from ftp.apple.com in the dts/mac/sys.soft/quicktime.",1993-11-01T00:00:00Z,http://arxiv.org/pdf/cs/9311102v1,2024-04-28,
cs/9312101v1,"Decidable Reasoning in Terminological Knowledge Representation Systems","Terminological knowledge representation systems (TKRSs) are tools for designing and using knowledge bases that make use of terminological languages (or concept languages). We analyze from a theoretical point of view a TKRS whose capabilities go beyond the ones of presently available TKRSs. The new features studied, often required in practical applications, can be summarized in three main points. First, we consider a highly expressive terminological language, called ALCNR, including general complements of concepts, number restrictions and role conjunction. Second, we allow to express inclusion statements between general concepts, and terminological cycles as a particular case. Third, we prove the decidability of a number of desirable TKRS-deduction services (like satisfiability, subsumption and instance checking) through a sound, complete and terminating calculus for reasoning in ALCNR-knowledge bases. Our calculus extends the general technique of constraint systems. As a byproduct of the proof, we get also the result that inclusion statements in ALCNR can be simulated by terminological cycles, if descriptive semantics is adopted.",1993-12-01T00:00:00Z,http://arxiv.org/pdf/cs/9312101v1,2024-04-28,
cs/9401101v1,"Teleo-Reactive Programs for Agent Control","A formalism is presented for computing and organizing actions for autonomous agents in dynamic environments. We introduce the notion of teleo-reactive (T-R) programs whose execution entails the construction of circuitry for the continuous computation of the parameters and conditions on which agent action is based. In addition to continuous feedback, T-R programs support parameter binding and recursion. A primary difference between T-R programs and many other circuit-based systems is that the circuitry of T-R programs is more compact; it is constructed at run time and thus does not have to anticipate all the contingencies that might arise over all possible runs. In addition, T-R programs are intuitive and easy to write and are written in a form that is compatible with automatic planning and learning methods. We briefly describe some experimental applications of T-R programs in the control of simulated and actual mobile robots.",1994-01-01T00:00:00Z,http://arxiv.org/pdf/cs/9401101v1,2024-04-28,
cs/9402101v1,"Learning the Past Tense of English Verbs: The Symbolic Pattern Associator vs. Connectionist Models","Learning the past tense of English verbs - a seemingly minor aspect of language acquisition - has generated heated debates since 1986, and has become a landmark task for testing the adequacy of cognitive modeling. Several artificial neural networks (ANNs) have been implemented, and a challenge for better symbolic models has been posed. In this paper, we present a general-purpose Symbolic Pattern Associator (SPA) based upon the decision-tree learning algorithm ID3. We conduct extensive head-to-head comparisons on the generalization ability between ANN models and the SPA under different representations. We conclude that the SPA generalizes the past tense of unseen verbs better than ANN models by a wide margin, and we offer insights as to why this should be the case. We also discuss a new default strategy for decision-tree learning algorithms.",1994-02-01T00:00:00Z,http://arxiv.org/pdf/cs/9402101v1,2024-04-28,
cs/9402102v1,"Substructure Discovery Using Minimum Description Length and Background Knowledge","The ability to identify interesting and repetitive substructures is an essential component to discovering knowledge in structural data. We describe a new version of our SUBDUE substructure discovery system based on the minimum description length principle. The SUBDUE system discovers substructures that compress the original data and represent structural concepts in the data. By replacing previously-discovered substructures in the data, multiple passes of SUBDUE produce a hierarchical description of the structural regularities in the data. SUBDUE uses a computationally-bounded inexact graph match that identifies similar, but not identical, instances of a substructure and finds an approximate measure of closeness of two substructures when under computational constraints. In addition to the minimum description length principle, other background knowledge can be used by SUBDUE to guide the search towards more appropriate substructures. Experiments in a variety of domains demonstrate SUBDUE's ability to find substructures capable of compressing the original data and to discover structural concepts important to the domain. Description of Online Appendix: This is a compressed tar file containing the SUBDUE discovery system, written in C. The program accepts as input databases represented in graph form, and will output discovered substructures with their corresponding value.",1994-02-01T00:00:00Z,http://arxiv.org/pdf/cs/9402102v1,2024-04-28,
cs/9402103v1,"Bias-Driven Revision of Logical Domain Theories","The theory revision problem is the problem of how best to go about revising a deficient domain theory using information contained in examples that expose inaccuracies. In this paper we present our approach to the theory revision problem for propositional domain theories. The approach described here, called PTR, uses probabilities associated with domain theory elements to numerically track the ``flow'' of proof through the theory. This allows us to measure the precise role of a clause or literal in allowing or preventing a (desired or undesired) derivation for a given example. This information is used to efficiently locate and repair flawed elements of the theory. PTR is proved to converge to a theory which correctly classifies all examples, and shown experimentally to be fast and accurate even for deep theories.",1994-02-01T00:00:00Z,http://arxiv.org/pdf/cs/9402103v1,2024-04-28,
cs/9403101v1,"Exploring the Decision Forest: An Empirical Investigation of Occam's Razor in Decision Tree Induction","We report on a series of experiments in which all decision trees consistent with the training data are constructed. These experiments were run to gain an understanding of the properties of the set of consistent decision trees and the factors that affect the accuracy of individual trees. In particular, we investigated the relationship between the size of a decision tree consistent with some training data and the accuracy of the tree on test data. The experiments were performed on a massively parallel Maspar computer. The results of the experiments on several artificial and two real world problems indicate that, for many of the problems investigated, smaller consistent decision trees are on average less accurate than the average accuracy of slightly larger trees.",1994-03-01T00:00:00Z,http://arxiv.org/pdf/cs/9403101v1,2024-04-28,
cs/9406101v1,"A Semantics and Complete Algorithm for Subsumption in the CLASSIC Description Logic","This paper analyzes the correctness of the subsumption algorithm used in CLASSIC, a description logic-based knowledge representation system that is being used in practical applications. In order to deal efficiently with individuals in CLASSIC descriptions, the developers have had to use an algorithm that is incomplete with respect to the standard, model-theoretic semantics for description logics. We provide a variant semantics for descriptions with respect to which the current implementation is complete, and which can be independently motivated. The soundness and completeness of the polynomial-time subsumption algorithm is established using description graphs, which are an abstracted version of the implementation structures used in CLASSIC, and are of independent interest.",1994-06-01T00:00:00Z,http://arxiv.org/pdf/cs/9406101v1,2024-04-28,
cs/9406102v1,"Applying GSAT to Non-Clausal Formulas","In this paper we describe how to modify GSAT so that it can be applied to non-clausal formulas. The idea is to use a particular ``score'' function which gives the number of clauses of the CNF conversion of a formula which are false under a given truth assignment. Its value is computed in linear time, without constructing the CNF conversion itself. The proposed methodology applies to most of the variants of GSAT proposed so far.",1994-06-01T00:00:00Z,http://arxiv.org/pdf/cs/9406102v1,2024-04-28,
cs/9408101v1,"Random Worlds and Maximum Entropy","Given a knowledge base KB containing first-order and statistical facts, we consider a principled method, called the random-worlds method, for computing a degree of belief that some formula Phi holds given KB. If we are reasoning about a world or system consisting of N individuals, then we can consider all possible worlds, or first-order models, with domain {1,...,N} that satisfy KB, and compute the fraction of them in which Phi is true. We define the degree of belief to be the asymptotic value of this fraction as N grows large. We show that when the vocabulary underlying Phi and KB uses constants and unary predicates only, we can naturally associate an entropy with each world. As N grows larger, there are many more worlds with higher entropy. Therefore, we can use a maximum-entropy computation to compute the degree of belief. This result is in a similar spirit to previous work in physics and artificial intelligence, but is far more general. Of equal interest to the result itself are the limitations on its scope. Most importantly, the restriction to unary predicates seems necessary. Although the random-worlds method makes sense in general, the connection to maximum entropy seems to disappear in the non-unary case. These observations suggest unexpected limitations to the applicability of maximum-entropy methods.",1994-08-01T00:00:00Z,http://arxiv.org/pdf/cs/9408101v1,2024-04-28,
cs/9408102v1,"Pattern Matching and Discourse Processing in Information Extraction from Japanese Text","Information extraction is the task of automatically picking up information of interest from an unconstrained text. Information of interest is usually extracted in two steps. First, sentence level processing locates relevant pieces of information scattered throughout the text; second, discourse processing merges coreferential information to generate the output. In the first step, pieces of information are locally identified without recognizing any relationships among them. A key word search or simple pattern search can achieve this purpose. The second step requires deeper knowledge in order to understand relationships among separately identified pieces of information. Previous information extraction systems focused on the first step, partly because they were not required to link up each piece of information with other pieces. To link the extracted pieces of information and map them onto a structured output format, complex discourse processing is essential. This paper reports on a Japanese information extraction system that merges information using a pattern matcher and discourse processor. Evaluation results show a high level of system performance which approaches human performance.",1994-08-01T00:00:00Z,http://arxiv.org/pdf/cs/9408102v1,2024-04-28,
cs/9408103v1,"A System for Induction of Oblique Decision Trees","This article describes a new system for induction of oblique decision trees. This system, OC1, combines deterministic hill-climbing with two forms of randomization to find a good oblique split (in the form of a hyperplane) at each node of a decision tree. Oblique decision tree methods are tuned especially for domains in which the attributes are numeric, although they can be adapted to symbolic or mixed symbolic/numeric attributes. We present extensive empirical studies, using both real and artificial data, that analyze OC1's ability to construct oblique trees that are smaller and more accurate than their axis-parallel counterparts. We also examine the benefits of randomization for the construction of oblique decision trees.",1994-08-01T00:00:00Z,http://arxiv.org/pdf/cs/9408103v1,2024-04-28,
cs/9409101v1,"On Planning while Learning","This paper introduces a framework for Planning while Learning where an agent is given a goal to achieve in an environment whose behavior is only partially known to the agent. We discuss the tractability of various plan-design processes. We show that for a large natural class of Planning while Learning systems, a plan can be presented and verified in a reasonable time. However, coming up algorithmically with a plan, even for simple classes of systems is apparently intractable. We emphasize the role of off-line plan-design processes, and show that, in most natural cases, the verification (projection) part can be carried out in an efficient algorithmic manner.",1994-09-01T00:00:00Z,http://arxiv.org/pdf/cs/9409101v1,2024-04-28,
cs/9412101v1,"Wrap-Up: a Trainable Discourse Module for Information Extraction","The vast amounts of on-line text now available have led to renewed interest in information extraction (IE) systems that analyze unrestricted text, producing a structured representation of selected information from the text. This paper presents a novel approach that uses machine learning to acquire knowledge for some of the higher level IE processing. Wrap-Up is a trainable IE discourse component that makes intersentential inferences and identifies logical relations among information extracted from the text. Previous corpus-based approaches were limited to lower level processing such as part-of-speech tagging, lexical disambiguation, and dictionary construction. Wrap-Up is fully trainable, and not only automatically decides what classifiers are needed, but even derives the feature set for each classifier automatically. Performance equals that of a partially trainable discourse module requiring manual customization for each domain.",1994-12-01T00:00:00Z,http://arxiv.org/pdf/cs/9412101v1,2024-04-28,
cs/9412102v1,"Operations for Learning with Graphical Models","This paper is a multidisciplinary review of empirical, statistical learning from a graphical model perspective. Well-known examples of graphical models include Bayesian networks, directed graphs representing a Markov chain, and undirected networks representing a Markov field. These graphical models are extended to model data analysis and empirical learning using the notation of plates. Graphical operations for simplifying and manipulating a problem are provided including decomposition, differentiation, and the manipulation of probability models from the exponential family. Two standard algorithm schemas for learning are reviewed in a graphical framework: Gibbs sampling and the expectation maximization algorithm. Using these operations and schemas, some popular algorithms can be synthesized from their graphical specification. This includes versions of linear regression, techniques for feed-forward networks, and learning Gaussian and discrete Bayesian networks from data. The paper concludes by sketching some implications for data analysis and summarizing how some popular algorithms fall within the framework presented. The main original contributions here are the decomposition techniques and the demonstration that graphical models provide a framework for understanding and developing complex learning algorithms.",1994-12-01T00:00:00Z,http://arxiv.org/pdf/cs/9412102v1,2024-04-28,
cs/9412103v1,"Total-Order and Partial-Order Planning: A Comparative Analysis","For many years, the intuitions underlying partial-order planning were largely taken for granted. Only in the past few years has there been renewed interest in the fundamental principles underlying this paradigm. In this paper, we present a rigorous comparative analysis of partial-order and total-order planning by focusing on two specific planners that can be directly compared. We show that there are some subtle assumptions that underly the wide-spread intuitions regarding the supposed efficiency of partial-order planning. For instance, the superiority of partial-order planning can depend critically upon the search strategy and the structure of the search space. Understanding the underlying assumptions is crucial for constructing efficient planners.",1994-12-01T00:00:00Z,http://arxiv.org/pdf/cs/9412103v1,2024-04-28,
cs/9501101v1,"Solving Multiclass Learning Problems via Error-Correcting Output Codes","Multiclass learning problems involve finding a definition for an unknown function f(x) whose range is a discrete set containing k &gt 2 values (i.e., k ``classes''). The definition is acquired by studying collections of training examples of the form [x_i, f (x_i)]. Existing approaches to multiclass learning problems include direct application of multiclass algorithms such as the decision-tree algorithms C4.5 and CART, application of binary concept learning algorithms to learn individual binary functions for each of the k classes, and application of binary concept learning algorithms with distributed output representations. This paper compares these three approaches to a new technique in which error-correcting codes are employed as a distributed output representation. We show that these output representations improve the generalization performance of both C4.5 and backpropagation on a wide range of multiclass learning tasks. We also demonstrate that this approach is robust with respect to changes in the size of the training sample, the assignment of distributed representations to particular classes, and the application of overfitting avoidance techniques such as decision-tree pruning. Finally, we show that---like the other methods---the error-correcting code technique can provide reliable class probability estimates. Taken together, these results demonstrate that error-correcting output codes provide a general-purpose method for improving the performance of inductive learning programs on multiclass problems.",1995-01-01T00:00:00Z,http://arxiv.org/pdf/cs/9501101v1,2024-04-28,
cs/9501102v1,"A Domain-Independent Algorithm for Plan Adaptation","The paradigms of transformational planning, case-based planning, and plan debugging all involve a process known as plan adaptation - modifying or repairing an old plan so it solves a new problem. In this paper we provide a domain-independent algorithm for plan adaptation, demonstrate that it is sound, complete, and systematic, and compare it to other adaptation algorithms in the literature. Our approach is based on a view of planning as searching a graph of partial plans. Generative planning starts at the graph's root and moves from node to node using plan-refinement operators. In planning by adaptation, a library plan - an arbitrary node in the plan graph - is the starting point for the search, and the plan-adaptation algorithm can apply both the same refinement operators available to a generative planner and can also retract constraints and steps from the plan. Our algorithm's completeness ensures that the adaptation algorithm will eventually search the entire graph and its systematicity ensures that it will do so without redundantly searching any parts of the graph.",1995-01-01T00:00:00Z,http://arxiv.org/pdf/cs/9501102v1,2024-04-28,
cs/9501103v1,"Truncating Temporal Differences: On the Efficient Implementation of TD(lambda) for Reinforcement Learning","Temporal difference (TD) methods constitute a class of methods for learning predictions in multi-step prediction problems, parameterized by a recency factor lambda. Currently the most important application of these methods is to temporal credit assignment in reinforcement learning. Well known reinforcement learning algorithms, such as AHC or Q-learning, may be viewed as instances of TD learning. This paper examines the issues of the efficient and general implementation of TD(lambda) for arbitrary lambda, for use with reinforcement learning algorithms optimizing the discounted sum of rewards. The traditional approach, based on eligibility traces, is argued to suffer from both inefficiency and lack of generality. The TTD (Truncated Temporal Differences) procedure is proposed as an alternative, that indeed only approximates TD(lambda), but requires very little computation per action and can be used with arbitrary function representation methods. The idea from which it is derived is fairly simple and not new, but probably unexplored so far. Encouraging experimental results are presented, suggesting that using lambda &gt 0 with the TTD procedure allows one to obtain a significant learning speedup at essentially the same cost as usual TD(0) learning.",1995-01-01T00:00:00Z,http://arxiv.org/pdf/cs/9501103v1,2024-04-28,
cs/9503102v1,"Cost-Sensitive Classification: Empirical Evaluation of a Hybrid Genetic Decision Tree Induction Algorithm","This paper introduces ICET, a new algorithm for cost-sensitive classification. ICET uses a genetic algorithm to evolve a population of biases for a decision tree induction algorithm. The fitness function of the genetic algorithm is the average cost of classification when using the decision tree, including both the costs of tests (features, measurements) and the costs of classification errors. ICET is compared here with three other algorithms for cost-sensitive classification - EG2, CS-ID3, and IDX - and also with C4.5, which classifies without regard to cost. The five algorithms are evaluated empirically on five real-world medical datasets. Three sets of experiments are performed. The first set examines the baseline performance of the five algorithms on the five datasets and establishes that ICET performs significantly better than its competitors. The second set tests the robustness of ICET under a variety of conditions and shows that ICET maintains its advantage. The third set looks at ICET's search in bias space and discovers a way to improve the search.",1995-03-01T00:00:00Z,http://arxiv.org/pdf/cs/9503102v1,2024-04-28,
cs/9504101v1,"Rerepresenting and Restructuring Domain Theories: A Constructive Induction Approach","Theory revision integrates inductive learning and background knowledge by combining training examples with a coarse domain theory to produce a more accurate theory. There are two challenges that theory revision and other theory-guided systems face. First, a representation language appropriate for the initial theory may be inappropriate for an improved theory. While the original representation may concisely express the initial theory, a more accurate theory forced to use that same representation may be bulky, cumbersome, and difficult to reach. Second, a theory structure suitable for a coarse domain theory may be insufficient for a fine-tuned theory. Systems that produce only small, local changes to a theory have limited value for accomplishing complex structural alterations that may be required. Consequently, advanced theory-guided learning systems require flexible representation and flexible structure. An analysis of various theory revision systems and theory-guided learning systems reveals specific strengths and weaknesses in terms of these two desired properties. Designed to capture the underlying qualities of each system, a new system uses theory-guided constructive induction. Experiments in three domains show improvement over previous theory-guided systems. This leads to a study of the behavior, limitations, and potential of theory-guided constructive induction.",1995-04-01T00:00:00Z,http://arxiv.org/pdf/cs/9504101v1,2024-04-28,
cs/9505101v1,"Using Pivot Consistency to Decompose and Solve Functional CSPs","Many studies have been carried out in order to increase the search efficiency of constraint satisfaction problems; among them, some make use of structural properties of the constraint network; others take into account semantic properties of the constraints, generally assuming that all the constraints possess the given property. In this paper, we propose a new decomposition method benefiting from both semantic properties of functional constraints (not bijective constraints) and structural properties of the network; furthermore, not all the constraints need to be functional. We show that under some conditions, the existence of solutions can be guaranteed. We first characterize a particular subset of the variables, which we name a root set. We then introduce pivot consistency, a new local consistency which is a weak form of path consistency and can be achieved in O(n^2d^2) complexity (instead of O(n^3d^3) for path consistency), and we present associated properties; in particular, we show that any consistent instantiation of the root set can be linearly extended to a solution, which leads to the presentation of the aforementioned new method for solving by decomposing functional CSPs.",1995-05-01T00:00:00Z,http://arxiv.org/pdf/cs/9505101v1,2024-04-28,
cs/9505102v1,"Adaptive Load Balancing: A Study in Multi-Agent Learning","We study the process of multi-agent reinforcement learning in the context of load balancing in a distributed system, without use of either central coordination or explicit communication. We first define a precise framework in which to study adaptive load balancing, important features of which are its stochastic nature and the purely local information available to individual agents. Given this framework, we show illuminating results on the interplay between basic adaptive behavior parameters and their effect on system efficiency. We then investigate the properties of adaptive load balancing in heterogeneous populations, and address the issue of exploration vs. exploitation in that context. Finally, we show that naive use of communication may not improve, and might even harm system efficiency.",1995-05-01T00:00:00Z,http://arxiv.org/pdf/cs/9505102v1,2024-04-28,
cs/9505103v1,"Provably Bounded-Optimal Agents","Since its inception, artificial intelligence has relied upon a theoretical foundation centered around perfect rationality as the desired property of intelligent systems. We argue, as others have done, that this foundation is inadequate because it imposes fundamentally unsatisfiable requirements. As a result, there has arisen a wide gap between theory and practice in AI, hindering progress in the field. We propose instead a property called bounded optimality. Roughly speaking, an agent is bounded-optimal if its program is a solution to the constrained optimization problem presented by its architecture and the task environment. We show how to construct agents with this property for a simple class of machine architectures in a broad class of real-time environments. We illustrate these results using a simple model of an automated mail sorting facility. We also define a weaker property, asymptotic bounded optimality (ABO), that generalizes the notion of optimality in classical complexity theory. We then construct universal ABO programs, i.e., programs that are ABO no matter what real-time constraints are applied. Universal ABO programs can be used as building blocks for more complex systems. We conclude with a discussion of the prospects for bounded optimality as a theoretical basis for AI, and relate it to similar trends in philosophy, economics, and game theory.",1995-05-01T00:00:00Z,http://arxiv.org/pdf/cs/9505103v1,2024-04-28,
cs/9505104v1,"Pac-Learning Recursive Logic Programs: Efficient Algorithms","We present algorithms that learn certain classes of function-free recursive logic programs in polynomial time from equivalence queries. In particular, we show that a single k-ary recursive constant-depth determinate clause is learnable. Two-clause programs consisting of one learnable recursive clause and one constant-depth determinate non-recursive clause are also learnable, if an additional ``basecase'' oracle is assumed. These results immediately imply the pac-learnability of these classes. Although these classes of learnable recursive programs are very constrained, it is shown in a companion paper that they are maximally general, in that generalizing either class in any natural way leads to a computationally difficult learning problem. Thus, taken together with its companion paper, this paper establishes a boundary of efficient learnability for recursive logic programs.",1995-05-01T00:00:00Z,http://arxiv.org/pdf/cs/9505104v1,2024-04-28,
cs/9505105v1,"Pac-learning Recursive Logic Programs: Negative Results","In a companion paper it was shown that the class of constant-depth determinate k-ary recursive clauses is efficiently learnable. In this paper we present negative results showing that any natural generalization of this class is hard to learn in Valiant's model of pac-learnability. In particular, we show that the following program classes are cryptographically hard to learn: programs with an unbounded number of constant-depth linear recursive clauses; programs with one constant-depth determinate clause containing an unbounded number of recursive calls; and programs with one linear recursive clause of constant locality. These results immediately imply the non-learnability of any more general class of programs. We also show that learning a constant-depth determinate program with either two linear recursive clauses or one linear recursive clause and one non-recursive clause is as hard as learning boolean DNF. Together with positive results from the companion paper, these negative results establish a boundary of efficient learnability for recursive function-free clauses.",1995-05-01T00:00:00Z,http://arxiv.org/pdf/cs/9505105v1,2024-04-28,
cs/9506101v1,"FLECS: Planning with a Flexible Commitment Strategy","There has been evidence that least-commitment planners can efficiently handle planning problems that involve difficult goal interactions. This evidence has led to the common belief that delayed-commitment is the ""best"" possible planning strategy. However, we recently found evidence that eager-commitment planners can handle a variety of planning problems more efficiently, in particular those with difficult operator choices. Resigned to the futility of trying to find a universally successful planning strategy, we devised a planner that can be used to study which domains and problems are best for which planning strategies. In this article we introduce this new planning algorithm, FLECS, which uses a FLExible Commitment Strategy with respect to plan-step orderings. It is able to use any strategy from delayed-commitment to eager-commitment. The combination of delayed and eager operator-ordering commitments allows FLECS to take advantage of the benefits of explicitly using a simulated execution state and reasoning about planning constraints. FLECS can vary its commitment strategy across different problems and domains, and also during the course of a single planning problem. FLECS represents a novel contribution to planning in that it explicitly provides the choice of which commitment strategy to use while planning. FLECS provides a framework to investigate the mapping from planning domains and problems to efficient planning strategies.",1995-06-01T00:00:00Z,http://arxiv.org/pdf/cs/9506101v1,2024-04-28,
cs/9506102v1,"Induction of First-Order Decision Lists: Results on Learning the Past Tense of English Verbs","This paper presents a method for inducing logic programs from examples that learns a new class of concepts called first-order decision lists, defined as ordered lists of clauses each ending in a cut. The method, called FOIDL, is based on FOIL (Quinlan, 1990) but employs intensional background knowledge and avoids the need for explicit negative examples. It is particularly useful for problems that involve rules with specific exceptions, such as learning the past-tense of English verbs, a task widely studied in the context of the symbolic/connectionist debate. FOIDL is able to learn concise, accurate programs for this problem from significantly fewer examples than previous methods (both connectionist and symbolic).",1995-06-01T00:00:00Z,http://arxiv.org/pdf/cs/9506102v1,2024-04-28,
cs/9507101v1,"Building and Refining Abstract Planning Cases by Change of Representation Language","ion is one of the most promising approaches to improve the performance of problem solvers. In several domains abstraction by dropping sentences of a domain description -- as used in most hierarchical planners -- has proven useful. In this paper we present examples which illustrate significant drawbacks of abstraction by dropping sentences. To overcome these drawbacks, we propose a more general view of abstraction involving the change of representation language. We have developed a new abstraction methodology and a related sound and complete learning algorithm that allows the complete change of representation language of planning cases from concrete to abstract. However, to achieve a powerful change of the representation language, the abstract language itself as well as rules which describe admissible ways of abstracting states must be provided in the domain model. This new abstraction approach is the core of Paris (Plan Abstraction and Refinement in an Integrated System), a system in which abstract planning cases are automatically learned from given concrete cases. An empirical study in the domain of process planning in mechanical engineering shows significant advantages of the proposed reasoning from abstract cases over classical hierarchical planning.",1995-07-01T00:00:00Z,http://arxiv.org/pdf/cs/9507101v1,2024-04-28,
cs/9508101v1,"Using Qualitative Hypotheses to Identify Inaccurate Data","Identifying inaccurate data has long been regarded as a significant and difficult problem in AI. In this paper, we present a new method for identifying inaccurate data on the basis of qualitative correlations among related data. First, we introduce the definitions of related data and qualitative correlations among related data. Then we put forward a new concept called support coefficient function (SCF). SCF can be used to extract, represent, and calculate qualitative correlations among related data within a dataset. We propose an approach to determining dynamic shift intervals of inaccurate data, and an approach to calculating possibility of identifying inaccurate data, respectively. Both of the approaches are based on SCF. Finally we present an algorithm for identifying inaccurate data by using qualitative correlations among related data as confirmatory or disconfirmatory evidence. We have developed a practical system for interpreting infrared spectra by applying the method, and have fully tested the system against several hundred real spectra. The experimental results show that the method is significantly better than the conventional methods used in many similar systems.",1995-08-01T00:00:00Z,http://arxiv.org/pdf/cs/9508101v1,2024-04-28,
cs/9508102v1,"An Integrated Framework for Learning and Reasoning","Learning and reasoning are both aspects of what is considered to be intelligence. Their studies within AI have been separated historically, learning being the topic of machine learning and neural networks, and reasoning falling under classical (or symbolic) AI. However, learning and reasoning are in many ways interdependent. This paper discusses the nature of some of these interdependencies and proposes a general framework called FLARE, that combines inductive learning using prior knowledge together with reasoning in a propositional setting. Several examples that test the framework are presented, including classical induction, many important reasoning protocols and two simple expert systems.",1995-08-01T00:00:00Z,http://arxiv.org/pdf/cs/9508102v1,2024-04-28,
cs/9510101v1,"Diffusion of Context and Credit Information in Markovian Models","This paper studies the problem of ergodicity of transition probability matrices in Markovian models, such as hidden Markov models (HMMs), and how it makes very difficult the task of learning to represent long-term context for sequential data. This phenomenon hurts the forward propagation of long-term context information, as well as learning a hidden state representation to represent long-term context, which depends on propagating credit information backwards in time. Using results from Markov chain theory, we show that this problem of diffusion of context and credit is reduced when the transition probabilities approach 0 or 1, i.e., the transition probability matrices are sparse and the model essentially deterministic. The results found in this paper apply to learning approaches based on continuous optimization, such as gradient descent and the Baum-Welch algorithm.",1995-10-01T00:00:00Z,http://arxiv.org/pdf/cs/9510101v1,2024-04-28,
cs/9510102v1,"Improving Connectionist Energy Minimization","Symmetric networks designed for energy minimization such as Boltzman machines and Hopfield nets are frequently investigated for use in optimization, constraint satisfaction and approximation of NP-hard problems. Nevertheless, finding a global solution (i.e., a global minimum for the energy function) is not guaranteed and even a local solution may take an exponential number of steps. We propose an improvement to the standard local activation function used for such networks. The improved algorithm guarantees that a global minimum is found in linear time for tree-like subnetworks. The algorithm, called activate, is uniform and does not assume that the network is tree-like. It can identify tree-like subnetworks even in cyclic topologies (arbitrary networks) and avoid local minima along these trees. For acyclic networks, the algorithm is guaranteed to converge to a global minimum from any initial state of the system (self-stabilization) and remains correct under various types of schedulers. On the negative side, we show that in the presence of cycles, no uniform algorithm exists that guarantees optimality even under a sequential asynchronous scheduler. An asynchronous scheduler can activate only one unit at a time while a synchronous scheduler can activate any number of units in a single time step. In addition, no uniform algorithm exists to optimize even acyclic networks when the scheduler is synchronous. Finally, we show how the algorithm can be improved using the cycle-cutset scheme. The general algorithm, called activate-with-cutset, improves over activate and has some performance guarantees that are related to the size of the network's cycle-cutset.",1995-10-01T00:00:00Z,http://arxiv.org/pdf/cs/9510102v1,2024-04-28,
cs/9510103v1,"Learning Membership Functions in a Function-Based Object Recognition System","Functionality-based recognition systems recognize objects at the category level by reasoning about how well the objects support the expected function. Such systems naturally associate a ``measure of goodness'' or ``membership value'' with a recognized object. This measure of goodness is the result of combining individual measures, or membership values, from potentially many primitive evaluations of different properties of the object's shape. A membership function is used to compute the membership value when evaluating a primitive of a particular physical property of an object. In previous versions of a recognition system known as Gruff, the membership function for each of the primitive evaluations was hand-crafted by the system designer. In this paper, we provide a learning component for the Gruff system, called Omlet, that automatically learns membership functions given a set of example objects labeled with their desired category measure. The learning algorithm is generally applicable to any problem in which low-level membership values are combined through an and-or tree structure to give a final overall membership value.",1995-10-01T00:00:00Z,http://arxiv.org/pdf/cs/9510103v1,2024-04-28,
cs/9511101v1,"Flexibly Instructable Agents","This paper presents an approach to learning from situated, interactive tutorial instruction within an ongoing agent. Tutorial instruction is a flexible (and thus powerful) paradigm for teaching tasks because it allows an instructor to communicate whatever types of knowledge an agent might need in whatever situations might arise. To support this flexibility, however, the agent must be able to learn multiple kinds of knowledge from a broad range of instructional interactions. Our approach, called situated explanation, achieves such learning through a combination of analytic and inductive techniques. It combines a form of explanation-based learning that is situated for each instruction with a full suite of contextually guided responses to incomplete explanations. The approach is implemented in an agent called Instructo-Soar that learns hierarchies of new tasks and other domain knowledge from interactive natural language instructions. Instructo-Soar meets three key requirements of flexible instructability that distinguish it from previous systems: (1) it can take known or unknown commands at any instruction point; (2) it can handle instructions that apply to either its current situation or to a hypothetical situation specified in language (as in, for instance, conditional instructions); and (3) it can learn, from instructions, each class of knowledge it uses to perform tasks.",1995-11-01T00:00:00Z,http://arxiv.org/pdf/cs/9511101v1,2024-04-28,
cs/9512101v1,"OPUS: An Efficient Admissible Algorithm for Unordered Search","OPUS is a branch and bound search algorithm that enables efficient admissible search through spaces for which the order of search operator application is not significant. The algorithm's search efficiency is demonstrated with respect to very large machine learning search spaces. The use of admissible search is of potential value to the machine learning community as it means that the exact learning biases to be employed for complex learning tasks can be precisely specified and manipulated. OPUS also has potential for application in other areas of artificial intelligence, notably, truth maintenance.",1995-12-01T00:00:00Z,http://arxiv.org/pdf/cs/9512101v1,2024-04-28,
cs/9512102v1,"Vision-Based Road Detection in Automotive Systems: A Real-Time Expectation-Driven Approach","The main aim of this work is the development of a vision-based road detection system fast enough to cope with the difficult real-time constraints imposed by moving vehicle applications. The hardware platform, a special-purpose massively parallel system, has been chosen to minimize system production and operational costs. This paper presents a novel approach to expectation-driven low-level image segmentation, which can be mapped naturally onto mesh-connected massively parallel SIMD architectures capable of handling hierarchical data structures. The input image is assumed to contain a distorted version of a given template; a multiresolution stretching process is used to reshape the original template in accordance with the acquired image content, minimizing a potential function. The distorted template is the process output.",1995-12-01T00:00:00Z,http://arxiv.org/pdf/cs/9512102v1,2024-04-28,
cs/9512103v1,"Generalization of Clauses under Implication","In the area of inductive learning, generalization is a main operation, and the usual definition of induction is based on logical implication. Recently there has been a rising interest in clausal representation of knowledge in machine learning. Almost all inductive learning systems that perform generalization of clauses use the relation theta-subsumption instead of implication. The main reason is that there is a well-known and simple technique to compute least general generalizations under theta-subsumption, but not under implication. However generalization under theta-subsumption is inappropriate for learning recursive clauses, which is a crucial problem since recursion is the basic program structure of logic programs. We note that implication between clauses is undecidable, and we therefore introduce a stronger form of implication, called T-implication, which is decidable between clauses. We show that for every finite set of clauses there exists a least general generalization under T-implication. We describe a technique to reduce generalizations under implication of a clause to generalizations under theta-subsumption of what we call an expansion of the original clause. Moreover we show that for every non-tautological clause there exists a T-complete expansion, which means that every generalization under T-implication of the clause is reduced to a generalization under theta-subsumption of the expansion.",1995-12-01T00:00:00Z,http://arxiv.org/pdf/cs/9512103v1,2024-04-28,
cs/9512104v1,"Decision-Theoretic Foundations for Causal Reasoning","We present a definition of cause and effect in terms of decision-theoretic primitives and thereby provide a principled foundation for causal reasoning. Our definition departs from the traditional view of causation in that causal assertions may vary with the set of decisions available. We argue that this approach provides added clarity to the notion of cause. Also in this paper, we examine the encoding of causal relationships in directed acyclic graphs. We describe a special class of influence diagrams, those in canonical form, and show its relationship to Pearl's representation of cause and effect. Finally, we show how canonical form facilitates counterfactual reasoning.",1995-12-01T00:00:00Z,http://arxiv.org/pdf/cs/9512104v1,2024-04-28,
cs/9512105v1,"Translating between Horn Representations and their Characteristic Models","Characteristic models are an alternative, model based, representation for Horn expressions. It has been shown that these two representations are incomparable and each has its advantages over the other. It is therefore natural to ask what is the cost of translating, back and forth, between these representations. Interestingly, the same translation questions arise in database theory, where it has applications to the design of relational databases. This paper studies the computational complexity of these problems. Our main result is that the two translation problems are equivalent under polynomial reductions, and that they are equivalent to the corresponding decision problem. Namely, translating is equivalent to deciding whether a given set of models is the set of characteristic models for a given Horn expression. We also relate these problems to the hypergraph transversal problem, a well known problem which is related to other applications in AI and for which no polynomial time algorithm is known. It is shown that in general our translation problems are at least as hard as the hypergraph transversal problem, and in a special case they are equivalent to it.",1995-12-01T00:00:00Z,http://arxiv.org/pdf/cs/9512105v1,2024-04-28,
cs/9512106v1,"Statistical Feature Combination for the Evaluation of Game Positions","This article describes an application of three well-known statistical methods in the field of game-tree search: using a large number of classified Othello positions, feature weights for evaluation functions with a game-phase-independent meaning are estimated by means of logistic regression, Fisher's linear discriminant, and the quadratic discriminant function for normally distributed features. Thereafter, the playing strengths are compared by means of tournaments between the resulting versions of a world-class Othello program. In this application, logistic regression - which is used here for the first time in the context of game playing - leads to better results than the other approaches.",1995-12-01T00:00:00Z,http://arxiv.org/pdf/cs/9512106v1,2024-04-28,
cs/9512107v1,"Rule-based Machine Learning Methods for Functional Prediction","We describe a machine learning method for predicting the value of a real-valued function, given the values of multiple input variables. The method induces solutions from samples in the form of ordered disjunctive normal form (DNF) decision rules. A central objective of the method and representation is the induction of compact, easily interpretable solutions. This rule-based decision model can be extended to search efficiently for similar cases prior to approximating function values. Experimental results on real-world data demonstrate that the new techniques are competitive with existing machine learning and statistical methods and can sometimes yield superior regression performance.",1995-12-01T00:00:00Z,http://arxiv.org/pdf/cs/9512107v1,2024-04-28,
cs/9601101v1,"The Design and Experimental Analysis of Algorithms for Temporal Reasoning","Many applications -- from planning and scheduling to problems in molecular biology -- rely heavily on a temporal reasoning component. In this paper, we discuss the design and empirical analysis of algorithms for a temporal reasoning system based on Allen's influential interval-based framework for representing temporal information. At the core of the system are algorithms for determining whether the temporal information is consistent, and, if so, finding one or more scenarios that are consistent with the temporal information. Two important algorithms for these tasks are a path consistency algorithm and a backtracking algorithm. For the path consistency algorithm, we develop techniques that can result in up to a ten-fold speedup over an already highly optimized implementation. For the backtracking algorithm, we develop variable and value ordering heuristics that are shown empirically to dramatically improve the performance of the algorithm. As well, we show that a previously suggested reformulation of the backtracking search problem can reduce the time and space requirements of the backtracking search. Taken together, the techniques we develop allow a temporal reasoning component to solve problems that are of practical size.",1996-01-01T00:00:00Z,http://arxiv.org/pdf/cs/9601101v1,2024-04-28,
cs/9602101v1,"Well-Founded Semantics for Extended Logic Programs with Dynamic Preferences","The paper describes an extension of well-founded semantics for logic programs with two types of negation. In this extension information about preferences between rules can be expressed in the logical language and derived dynamically. This is achieved by using a reserved predicate symbol and a naming technique. Conflicts among rules are resolved whenever possible on the basis of derived preference information. The well-founded conclusions of prioritized logic programs can be computed in polynomial time. A legal reasoning example illustrates the usefulness of the approach.",1996-02-01T00:00:00Z,http://arxiv.org/pdf/cs/9602101v1,2024-04-28,
cs/9602102v1,"Logarithmic-Time Updates and Queries in Probabilistic Networks","Traditional databases commonly support efficient query and update procedures that operate in time which is sublinear in the size of the database. Our goal in this paper is to take a first step toward dynamic reasoning in probabilistic databases with comparable efficiency. We propose a dynamic data structure that supports efficient algorithms for updating and querying singly connected Bayesian networks. In the conventional algorithm, new evidence is absorbed in O(1) time and queries are processed in time O(N), where N is the size of the network. We propose an algorithm which, after a preprocessing phase, allows us to answer queries in time O(log N) at the expense of O(log N) time per evidence absorption. The usefulness of sub-linear processing time manifests itself in applications requiring (near) real-time response over large probabilistic databases. We briefly discuss a potential application of dynamic probabilistic reasoning in computational biology.",1996-02-01T00:00:00Z,http://arxiv.org/pdf/cs/9602102v1,2024-04-28,
cs/9603101v1,"Quantum Computing and Phase Transitions in Combinatorial Search","We introduce an algorithm for combinatorial search on quantum computers that is capable of significantly concentrating amplitude into solutions for some NP search problems, on average. This is done by exploiting the same aspects of problem structure as used by classical backtrack methods to avoid unproductive search choices. This quantum algorithm is much more likely to find solutions than the simple direct use of quantum parallelism. Furthermore, empirical evaluation on small problems shows this quantum algorithm displays the same phase transition behavior, and at the same location, as seen in many previously studied classical search methods. Specifically, difficult problem instances are concentrated near the abrupt change from underconstrained to overconstrained problems.",1996-03-01T00:00:00Z,http://arxiv.org/pdf/cs/9603101v1,2024-04-28,
cs/9603102v1,"Mean Field Theory for Sigmoid Belief Networks","We develop a mean field theory for sigmoid belief networks based on ideas from statistical mechanics. Our mean field theory provides a tractable approximation to the true probability distribution in these networks; it also yields a lower bound on the likelihood of evidence. We demonstrate the utility of this framework on a benchmark problem in statistical pattern recognition---the classification of handwritten digits.",1996-03-01T00:00:00Z,http://arxiv.org/pdf/cs/9603102v1,2024-04-28,
cs/9603103v1,"Improved Use of Continuous Attributes in C4.5","A reported weakness of C4.5 in domains with continuous attributes is addressed by modifying the formation and evaluation of tests on continuous attributes. An MDL-inspired penalty is applied to such tests, eliminating some of them from consideration and altering the relative desirability of all tests. Empirical trials show that the modifications lead to smaller decision trees with higher predictive accuracies. Results also confirm that a new version of C4.5 incorporating these changes is superior to recent approaches that use global discretization and that construct small trees with multi-interval splits.",1996-03-01T00:00:00Z,http://arxiv.org/pdf/cs/9603103v1,2024-04-28,
cs/9603104v1,"Active Learning with Statistical Models","For many types of machine learning algorithms, one can compute the statistically `optimal' way to select training data. In this paper, we review how optimal data selection techniques have been used with feedforward neural networks. We then show how the same principles may be used to select data for two alternative, statistically-based learning architectures: mixtures of Gaussians and locally weighted regression. While the techniques for neural networks are computationally expensive and approximate, the techniques for mixtures of Gaussians and locally weighted regression are both efficient and accurate. Empirically, we observe that the optimality criterion sharply decreases the number of training examples the learner needs in order to achieve good performance.",1996-03-01T00:00:00Z,http://arxiv.org/pdf/cs/9603104v1,2024-04-28,
cs/9604101v1,"A Divergence Critic for Inductive Proof","Inductive theorem provers often diverge. This paper describes a simple critic, a computer program which monitors the construction of inductive proofs attempting to identify diverging proof attempts. Divergence is recognized by means of a ``difference matching'' procedure. The critic then proposes lemmas and generalizations which ``ripple'' these differences away so that the proof can go through without divergence. The critic enables the theorem prover Spike to prove many theorems completely automatically from the definitions alone.",1996-04-01T00:00:00Z,http://arxiv.org/pdf/cs/9604101v1,2024-04-28,
cs/9604102v1,"Practical Methods for Proving Termination of General Logic Programs","Termination of logic programs with negated body atoms (here called general logic programs) is an important topic. One reason is that many computational mechanisms used to process negated atoms, like Clark's negation as failure and Chan's constructive negation, are based on termination conditions. This paper introduces a methodology for proving termination of general logic programs w.r.t. the Prolog selection rule. The idea is to distinguish parts of the program depending on whether or not their termination depends on the selection rule. To this end, the notions of low-, weakly up-, and up-acceptable program are introduced. We use these notions to develop a methodology for proving termination of general logic programs, and show how interesting problems in non-monotonic reasoning can be formalized and implemented by means of terminating general logic programs.",1996-04-01T00:00:00Z,http://arxiv.org/pdf/cs/9604102v1,2024-04-28,
cs/9604103v1,"Iterative Optimization and Simplification of Hierarchical Clusterings","Clustering is often used for discovering structure in data. Clustering systems differ in the objective function used to evaluate clustering quality and the control strategy used to search the space of clusterings. Ideally, the search strategy should consistently construct clusterings of high quality, but be computationally inexpensive as well. In general, we cannot have it both ways, but we can partition the search so that a system inexpensively constructs a `tentative' clustering for initial examination, followed by iterative optimization, which continues to search in background for improved clusterings. Given this motivation, we evaluate an inexpensive strategy for creating initial clusterings, coupled with several control strategies for iterative optimization, each of which repeatedly modifies an initial clustering in search of a better one. One of these methods appears novel as an iterative optimization strategy in clustering contexts. Once a clustering has been constructed it is judged by analysts -- often according to task-specific criteria. Several authors have abstracted these criteria and posited a generic performance task akin to pattern completion, where the error rate over completed patterns is used to `externally' judge clustering utility. Given this performance task, we adapt resampling-based pruning strategies used by supervised learning systems to the task of simplifying hierarchical clusterings, thus promising to ease post-clustering analysis. Finally, we propose a number of objective functions, based on attribute-selection measures for decision-tree induction, that might perform well on the error rate and simplicity dimensions.",1996-04-01T00:00:00Z,http://arxiv.org/pdf/cs/9604103v1,2024-04-28,
cs/9605101v1,"Further Experimental Evidence against the Utility of Occam's Razor","This paper presents new experimental evidence against the utility of Occam's razor. A~systematic procedure is presented for post-processing decision trees produced by C4.5. This procedure was derived by rejecting Occam's razor and instead attending to the assumption that similar objects are likely to belong to the same class. It increases a decision tree's complexity without altering the performance of that tree on the training data from which it is inferred. The resulting more complex decision trees are demonstrated to have, on average, for a variety of common learning tasks, higher predictive accuracy than the less complex original decision trees. This result raises considerable doubt about the utility of Occam's razor as it is commonly applied in modern machine learning.",1996-05-01T00:00:00Z,http://arxiv.org/pdf/cs/9605101v1,2024-04-28,
cs/9605102v1,"Least Generalizations and Greatest Specializations of Sets of Clauses","The main operations in Inductive Logic Programming (ILP) are generalization and specialization, which only make sense in a generality order. In ILP, the three most important generality orders are subsumption, implication and implication relative to background knowledge. The two languages used most often are languages of clauses and languages of only Horn clauses. This gives a total of six different ordered languages. In this paper, we give a systematic treatment of the existence or non-existence of least generalizations and greatest specializations of finite sets of clauses in each of these six ordered sets. We survey results already obtained by others and also contribute some answers of our own. Our main new results are, firstly, the existence of a computable least generalization under implication of every finite set of clauses containing at least one non-tautologous function-free clause (among other, not necessarily function-free clauses). Secondly, we show that such a least generalization need not exist under relative implication, not even if both the set that is to be generalized and the background knowledge are function-free. Thirdly, we give a complete discussion of existence and non-existence of greatest specializations in each of the six ordered languages.",1996-05-01T00:00:00Z,http://arxiv.org/pdf/cs/9605102v1,2024-04-28,
cs/9605103v1,"Reinforcement Learning: A Survey","This paper surveys the field of reinforcement learning from a computer-science perspective. It is written to be accessible to researchers familiar with machine learning. Both the historical basis of the field and a broad selection of current work are summarized. Reinforcement learning is the problem faced by an agent that learns behavior through trial-and-error interactions with a dynamic environment. The work described here has a resemblance to work in psychology, but differs considerably in the details and in the use of the word ``reinforcement.'' The paper discusses central issues of reinforcement learning, including trading off exploration and exploitation, establishing the foundations of the field via Markov decision theory, learning from delayed reinforcement, constructing empirical models to accelerate learning, making use of generalization and hierarchy, and coping with hidden state. It concludes with a survey of some implemented systems and an assessment of the practical utility of current methods for reinforcement learning.",1996-05-01T00:00:00Z,http://arxiv.org/pdf/cs/9605103v1,2024-04-28,
cs/9605104v1,"Adaptive Problem-solving for Large-scale Scheduling Problems: A Case Study","Although most scheduling problems are NP-hard, domain specific techniques perform well in practice but are quite expensive to construct. In adaptive problem-solving solving, domain specific knowledge is acquired automatically for a general problem solver with a flexible control architecture. In this approach, a learning system explores a space of possible heuristic methods for one well-suited to the eccentricities of the given domain and problem distribution. In this article, we discuss an application of the approach to scheduling satellite communications. Using problem distributions based on actual mission requirements, our approach identifies strategies that not only decrease the amount of CPU time required to produce schedules, but also increase the percentage of problems that are solvable within computational resource limitations.",1996-05-01T00:00:00Z,http://arxiv.org/pdf/cs/9605104v1,2024-04-28,
cs/9605105v1,"A Formal Framework for Speedup Learning from Problems and Solutions","Speedup learning seeks to improve the computational efficiency of problem solving with experience. In this paper, we develop a formal framework for learning efficient problem solving from random problems and their solutions. We apply this framework to two different representations of learned knowledge, namely control rules and macro-operators, and prove theorems that identify sufficient conditions for learning in each representation. Our proofs are constructive in that they are accompanied with learning algorithms. Our framework captures both empirical and explanation-based speedup learning in a unified fashion. We illustrate our framework with implementations in two domains: symbolic integration and Eight Puzzle. This work integrates many strands of experimental and theoretical work in machine learning, including empirical learning of control rules, macro-operator learning, Explanation-Based Learning (EBL), and Probably Approximately Correct (PAC) Learning.",1996-05-01T00:00:00Z,http://arxiv.org/pdf/cs/9605105v1,2024-04-28,
cs/9605106v1,"2Planning for Contingencies: A Decision-based Approach","A fundamental assumption made by classical AI planners is that there is no uncertainty in the world: the planner has full knowledge of the conditions under which the plan will be executed and the outcome of every action is fully predictable. These planners cannot therefore construct contingency plans, i.e., plans in which different actions are performed in different circumstances. In this paper we discuss some issues that arise in the representation and construction of contingency plans and describe Cassandra, a partial-order contingency planner. Cassandra uses explicit decision-steps that enable the agent executing the plan to decide which plan branch to follow. The decision-steps in a plan result in subgoals to acquire knowledge, which are planned for in the same way as any other subgoals. Cassandra thus distinguishes the process of gathering information from the process of making decisions. The explicit representation of decisions in Cassandra allows a coherent approach to the problems of contingent planning, and provides a solid base for extensions such as the use of different decision-making procedures.",1996-05-01T00:00:00Z,http://arxiv.org/pdf/cs/9605106v1,2024-04-28,
cs/9606101v1,"A Principled Approach Towards Symbolic Geometric Constraint Satisfaction","An important problem in geometric reasoning is to find the configuration of a collection of geometric bodies so as to satisfy a set of given constraints. Recently, it has been suggested that this problem can be solved efficiently by symbolically reasoning about geometry. This approach, called degrees of freedom analysis, employs a set of specialized routines called plan fragments that specify how to change the configuration of a set of bodies to satisfy a new constraint while preserving existing constraints. A potential drawback, which limits the scalability of this approach, is concerned with the difficulty of writing plan fragments. In this paper we address this limitation by showing how these plan fragments can be automatically synthesized using first principles about geometric bodies, actions, and topology.",1996-06-01T00:00:00Z,http://arxiv.org/pdf/cs/9606101v1,2024-04-28,
cs/9606102v1,"On Partially Controlled Multi-Agent Systems","Motivated by the control theoretic distinction between controllable and uncontrollable events, we distinguish between two types of agents within a multi-agent system: controllable agents, which are directly controlled by the system's designer, and uncontrollable agents, which are not under the designer's direct control. We refer to such systems as partially controlled multi-agent systems, and we investigate how one might influence the behavior of the uncontrolled agents through appropriate design of the controlled agents. In particular, we wish to understand which problems are naturally described in these terms, what methods can be applied to influence the uncontrollable agents, the effectiveness of such methods, and whether similar methods work across different domains. Using a game-theoretic framework, this paper studies the design of partially controlled multi-agent systems in two contexts: in one context, the uncontrollable agents are expected utility maximizers, while in the other they are reinforcement learners. We suggest different techniques for controlling agents' behavior in each domain, assess their success, and examine their relationship.",1996-06-01T00:00:00Z,http://arxiv.org/pdf/cs/9606102v1,2024-04-28,
cs/9608103v1,"Spatial Aggregation: Theory and Applications","Visual thinking plays an important role in scientific reasoning. Based on the research in automating diverse reasoning tasks about dynamical systems, nonlinear controllers, kinematic mechanisms, and fluid motion, we have identified a style of visual thinking, imagistic reasoning. Imagistic reasoning organizes computations around image-like, analogue representations so that perceptual and symbolic operations can be brought to bear to infer structure and behavior. Programs incorporating imagistic reasoning have been shown to perform at an expert level in domains that defy current analytic or numerical methods. We have developed a computational paradigm, spatial aggregation, to unify the description of a class of imagistic problem solvers. A program written in this paradigm has the following properties. It takes a continuous field and optional objective functions as input, and produces high-level descriptions of structure, behavior, or control actions. It computes a multi-layer of intermediate representations, called spatial aggregates, by forming equivalence classes and adjacency relations. It employs a small set of generic operators such as aggregation, classification, and localization to perform bidirectional mapping between the information-rich field and successively more abstract spatial aggregates. It uses a data structure, the neighborhood graph, as a common interface to modularize computations. To illustrate our theory, we describe the computational structure of three implemented problem solvers -- KAM, MAPS, and HIPAIR --- in terms of the spatial aggregation generic operators by mixing and matching a library of commonly used routines.",1996-08-01T00:00:00Z,http://arxiv.org/pdf/cs/9608103v1,2024-04-28,
cs/9608104v1,"A Hierarchy of Tractable Subsets for Computing Stable Models","Finding the stable models of a knowledge base is a significant computational problem in artificial intelligence. This task is at the computational heart of truth maintenance systems, autoepistemic logic, and default logic. Unfortunately, it is NP-hard. In this paper we present a hierarchy of classes of knowledge bases, Omega_1,Omega_2,..., with the following properties: first, Omega_1 is the class of all stratified knowledge bases; second, if a knowledge base Pi is in Omega_k, then Pi has at most k stable models, and all of them may be found in time O(lnk), where l is the length of the knowledge base and n the number of atoms in Pi; third, for an arbitrary knowledge base Pi, we can find the minimum k such that Pi belongs to Omega_k in time polynomial in the size of Pi; and, last, where K is the class of all knowledge bases, it is the case that union{i=1 to infty} Omega_i = K, that is, every knowledge base belongs to some class in the hierarchy.",1996-08-01T00:00:00Z,http://arxiv.org/pdf/cs/9608104v1,2024-04-28,
cs/9609101v1,"Accelerating Partial-Order Planners: Some Techniques for Effective Search Control and Pruning","We propose some domain-independent techniques for bringing well-founded partial-order planners closer to practicality. The first two techniques are aimed at improving search control while keeping overhead costs low. One is based on a simple adjustment to the default A* heuristic used by UCPOP to select plans for refinement. The other is based on preferring ``zero commitment'' (forced) plan refinements whenever possible, and using LIFO prioritization otherwise. A more radical technique is the use of operator parameter domains to prune search. These domains are initially computed from the definitions of the operators and the initial and goal conditions, using a polynomial-time algorithm that propagates sets of constants through the operator graph, starting in the initial conditions. During planning, parameter domains can be used to prune nonviable operator instances and to remove spurious clobbering threats. In experiments based on modifications of UCPOP, our improved plan and goal selection strategies gave speedups by factors ranging from 5 to more than 1000 for a variety of problems that are nontrivial for the unmodified version. Crucially, the hardest problems gave the greatest improvements. The pruning technique based on parameter domains often gave speedups by an order of magnitude or more for difficult problems, both with the default UCPOP search strategy and with our improved strategy. The Lisp code for our techniques and for the test problems is provided in on-line appendices.",1996-09-01T00:00:00Z,http://arxiv.org/pdf/cs/9609101v1,2024-04-28,
cs/9609102v1,"Cue Phrase Classification Using Machine Learning","Cue phrases may be used in a discourse sense to explicitly signal discourse structure, but also in a sentential sense to convey semantic rather than structural information. Correctly classifying cue phrases as discourse or sentential is critical in natural language processing systems that exploit discourse structure, e.g., for performing tasks such as anaphora resolution and plan recognition. This paper explores the use of machine learning for classifying cue phrases as discourse or sentential. Two machine learning programs (Cgrendel and C4.5) are used to induce classification models from sets of pre-classified cue phrases and their features in text and speech. Machine learning is shown to be an effective technique for not only automating the generation of classification models, but also for improving upon previous results. When compared to manually derived classification models already in the literature, the learned models often perform with higher accuracy and contain new linguistic insights into the data. In addition, the ability to automatically construct classification models makes it easier to comparatively analyze the utility of alternative feature representations of the data. Finally, the ease of retraining makes the learning approach more scalable and flexible than manual methods.",1996-09-01T00:00:00Z,http://arxiv.org/pdf/cs/9609102v1,2024-04-28,
cs/9610101v1,"Mechanisms for Automated Negotiation in State Oriented Domains","This paper lays part of the groundwork for a domain theory of negotiation, that is, a way of classifying interactions so that it is clear, given a domain, which negotiation mechanisms and strategies are appropriate. We define State Oriented Domains, a general category of interaction. Necessary and sufficient conditions for cooperation are outlined. We use the notion of worth in an altered definition of utility, thus enabling agreements in a wider class of joint-goal reachable situations. An approach is offered for conflict resolution, and it is shown that even in a conflict situation, partial cooperative steps can be taken by interacting agents (that is, agents in fundamental conflict might still agree to cooperate up to a certain point). A Unified Negotiation Protocol (UNP) is developed that can be used in all types of encounters. It is shown that in certain borderline cooperative situations, a partial cooperative agreement (i.e., one that does not achieve all agents' goals) might be preferred by all agents, even though there exists a rational agreement that would achieve all their goals. Finally, we analyze cases where agents have incomplete information on the goals and worth of other agents. First we consider the case where agents' goals are private information, and we analyze what goal declaration strategies the agents might adopt to increase their utility. Then, we consider the situation where the agents' goals (and therefore stand-alone costs) are common knowledge, but the worth they attach to their goals is private information. We introduce two mechanisms, one 'strict', the other 'tolerant', and analyze their affects on the stability and efficiency of negotiation outcomes.",1996-10-01T00:00:00Z,http://arxiv.org/pdf/cs/9610101v1,2024-04-28,
cs/9610102v1,"Learning First-Order Definitions of Functions","First-order learning involves finding a clause-form definition of a relation from examples of the relation and relevant background information. In this paper, a particular first-order learning system is modified to customize it for finding definitions of functional relations. This restriction leads to faster learning times and, in some cases, to definitions that have higher predictive accuracy. Other first-order learning systems might benefit from similar specialization.",1996-10-01T00:00:00Z,http://arxiv.org/pdf/cs/9610102v1,2024-04-28,
cs/9611101v1,"MUSE CSP: An Extension to the Constraint Satisfaction Problem","This paper describes an extension to the constraint satisfaction problem (CSP) called MUSE CSP (MUltiply SEgmented Constraint Satisfaction Problem). This extension is especially useful for those problems which segment into multiple sets of partially shared variables. Such problems arise naturally in signal processing applications including computer vision, speech processing, and handwriting recognition. For these applications, it is often difficult to segment the data in only one way given the low-level information utilized by the segmentation algorithms. MUSE CSP can be used to compactly represent several similar instances of the constraint satisfaction problem. If multiple instances of a CSP have some common variables which have the same domains and constraints, then they can be combined into a single instance of a MUSE CSP, reducing the work required to apply the constraints. We introduce the concepts of MUSE node consistency, MUSE arc consistency, and MUSE path consistency. We then demonstrate how MUSE CSP can be used to compactly represent lexically ambiguous sentences and the multiple sentence hypotheses that are often generated by speech recognition algorithms so that grammar constraints can be used to provide parses for all syntactically correct sentences. Algorithms for MUSE arc and path consistency are provided. Finally, we discuss how to create a MUSE CSP from a set of CSPs which are labeled to indicate when the same variable is shared by more than a single CSP.",1996-11-01T00:00:00Z,http://arxiv.org/pdf/cs/9611101v1,2024-04-28,
cs/9612101v1,"Exploiting Causal Independence in Bayesian Network Inference","A new method is proposed for exploiting causal independencies in exact Bayesian network inference. A Bayesian network can be viewed as representing a factorization of a joint probability into the multiplication of a set of conditional probabilities. We present a notion of causal independence that enables one to further factorize the conditional probabilities into a combination of even smaller factors and consequently obtain a finer-grain factorization of the joint probability. The new formulation of causal independence lets us specify the conditional probability of a variable given its parents in terms of an associative and commutative operator, such as ``or'', ``sum'' or ``max'', on the contribution of each parent. We start with a simple algorithm VE for Bayesian network inference that, given evidence and a query variable, uses the factorization to find the posterior distribution of the query. We show how this algorithm can be extended to exploit causal independence. Empirical studies, based on the CPCS networks for medical diagnosis, show that this method is more efficient than previous methods and allows for inference in larger networks than previous algorithms.",1996-12-01T00:00:00Z,http://arxiv.org/pdf/cs/9612101v1,2024-04-28,
cs/9612102v1,"Quantitative Results Comparing Three Intelligent Interfaces for Information Capture: A Case Study Adding Name Information into an Electronic Personal Organizer","Efficiently entering information into a computer is key to enjoying the benefits of computing. This paper describes three intelligent user interfaces: handwriting recognition, adaptive menus, and predictive fillin. In the context of adding a personUs name and address to an electronic organizer, tests show handwriting recognition is slower than typing on an on-screen, soft keyboard, while adaptive menus and predictive fillin can be twice as fast. This paper also presents strategies for applying these three interfaces to other information collection domains.",1996-12-01T00:00:00Z,http://arxiv.org/pdf/cs/9612102v1,2024-04-28,
cs/9612103v1,"Characterizations of Decomposable Dependency Models","Decomposable dependency models possess a number of interesting and useful properties. This paper presents new characterizations of decomposable models in terms of independence relationships, which are obtained by adding a single axiom to the well-known set characterizing dependency models that are isomorphic to undirected graphs. We also briefly discuss a potential application of our results to the problem of learning graphical models from data.",1996-12-01T00:00:00Z,http://arxiv.org/pdf/cs/9612103v1,2024-04-28,
cs/9701101v1,"Improved Heterogeneous Distance Functions","Instance-based learning techniques typically handle continuous and linear input values well, but often do not handle nominal input attributes appropriately. The Value Difference Metric (VDM) was designed to find reasonable distance values between nominal attribute values, but it largely ignores continuous attributes, requiring discretization to map continuous values into nominal values. This paper proposes three new heterogeneous distance functions, called the Heterogeneous Value Difference Metric (HVDM), the Interpolated Value Difference Metric (IVDM), and the Windowed Value Difference Metric (WVDM). These new distance functions are designed to handle applications with nominal attributes, continuous attributes, or both. In experiments on 48 applications the new distance metrics achieve higher classification accuracy on average than three previous distance functions on those datasets that have both nominal and continuous attributes.",1997-01-01T00:00:00Z,http://arxiv.org/pdf/cs/9701101v1,2024-04-28,
cs/9701102v1,"SCREEN: Learning a Flat Syntactic and Semantic Spoken Language Analysis Using Artificial Neural Networks","Previous approaches of analyzing spontaneously spoken language often have been based on encoding syntactic and semantic knowledge manually and symbolically. While there has been some progress using statistical or connectionist language models, many current spoken- language systems still use a relatively brittle, hand-coded symbolic grammar or symbolic semantic component. In contrast, we describe a so-called screening approach for learning robust processing of spontaneously spoken language. A screening approach is a flat analysis which uses shallow sequences of category representations for analyzing an utterance at various syntactic, semantic and dialog levels. Rather than using a deeply structured symbolic analysis, we use a flat connectionist analysis. This screening approach aims at supporting speech and language processing by using (1) data-driven learning and (2) robustness of connectionist networks. In order to test this approach, we have developed the SCREEN system which is based on this new robust, learned and flat analysis. In this paper, we focus on a detailed description of SCREEN's architecture, the flat syntactic and semantic analysis, the interaction with a speech recognizer, and a detailed evaluation analysis of the robustness under the influence of noisy or incomplete input. The main result of this paper is that flat representations allow more robust processing of spontaneous spoken language than deeply structured representations. In particular, we show how the fault-tolerance and learning capability of connectionist networks can support a flat analysis for providing more robust spoken-language processing within an overall hybrid symbolic/connectionist framework.",1997-01-01T00:00:00Z,http://arxiv.org/pdf/cs/9701102v1,2024-04-28,
cs/9703101v1,"A Uniform Framework for Concept Definitions in Description Logics","Most modern formalisms used in Databases and Artificial Intelligence for describing an application domain are based on the notions of class (or concept) and relationship among classes. One interesting feature of such formalisms is the possibility of defining a class, i.e., providing a set of properties that precisely characterize the instances of the class. Many recent articles point out that there are several ways of assigning a meaning to a class definition containing some sort of recursion. In this paper, we argue that, instead of choosing a single style of semantics, we achieve better results by adopting a formalism that allows for different semantics to coexist. We demonstrate the feasibility of our argument, by presenting a knowledge representation formalism, the description logic muALCQ, with the above characteristics. In addition to the constructs for conjunction, disjunction, negation, quantifiers, and qualified number restrictions, muALCQ includes special fixpoint constructs to express (suitably interpreted) recursive definitions. These constructs enable the usual frame-based descriptions to be combined with definitions of recursive data structures such as directed acyclic graphs, lists, streams, etc. We establish several properties of muALCQ, including the decidability and the computational complexity of reasoning, by formulating a correspondence with a particular modal logic of programs called the modal mu-calculus.",1997-03-01T00:00:00Z,http://arxiv.org/pdf/cs/9703101v1,2024-04-28,
cs/9704101v1,"Lifeworld Analysis","We argue that the analysis of agent/environment interactions should be extended to include the conventions and invariants maintained by agents throughout their activity. We refer to this thicker notion of environment as a lifeworld and present a partial set of formal tools for describing structures of lifeworlds and the ways in which they computationally simplify activity. As one specific example, we apply the tools to the analysis of the Toast system and show how versions of the system with very different control structures in fact implement a common control structure together with different conventions for encoding task state in the positions or states of objects in the environment.",1997-04-01T00:00:00Z,http://arxiv.org/pdf/cs/9704101v1,2024-04-28,
cs/9705101v1,"Query DAGs: A Practical Paradigm for Implementing Belief-Network Inference","We describe a new paradigm for implementing inference in belief networks, which consists of two steps: (1) compiling a belief network into an arithmetic expression called a Query DAG (Q-DAG); and (2) answering queries using a simple evaluation algorithm. Each node of a Q-DAG represents a numeric operation, a number, or a symbol for evidence. Each leaf node of a Q-DAG represents the answer to a network query, that is, the probability of some event of interest. It appears that Q-DAGs can be generated using any of the standard algorithms for exact inference in belief networks (we show how they can be generated using clustering and conditioning algorithms). The time and space complexity of a Q-DAG generation algorithm is no worse than the time complexity of the inference algorithm on which it is based. The complexity of a Q-DAG evaluation algorithm is linear in the size of the Q-DAG, and such inference amounts to a standard evaluation of the arithmetic expression it represents. The intended value of Q-DAGs is in reducing the software and hardware resources required to utilize belief networks in on-line, real-world applications. The proposed framework also facilitates the development of on-line inference on different software and hardware platforms due to the simplicity of the Q-DAG evaluation algorithm. Interestingly enough, Q-DAGs were found to serve other purposes: simple techniques for reducing Q-DAGs tend to subsume relatively complex optimization techniques for belief-network inference, such as network-pruning and computation-caching.",1997-05-01T00:00:00Z,http://arxiv.org/pdf/cs/9705101v1,2024-04-28,
cs/9705102v1,"Connectionist Theory Refinement: Genetically Searching the Space of Network Topologies","An algorithm that learns from a set of examples should ideally be able to exploit the available resources of (a) abundant computing power and (b) domain-specific knowledge to improve its ability to generalize. Connectionist theory-refinement systems, which use background knowledge to select a neural network's topology and initial weights, have proven to be effective at exploiting domain-specific knowledge; however, most do not exploit available computing power. This weakness occurs because they lack the ability to refine the topology of the neural networks they produce, thereby limiting generalization, especially when given impoverished domain theories. We present the REGENT algorithm which uses (a) domain-specific knowledge to help create an initial population of knowledge-based neural networks and (b) genetic operators of crossover and mutation (specifically designed for knowledge-based networks) to continually search for better network topologies. Experiments on three real-world domains indicate that our new algorithm is able to significantly increase generalization compared to a standard connectionist theory-refinement system, as well as our previous algorithm for growing knowledge-based networks.",1997-05-01T00:00:00Z,http://arxiv.org/pdf/cs/9705102v1,2024-04-28,
cs/9706101v1,"Flaw Selection Strategies for Partial-Order Planning","Several recent studies have compared the relative efficiency of alternative flaw selection strategies for partial-order causal link (POCL) planning. We review this literature, and present new experimental results that generalize the earlier work and explain some of the discrepancies in it. In particular, we describe the Least-Cost Flaw Repair (LCFR) strategy developed and analyzed by Joslin and Pollack (1994), and compare it with other strategies, including Gerevini and Schubert's (1996) ZLIFO strategy. LCFR and ZLIFO make very different, and apparently conflicting claims about the most effective way to reduce search-space size in POCL planning. We resolve this conflict, arguing that much of the benefit that Gerevini and Schubert ascribe to the LIFO component of their ZLIFO strategy is better attributed to other causes. We show that for many problems, a strategy that combines least-cost flaw selection with the delay of separable threats will be effective in reducing search-space size, and will do so without excessive computational overhead. Although such a strategy thus provides a good default, we also show that certain domain characteristics may reduce its effectiveness.",1997-06-01T00:00:00Z,http://arxiv.org/pdf/cs/9706101v1,2024-04-28,
cs/9706102v1,"A Complete Classification of Tractability in RCC-5","We investigate the computational properties of the spatial algebra RCC-5 which is a restricted version of the RCC framework for spatial reasoning. The satisfiability problem for RCC-5 is known to be NP-complete but not much is known about its approximately four billion subclasses. We provide a complete classification of satisfiability for all these subclasses into polynomial and NP-complete respectively. In the process, we identify all maximal tractable subalgebras which are four in total.",1997-06-01T00:00:00Z,http://arxiv.org/pdf/cs/9706102v1,2024-04-28,
cs/9707101v1,"A New Look at the Easy-Hard-Easy Pattern of Combinatorial Search Difficulty","The easy-hard-easy pattern in the difficulty of combinatorial search problems as constraints are added has been explained as due to a competition between the decrease in number of solutions and increased pruning. We test the generality of this explanation by examining one of its predictions: if the number of solutions is held fixed by the choice of problems, then increased pruning should lead to a monotonic decrease in search cost. Instead, we find the easy-hard-easy pattern in median search cost even when the number of solutions is held constant, for some search methods. This generalizes previous observations of this pattern and shows that the existing theory does not explain the full range of the peak in search cost. In these cases the pattern appears to be due to changes in the size of the minimal unsolvable subproblems, rather than changing numbers of solutions.",1997-07-01T00:00:00Z,http://arxiv.org/pdf/cs/9707101v1,2024-04-28,
cs/9707102v1,"Eight Maximal Tractable Subclasses of Allen's Algebra with Metric Time","This paper combines two important directions of research in temporal resoning: that of finding maximal tractable subclasses of Allen's interval algebra, and that of reasoning with metric temporal information. Eight new maximal tractable subclasses of Allen's interval algebra are presented, some of them subsuming previously reported tractable algebras. The algebras allow for metric temporal constraints on interval starting or ending points, using the recent framework of Horn DLRs. Two of the algebras can express the notion of sequentiality between intervals, being the first such algebras admitting both qualitative and metric time.",1997-07-01T00:00:00Z,http://arxiv.org/pdf/cs/9707102v1,2024-04-28,
cs/9707103v1,"Defining Relative Likelihood in Partially-Ordered Preferential Structures","Starting with a likelihood or preference order on worlds, we extend it to a likelihood ordering on sets of worlds in a natural way, and examine the resulting logic. Lewis earlier considered such a notion of relative likelihood in the context of studying counterfactuals, but he assumed a total preference order on worlds. Complications arise when examining partial orders that are not present for total orders. There are subtleties involving the exact approach to lifting the order on worlds to an order on sets of worlds. In addition, the axiomatization of the logic of relative likelihood in the case of partial orders gives insight into the connection between relative likelihood and default reasoning.",1997-07-01T00:00:00Z,http://arxiv.org/pdf/cs/9707103v1,2024-04-28,
cs/9709101v1,"Towards Flexible Teamwork","Many AI researchers are today striving to build agent teams for complex, dynamic multi-agent domains, with intended applications in arenas such as education, training, entertainment, information integration, and collective robotics. Unfortunately, uncertainties in these complex, dynamic domains obstruct coherent teamwork. In particular, team members often encounter differing, incomplete, and possibly inconsistent views of their environment. Furthermore, team members can unexpectedly fail in fulfilling responsibilities or discover unexpected opportunities. Highly flexible coordination and communication is key in addressing such uncertainties. Simply fitting individual agents with precomputed coordination plans will not do, for their inflexibility can cause severe failures in teamwork, and their domain-specificity hinders reusability. Our central hypothesis is that the key to such flexibility and reusability is providing agents with general models of teamwork. Agents exploit such models to autonomously reason about coordination and communication, providing requisite flexibility. Furthermore, the models enable reuse across domains, both saving implementation effort and enforcing consistency. This article presents one general, implemented model of teamwork, called STEAM. The basic building block of teamwork in STEAM is joint intentions (Cohen & Levesque, 1991b); teamwork in STEAM is based on agents' building up a (partial) hierarchy of joint intentions (this hierarchy is seen to parallel Grosz & Kraus's partial SharedPlans, 1996). Furthermore, in STEAM, team members monitor the team's and individual members' performance, reorganizing the team as necessary. Finally, decision-theoretic communication selectivity in STEAM ensures reduction in communication overheads of teamwork, with appropriate sensitivity to the environmental conditions. This article describes STEAM's application in three different complex domains, and presents detailed empirical results.",1997-09-01T00:00:00Z,http://arxiv.org/pdf/cs/9709101v1,2024-04-28,
cs/9709102v1,"Identifying Hierarchical Structure in Sequences: A linear-time algorithm","SEQUITUR is an algorithm that infers a hierarchical structure from a sequence of discrete symbols by replacing repeated phrases with a grammatical rule that generates the phrase, and continuing this process recursively. The result is a hierarchical representation of the original sequence, which offers insights into its lexical structure. The algorithm is driven by two constraints that reduce the size of the grammar, and produce structure as a by-product. SEQUITUR breaks new ground by operating incrementally. Moreover, the method's simple structure permits a proof that it operates in space and time that is linear in the size of the input. Our implementation can process 50,000 symbols per second and has been applied to an extensive range of real world sequences.",1997-09-01T00:00:00Z,http://arxiv.org/pdf/cs/9709102v1,2024-04-28,
cs/9711102v1,"Storing and Indexing Plan Derivations through Explanation-based Analysis of Retrieval Failures","Case-Based Planning (CBP) provides a way of scaling up domain-independent planning to solve large problems in complex domains. It replaces the detailed and lengthy search for a solution with the retrieval and adaptation of previous planning experiences. In general, CBP has been demonstrated to improve performance over generative (from-scratch) planning. However, the performance improvements it provides are dependent on adequate judgements as to problem similarity. In particular, although CBP may substantially reduce planning effort overall, it is subject to a mis-retrieval problem. The success of CBP depends on these retrieval errors being relatively rare. This paper describes the design and implementation of a replay framework for the case-based planner DERSNLP+EBL. DERSNLP+EBL extends current CBP methodology by incorporating explanation-based learning techniques that allow it to explain and learn from the retrieval failures it encounters. These techniques are used to refine judgements about case similarity in response to feedback when a wrong decision has been made. The same failure analysis is used in building the case library, through the addition of repairing cases. Large problems are split and stored as single goal subproblems. Multi-goal problems are stored only when these smaller cases fail to be merged into a full solution. An empirical evaluation of this approach demonstrates the advantage of learning from experienced retrieval failure.",1997-11-01T00:00:00Z,http://arxiv.org/pdf/cs/9711102v1,2024-04-28,
cs/9711103v1,"A Model Approximation Scheme for Planning in Partially Observable Stochastic Domains","Partially observable Markov decision processes (POMDPs) are a natural model for planning problems where effects of actions are nondeterministic and the state of the world is not completely observable. It is difficult to solve POMDPs exactly. This paper proposes a new approximation scheme. The basic idea is to transform a POMDP into another one where additional information is provided by an oracle. The oracle informs the planning agent that the current state of the world is in a certain region. The transformed POMDP is consequently said to be region observable. It is easier to solve than the original POMDP. We propose to solve the transformed POMDP and use its optimal policy to construct an approximate policy for the original POMDP. By controlling the amount of additional information that the oracle provides, it is possible to find a proper tradeoff between computational time and approximation quality. In terms of algorithmic contributions, we study in details how to exploit region observability in solving the transformed POMDP. To facilitate the study, we also propose a new exact algorithm for general POMDPs. The algorithm is conceptually simple and yet is significantly more efficient than all previous exact algorithms.",1997-11-01T00:00:00Z,http://arxiv.org/pdf/cs/9711103v1,2024-04-28,
cs/9711104v1,"Dynamic Non-Bayesian Decision Making","The model of a non-Bayesian agent who faces a repeated game with incomplete information against Nature is an appropriate tool for modeling general agent-environment interactions. In such a model the environment state (controlled by Nature) may change arbitrarily, and the feedback/reward function is initially unknown. The agent is not Bayesian, that is he does not form a prior probability neither on the state selection strategy of Nature, nor on his reward function. A policy for the agent is a function which assigns an action to every history of observations and actions. Two basic feedback structures are considered. In one of them -- the perfect monitoring case -- the agent is able to observe the previous environment state as part of his feedback, while in the other -- the imperfect monitoring case -- all that is available to the agent is the reward obtained. Both of these settings refer to partially observable processes, where the current environment state is unknown. Our main result refers to the competitive ratio criterion in the perfect monitoring case. We prove the existence of an efficient stochastic policy that ensures that the competitive ratio is obtained at almost all stages with an arbitrarily high probability, where efficiency is measured in terms of rate of convergence. It is further shown that such an optimal policy does not exist in the imperfect monitoring case. Moreover, it is proved that in the perfect monitoring case there does not exist a deterministic policy that satisfies our long run optimality criterion. In addition, we discuss the maxmin criterion and prove that a deterministic efficient optimal strategy does exist in the imperfect monitoring case under this criterion. Finally we show that our approach to long-run optimality can be viewed as qualitative, which distinguishes it from previous work in this area.",1997-11-01T00:00:00Z,http://arxiv.org/pdf/cs/9711104v1,2024-04-28,
cs/9712101v1,"When Gravity Fails: Local Search Topology","Local search algorithms for combinatorial search problems frequently encounter a sequence of states in which it is impossible to improve the value of the objective function; moves through these regions, called plateau moves, dominate the time spent in local search. We analyze and characterize plateaus for three different classes of randomly generated Boolean Satisfiability problems. We identify several interesting features of plateaus that impact the performance of local search algorithms. We show that local minima tend to be small but occasionally may be very large. We also show that local minima can be escaped without unsatisfying a large number of clauses, but that systematically searching for an escape route may be computationally expensive if the local minimum is large. We show that plateaus with exits, called benches, tend to be much larger than minima, and that some benches have very few exit states which local search can use to escape. We show that the solutions (i.e., global minima) of randomly generated problem instances form clusters, which behave similarly to local minima. We revisit several enhancements of local search algorithms and explain their performance in light of our results. Finally we discuss strategies for creating the next generation of local search algorithms.",1997-12-01T00:00:00Z,http://arxiv.org/pdf/cs/9712101v1,2024-04-28,
cs/9712102v1,"Bidirectional Heuristic Search Reconsidered","The assessment of bidirectional heuristic search has been incorrect since it was first published more than a quarter of a century ago. For quite a long time, this search strategy did not achieve the expected results, and there was a major misunderstanding about the reasons behind it. Although there is still wide-spread belief that bidirectional heuristic search is afflicted by the problem of search frontiers passing each other, we demonstrate that this conjecture is wrong. Based on this finding, we present both a new generic approach to bidirectional heuristic search and a new approach to dynamically improving heuristic values that is feasible in bidirectional search only. These approaches are put into perspective with both the traditional and more recently proposed approaches in order to facilitate a better overall understanding. Empirical results of experiments with our new approaches show that bidirectional heuristic search can be performed very efficiently and also with limited memory. These results suggest that bidirectional heuristic search appears to be better for solving certain difficult problems than corresponding unidirectional search. This provides some evidence for the usefulness of a search strategy that was long neglected. In summary, we show that bidirectional heuristic search is viable and consequently propose that it be reconsidered.",1997-12-01T00:00:00Z,http://arxiv.org/pdf/cs/9712102v1,2024-04-28,
cs/9801101v1,"Incremental Recompilation of Knowledge","Approximating a general formula from above and below by Horn formulas (its Horn envelope and Horn core, respectively) was proposed by Selman and Kautz (1991, 1996) as a form of ``knowledge compilation,'' supporting rapid approximate reasoning; on the negative side, this scheme is static in that it supports no updates, and has certain complexity drawbacks pointed out by Kavvadias, Papadimitriou and Sideri (1993). On the other hand, the many frameworks and schemes proposed in the literature for theory update and revision are plagued by serious complexity-theoretic impediments, even in the Horn case, as was pointed out by Eiter and Gottlob (1992), and is further demonstrated in the present paper. More fundamentally, these schemes are not inductive, in that they may lose in a single update any positive properties of the represented sets of formulas (small size, Horn structure, etc.). In this paper we propose a new scheme, incremental recompilation, which combines Horn approximation and model-based updates; this scheme is inductive and very efficient, free of the problems facing its constituents. A set of formulas is represented by an upper and lower Horn approximation. To update, we replace the upper Horn formula by the Horn envelope of its minimum-change update, and similarly the lower one by the Horn core of its update; the key fact which enables this scheme is that Horn envelopes and cores are easy to compute when the underlying formula is the result of a minimum-change update of a Horn formula by a clause. We conjecture that efficient algorithms are possible for more complex updates.",1998-01-01T00:00:00Z,http://arxiv.org/pdf/cs/9801101v1,2024-04-28,
cs/9801102v1,"Monotonicity and Persistence in Preferential Logics","An important characteristic of many logics for Artificial Intelligence is their nonmonotonicity. This means that adding a formula to the premises can invalidate some of the consequences. There may, however, exist formulae that can always be safely added to the premises without destroying any of the consequences: we say they respect monotonicity. Also, there may be formulae that, when they are a consequence, can not be invalidated when adding any formula to the premises: we call them conservative. We study these two classes of formulae for preferential logics, and show that they are closely linked to the formulae whose truth-value is preserved along the (preferential) ordering. We will consider some preferential logics for illustration, and prove syntactic characterization results for them. The results in this paper may improve the efficiency of theorem provers for preferential logics.",1998-01-01T00:00:00Z,http://arxiv.org/pdf/cs/9801102v1,2024-04-28,
cs/9803101v1,"Synthesizing Customized Planners from Specifications","Existing plan synthesis approaches in artificial intelligence fall into two categories -- domain independent and domain dependent. The domain independent approaches are applicable across a variety of domains, but may not be very efficient in any one given domain. The domain dependent approaches need to be (re)designed for each domain separately, but can be very efficient in the domain for which they are designed. One enticing alternative to these approaches is to automatically synthesize domain independent planners given the knowledge about the domain and the theory of planning. In this paper, we investigate the feasibility of using existing automated software synthesis tools to support such synthesis. Specifically, we describe an architecture called CLAY in which the Kestrel Interactive Development System (KIDS) is used to derive a domain-customized planner through a semi-automatic combination of a declarative theory of planning, and the declarative control knowledge specific to a given domain, to semi-automatically combine them to derive domain-customized planners. We discuss what it means to write a declarative theory of planning and control knowledge for KIDS, and illustrate our approach by generating a class of domain-specific planners using state space refinements. Our experiments show that the synthesized planners can outperform classical refinement planners (implemented as instantiations of UCP, Kambhampati & Srivastava, 1995), using the same control knowledge. We will contrast the costs and benefits of the synthesis approach with conventional methods for customizing domain independent planners.",1998-03-01T00:00:00Z,http://arxiv.org/pdf/cs/9803101v1,2024-04-28,
cs/9803102v1,"Cached Sufficient Statistics for Efficient Machine Learning with Large Datasets","This paper introduces new algorithms and data structures for quick counting for machine learning datasets. We focus on the counting task of constructing contingency tables, but our approach is also applicable to counting the number of records in a dataset that match conjunctive queries. Subject to certain assumptions, the costs of these operations can be shown to be independent of the number of records in the dataset and loglinear in the number of non-zero entries in the contingency table. We provide a very sparse data structure, the ADtree, to minimize memory use. We provide analytical worst-case bounds for this structure for several models of data distribution. We empirically demonstrate that tractably-sized data structures can be produced for large real-world datasets by (a) using a sparse tree structure that never allocates memory for counts of zero, (b) never allocating memory for counts that can be deduced from other counts, and (c) not bothering to expand the tree fully near its leaves. We show how the ADtree can be used to accelerate Bayes net structure finding algorithms, rule learning algorithms, and feature selection algorithms, and we provide a number of empirical results comparing ADtree methods against traditional direct counting approaches. We also discuss the possible uses of ADtrees in other machine learning methods, and discuss the merits of ADtrees in comparison with alternative representations such as kd-trees, R-trees and Frequent Sets.",1998-03-01T00:00:00Z,http://arxiv.org/pdf/cs/9803102v1,2024-04-28,
cs/9803103v1,"Tractability of Theory Patching","In this paper we consider the problem of `theory patching', in which we are given a domain theory, some of whose components are indicated to be possibly flawed, and a set of labeled training examples for the domain concept. The theory patching problem is to revise only the indicated components of the theory, such that the resulting theory correctly classifies all the training examples. Theory patching is thus a type of theory revision in which revisions are made to individual components of the theory. Our concern in this paper is to determine for which classes of logical domain theories the theory patching problem is tractable. We consider both propositional and first-order domain theories, and show that the theory patching problem is equivalent to that of determining what information contained in a theory is `stable' regardless of what revisions might be performed to the theory. We show that determining stability is tractable if the input theory satisfies two conditions: that revisions to each theory component have monotonic effects on the classification of examples, and that theory components act independently in the classification of examples in the theory. We also show how the concepts introduced can be used to determine the soundness and completeness of particular theory patching algorithms.",1998-03-01T00:00:00Z,http://arxiv.org/pdf/cs/9803103v1,2024-04-28,
cs/9805101v1,"Integrative Windowing","In this paper we re-investigate windowing for rule learning algorithms. We show that, contrary to previous results for decision tree learning, windowing can in fact achieve significant run-time gains in noise-free domains and explain the different behavior of rule learning algorithms by the fact that they learn each rule independently. The main contribution of this paper is integrative windowing, a new type of algorithm that further exploits this property by integrating good rules into the final theory right after they have been discovered. Thus it avoids re-learning these rules in subsequent iterations of the windowing process. Experimental evidence in a variety of noise-free domains shows that integrative windowing can in fact achieve substantial run-time gains. Furthermore, we discuss the problem of noise in windowing and present an algorithm that is able to achieve run-time gains in a set of experiments in a simple domain with artificial noise.",1998-05-01T00:00:00Z,http://arxiv.org/pdf/cs/9805101v1,2024-04-28,
cs/9806101v1,"Model-Based Diagnosis using Structured System Descriptions","This paper presents a comprehensive approach for model-based diagnosis which includes proposals for characterizing and computing preferred diagnoses, assuming that the system description is augmented with a system structure (a directed graph explicating the interconnections between system components). Specifically, we first introduce the notion of a consequence, which is a syntactically unconstrained propositional sentence that characterizes all consistency-based diagnoses and show that standard characterizations of diagnoses, such as minimal conflicts, correspond to syntactic variations on a consequence. Second, we propose a new syntactic variation on the consequence known as negation normal form (NNF) and discuss its merits compared to standard variations. Third, we introduce a basic algorithm for computing consequences in NNF given a structured system description. We show that if the system structure does not contain cycles, then there is always a linear-size consequence in NNF which can be computed in linear time. For arbitrary system structures, we show a precise connection between the complexity of computing consequences and the topology of the underlying system structure. Finally, we present an algorithm that enumerates the preferred diagnoses characterized by a consequence. The algorithm is shown to take linear time in the size of the consequence if the preference criterion satisfies some general conditions.",1998-06-01T00:00:00Z,http://arxiv.org/pdf/cs/9806101v1,2024-04-28,
cs/9806102v1,"A Selective Macro-learning Algorithm and its Application to the NxN Sliding-Tile Puzzle","One of the most common mechanisms used for speeding up problem solvers is macro-learning. Macros are sequences of basic operators acquired during problem solving. Macros are used by the problem solver as if they were basic operators. The major problem that macro-learning presents is the vast number of macros that are available for acquisition. Macros increase the branching factor of the search space and can severely degrade problem-solving efficiency. To make macro learning useful, a program must be selective in acquiring and utilizing macros. This paper describes a general method for selective acquisition of macros. Solvable training problems are generated in increasing order of difficulty. The only macros acquired are those that take the problem solver out of a local minimum to a better state. The utility of the method is demonstrated in several domains, including the domain of NxN sliding-tile puzzles. After learning on small puzzles, the system is able to efficiently solve puzzles of any size.",1998-06-01T00:00:00Z,http://arxiv.org/pdf/cs/9806102v1,2024-04-28,
cs/9808101v1,"The Computational Complexity of Probabilistic Planning","We examine the computational complexity of testing and finding small plans in probabilistic planning domains with both flat and propositional representations. The complexity of plan evaluation and existence varies with the plan type sought; we examine totally ordered plans, acyclic plans, and looping plans, and partially ordered plans under three natural definitions of plan value. We show that problems of interest are complete for a variety of complexity classes: PL, P, NP, co-NP, PP, NP^PP, co-NP^PP, and PSPACE. In the process of proving that certain planning problems are complete for NP^PP, we introduce a new basic NP^PP-complete problem, E-MAJSAT, which generalizes the standard Boolean satisfiability problem to computations involving probabilistic quantities; our results suggest that the development of good heuristics for E-MAJSAT could be important for the creation of efficient algorithms for a wide variety of problems.",1998-08-01T00:00:00Z,http://arxiv.org/pdf/cs/9808101v1,2024-04-28,
cs/9810016v1,"SYNERGY: A Linear Planner Based on Genetic Programming","In this paper we describe SYNERGY, which is a highly parallelizable, linear planning system that is based on the genetic programming paradigm. Rather than reasoning about the world it is planning for, SYNERGY uses artificial selection, recombination and fitness measure to generate linear plans that solve conjunctive goals. We ran SYNERGY on several domains (e.g., the briefcase problem and a few variants of the robot navigation problem), and the experimental results show that our planner is capable of handling problem instances that are one to two orders of magnitude larger than the ones solved by UCPOP. In order to facilitate the search reduction and to enhance the expressive power of SYNERGY, we also propose two major extensions to our planning system: a formalism for using hierarchical planning operators, and a framework for planning in dynamic environments.",1998-10-16T22:11:35Z,http://arxiv.org/pdf/cs/9810016v1,2024-04-28,
cs/9811024v1,"The Essence of Constraint Propagation","We show that several constraint propagation algorithms (also called (local) consistency, consistency enforcing, Waltz, filtering or narrowing algorithms) are instances of algorithms that deal with chaotic iteration. To this end we propose a simple abstract framework that allows us to classify and compare these algorithms and to establish in a uniform way their basic properties.",1998-11-13T13:04:02Z,http://arxiv.org/pdf/cs/9811024v1,2024-04-28,
cs/9812010v1,"Towards a computational theory of human daydreaming","This paper examines the phenomenon of daydreaming: spontaneously recalling or imagining personal or vicarious experiences in the past or future. The following important roles of daydreaming in human cognition are postulated: plan preparation and rehearsal, learning from failures and successes, support for processes of creativity, emotion regulation, and motivation.   A computational theory of daydreaming and its implementation as the program DAYDREAMER are presented. DAYDREAMER consists of 1) a scenario generator based on relaxed planning, 2) a dynamic episodic memory of experiences used by the scenario generator, 3) a collection of personal goals and control goals which guide the scenario generator, 4) an emotion component in which daydreams initiate, and are initiated by, emotional states arising from goal outcomes, and 5) domain knowledge of interpersonal relations and common everyday occurrences.   The role of emotions and control goals in daydreaming is discussed. Four control goals commonly used in guiding daydreaming are presented: rationalization, failure/success reversal, revenge, and preparation. The role of episodic memory in daydreaming is considered, including how daydreamed information is incorporated into memory and later used. An initial version of DAYDREAMER which produces several daydreams (in English) is currently running.",1998-12-10T16:29:07Z,http://arxiv.org/pdf/cs/9812010v1,2024-04-28,
cs/9812017v1,"A reusable iterative optimization software library to solve combinatorial problems with approximate reasoning","Real world combinatorial optimization problems such as scheduling are typically too complex to solve with exact methods. Additionally, the problems often have to observe vaguely specified constraints of different importance, the available data may be uncertain, and compromises between antagonistic criteria may be necessary. We present a combination of approximate reasoning based constraints and iterative optimization based heuristics that help to model and solve such problems in a framework of C++ software libraries called StarFLIP++. While initially developed to schedule continuous caster units in steel plants, we present in this paper results from reusing the library components in a shift scheduling system for the workforce of an industrial production plant.",1998-12-15T21:45:15Z,http://arxiv.org/pdf/cs/9812017v1,2024-04-28,
cs/9903016v1,"Modeling Belief in Dynamic Systems, Part II: Revision and Update","The study of belief change has been an active area in philosophy and AI. In recent years two special cases of belief change, belief revision and belief update, have been studied in detail. In a companion paper (Friedman & Halpern, 1997), we introduce a new framework to model belief change. This framework combines temporal and epistemic modalities with a notion of plausibility, allowing us to examine the change of beliefs over time. In this paper, we show how belief revision and belief update can be captured in our framework. This allows us to compare the assumptions made by each method, and to better understand the principles underlying them. In particular, it shows that Katsuno and Mendelzon's notion of belief update (Katsuno & Mendelzon, 1991a) depends on several strong assumptions that may limit its applicability in artificial intelligence. Finally, our analysis allow us to identify a notion of minimal change that underlies a broad range of belief change operations including revision and update.",1999-03-24T00:22:01Z,http://arxiv.org/pdf/cs/9903016v1,2024-04-28,
cs/9906002v1,"The Symbol Grounding Problem","How can the semantic interpretation of a formal symbol system be made intrinsic to the system, rather than just parasitic on the meanings in our heads? How can the meanings of the meaningless symbol tokens, manipulated solely on the basis of their (arbitrary) shapes, be grounded in anything but other meaningless symbols? The problem is analogous to trying to learn Chinese from a Chinese/Chinese dictionary alone. A candidate solution is sketched: Symbolic representations must be grounded bottom-up in nonsymbolic representations of two kinds: (1) ""iconic representations,"" which are analogs of the proximal sensory projections of distal objects and events, and (2) ""categorical representations,"" which are learned and innate feature-detectors that pick out the invariant features of object and event categories from their sensory projections. Elementary symbols are the names of these object and event categories, assigned on the basis of their (nonsymbolic) categorical representations. Higher-order (3) ""symbolic representations,"" grounded in these elementary symbols, consist of symbol strings describing category membership relations (e.g., ""An X is a Y that is Z"").",1999-06-01T19:57:24Z,http://arxiv.org/pdf/cs/9906002v1,2024-04-28,
cs/9909003v1,"Iterative Deepening Branch and Bound","In tree search problem the best-first search algorithm needs too much of space . To remove such drawbacks of these algorithms the IDA* was developed which is both space and time cost efficient. But again IDA* can give an optimal solution for real valued problems like Flow shop scheduling, Travelling Salesman and 0/1 Knapsack due to their real valued cost estimates. Thus further modifications are done on it and the Iterative Deepening Branch and Bound Search Algorithms is developed which meets the requirements. We have tried using this algorithm for the Flow Shop Scheduling Problem and have found that it is quite effective.",1999-09-03T10:31:46Z,http://arxiv.org/pdf/cs/9909003v1,2024-04-28,
cs/9910016v1,"Probabilistic Agent Programs","Agents are small programs that autonomously take actions based on changes in their environment or ``state.'' Over the last few years, there have been an increasing number of efforts to build agents that can interact and/or collaborate with other agents. In one of these efforts, Eiter, Subrahmanian amd Pick (AIJ, 108(1-2), pages 179-255) have shown how agents may be built on top of legacy code. However, their framework assumes that agent states are completely determined, and there is no uncertainty in an agent's state. Thus, their framework allows an agent developer to specify how his agents will react when the agent is 100% sure about what is true/false in the world state. In this paper, we propose the concept of a \emph{probabilistic agent program} and show how, given an arbitrary program written in any imperative language, we may build a declarative ``probabilistic'' agent program on top of it which supports decision making in the presence of uncertainty. We provide two alternative semantics for probabilistic agent programs. We show that the second semantics, though more epistemically appealing, is more complex to compute. We provide sound and complete algorithms to compute the semantics of \emph{positive} agent programs.",1999-10-21T09:35:38Z,http://arxiv.org/pdf/cs/9910016v1,2024-04-28,
cs/9911012v2,"Cox's Theorem Revisited","The assumptions needed to prove Cox's Theorem are discussed and examined. Various sets of assumptions under which a Cox-style theorem can be proved are provided, although all are rather strong and, arguably, not natural.",1999-11-27T17:57:17Z,http://arxiv.org/pdf/cs/9911012v2,2024-04-28,
cs/0002002v1,"Uniform semantic treatment of default and autoepistemic logics","We revisit the issue of connections between two leading formalisms in nonmonotonic reasoning: autoepistemic logic and default logic. For each logic we develop a comprehensive semantic framework based on the notion of a belief pair. The set of all belief pairs together with the so called knowledge ordering forms a complete lattice. For each logic, we introduce several semantics by means of fixpoints of operators on the lattice of belief pairs. Our results elucidate an underlying isomorphism of the respective semantic constructions. In particular, we show that the interpretation of defaults as modal formulas proposed by Konolige allows us to represent all semantics for default logic in terms of the corresponding semantics for autoepistemic logic. Thus, our results conclusively establish that default logic can indeed be viewed as a fragment of autoepistemic logic. However, as we also demonstrate, the semantics of Moore and Reiter are given by different operators and occupy different locations in their corresponding families of semantics. This result explains the source of the longstanding difficulty to formally relate these two semantics. In the paper, we also discuss approximating skeptical reasoning with autoepistemic and default logics and establish constructive principles behind such approximations.",2000-02-03T21:44:57Z,http://arxiv.org/pdf/cs/0002002v1,2024-04-28,
cs/0002003v1,"On the accuracy and running time of GSAT","Randomized algorithms for deciding satisfiability were shown to be effective in solving problems with thousands of variables. However, these algorithms are not complete. That is, they provide no guarantee that a satisfying assignment, if one exists, will be found. Thus, when studying randomized algorithms, there are two important characteristics that need to be considered: the running time and, even more importantly, the accuracy --- a measure of likelihood that a satisfying assignment will be found, provided one exists. In fact, we argue that without a reference to the accuracy, the notion of the running time for randomized algorithms is not well-defined. In this paper, we introduce a formal notion of accuracy. We use it to define a concept of the running time. We use both notions to study the random walk strategy GSAT algorithm. We investigate the dependence of accuracy on properties of input formulas such as clause-to-variable ratio and the number of satisfying assignments. We demonstrate that the running time of GSAT grows exponentially in the number of variables of the input formula for randomly generated 3-CNF formulas and for the formulas encoding 3- and 4-colorability of graphs.",2000-02-04T12:53:57Z,http://arxiv.org/pdf/cs/0002003v1,2024-04-28,
cs/0002009v1,"Syntactic Autonomy: Why There is no Autonomy without Symbols and How Self-Organization Might Evolve Them","Two different types of agency are discussed based on dynamically coherent and incoherent couplings with an environment respectively. I propose that until a private syntax (syntactic autonomy) is discovered by dynamically coherent agents, there are no significant or interesting types of closure or autonomy. When syntactic autonomy is established, then, because of a process of description-based selected self-organization, open-ended evolution is enabled. At this stage, agents depend, in addition to dynamics, on localized, symbolic memory, thus adding a level of dynamical incoherence to their interaction with the environment. Furthermore, it is the appearance of syntactic autonomy which enables much more interesting types of closures amongst agents which share the same syntax. To investigate how we can study the emergence of syntax from dynamical systems, experiments with cellular automata leading to emergent computation to solve non-trivial tasks are discussed. RNA editing is also mentioned as a process that may have been used to obtain a primordial biological code necessary open-ended evolution.",2000-02-16T18:09:20Z,http://arxiv.org/pdf/cs/0002009v1,2024-04-28,
cs/0003008v1,"Consistency Management of Normal Logic Program by Top-down Abductive Proof Procedure","This paper presents a method of computing a revision of a function-free normal logic program. If an added rule is inconsistent with a program, that is, if it leads to a situation such that no stable model exists for a new program, then deletion and addition of rules are performed to avoid inconsistency. We specify a revision by translating a normal logic program into an abductive logic program with abducibles to represent deletion and addition of rules. To compute such deletion and addition, we propose an adaptation of our top-down abductive proof procedure to compute a relevant abducibles to an added rule. We compute a minimally revised program, by choosing a minimal set of abducibles among all the sets of abducibles computed by a top-down proof procedure.",2000-03-05T10:29:03Z,http://arxiv.org/pdf/cs/0003008v1,2024-04-28,
cs/0003012v1,"Defeasible Reasoning in OSCAR","This is a system description for the OSCAR defeasible reasoner.",2000-03-06T22:23:00Z,http://arxiv.org/pdf/cs/0003012v1,2024-04-28,
cs/0003016v1,"Abductive and Consistency-Based Diagnosis Revisited: a Modeling Perspective","Diagnostic reasoning has been characterized logically as consistency-based reasoning or abductive reasoning. Previous analyses in the literature have shown, on the one hand, that choosing the (in general more restrictive) abductive definition may be appropriate or not, depending on the content of the knowledge base [Console&Torasso91], and, on the other hand, that, depending on the choice of the definition the same knowledge should be expressed in different form [Poole94].   Since in Model-Based Diagnosis a major problem is finding the right way of abstracting the behavior of the system to be modeled, this paper discusses the relation between modeling, and in particular abstraction in the model, and the notion of diagnosis.",2000-03-07T11:39:53Z,http://arxiv.org/pdf/cs/0003016v1,2024-04-28,
cs/0003020v2,"ACLP: Integrating Abduction and Constraint Solving","ACLP is a system which combines abductive reasoning and constraint solving by integrating the frameworks of Abductive Logic Programming (ALP) and Constraint Logic Programming (CLP). It forms a general high-level knowledge representation environment for abductive problems in Artificial Intelligence and other areas. In ACLP, the task of abduction is supported and enhanced by its non-trivial integration with constraint solving facilitating its application to complex problems. The ACLP system is currently implemented on top of the CLP language of ECLiPSe as a meta-interpreter exploiting its underlying constraint solver for finite domains. It has been applied to the problems of planning and scheduling in order to test its computational effectiveness compared with the direct use of the (lower level) constraint solving framework of CLP on which it is built. These experiments provide evidence that the abductive framework of ACLP does not compromise significantly the computational efficiency of the solutions. Other experiments show the natural ability of ACLP to accommodate easily and in a robust way new or changing requirements of the original problem.",2000-03-07T22:47:13Z,http://arxiv.org/pdf/cs/0003020v2,2024-04-28,
cs/0003021v1,"Relevance Sensitive Non-Monotonic Inference on Belief Sequences","We present a method for relevance sensitive non-monotonic inference from belief sequences which incorporates insights pertaining to prioritized inference and relevance sensitive, inconsistency tolerant belief revision.   Our model uses a finite, logically open sequence of propositional formulas as a representation for beliefs and defines a notion of inference from maxiconsistent subsets of formulas guided by two orderings: a temporal sequencing and an ordering based on relevance relations between the conclusion and formulas in the sequence. The relevance relations are ternary (using context as a parameter) as opposed to standard binary axiomatizations. The inference operation thus defined easily handles iterated revision by maintaining a revision history, blocks the derivation of inconsistent answers from a possibly inconsistent sequence and maintains the distinction between explicit and implicit beliefs. In doing so, it provides a finitely presented formalism and a plausible model of reasoning for automated agents.",2000-03-08T03:03:36Z,http://arxiv.org/pdf/cs/0003021v1,2024-04-28,
cs/0003023v1,"Probabilistic Default Reasoning with Conditional Constraints","We propose a combination of probabilistic reasoning from conditional constraints with approaches to default reasoning from conditional knowledge bases. In detail, we generalize the notions of Pearl's entailment in system Z, Lehmann's lexicographic entailment, and Geffner's conditional entailment to conditional constraints. We give some examples that show that the new notions of z-, lexicographic, and conditional entailment have similar properties like their classical counterparts. Moreover, we show that the new notions of z-, lexicographic, and conditional entailment are proper generalizations of both their classical counterparts and the classical notion of logical entailment for conditional constraints.",2000-03-08T11:05:45Z,http://arxiv.org/pdf/cs/0003023v1,2024-04-28,
cs/0003024v1,"A Compiler for Ordered Logic Programs","This paper describes a system, called PLP, for compiling ordered logic programs into standard logic programs under the answer set semantics. In an ordered logic program, rules are named by unique terms, and preferences among rules are given by a set of dedicated atoms. An ordered logic program is transformed into a second, regular, extended logic program wherein the preferences are respected, in that the answer sets obtained in the transformed theory correspond with the preferred answer sets of the original theory. Since the result of the translation is an extended logic program, existing logic programming systems can be used as underlying reasoning engine. In particular, PLP is conceived as a front-end to the logic programming systems dlv and smodels.",2000-03-08T10:15:51Z,http://arxiv.org/pdf/cs/0003024v1,2024-04-28,
cs/0003027v1,SLDNFA-system,"The SLDNFA-system results from the LP+ project at the K.U.Leuven, which investigates logics and proof procedures for these logics for declarative knowledge representation. Within this project inductive definition logic (ID-logic) is used as representation logic. Different solvers are being developed for this logic and one of these is SLDNFA. A prototype of the system is available and used for investigating how to solve efficiently problems represented in ID-logic.",2000-03-08T13:22:44Z,http://arxiv.org/pdf/cs/0003027v1,2024-04-28,
cs/0003028v1,"Logic Programs with Compiled Preferences","We describe an approach for compiling preferences into logic programs under the answer set semantics. An ordered logic program is an extended logic program in which rules are named by unique terms, and in which preferences among rules are given by a set of dedicated atoms. An ordered logic program is transformed into a second, regular, extended logic program wherein the preferences are respected, in that the answer sets obtained in the transformed theory correspond with the preferred answer sets of the original theory. Our approach allows both the specification of static orderings (as found in most previous work), in which preferences are external to a logic program, as well as orderings on sets of rules. In large part then, we are interested in describing a general methodology for uniformly incorporating preference information in a logic program. Since the result of our translation is an extended logic program, we can make use of existing implementations, such as dlv and smodels. To this end, we have developed a compiler, available on the web, as a front-end for these programming systems.",2000-03-08T14:09:56Z,http://arxiv.org/pdf/cs/0003028v1,2024-04-28,
cs/0003029v1,"Fuzzy Approaches to Abductive Inference","This paper proposes two kinds of fuzzy abductive inference in the framework of fuzzy rule base. The abductive inference processes described here depend on the semantic of the rule. We distinguish two classes of interpretation of a fuzzy rule, certainty generation rules and possible generation rules. In this paper we present the architecture of abductive inference in the first class of interpretation. We give two kinds of problem that we can resolve by using the proposed models of inference.",2000-03-08T14:56:58Z,http://arxiv.org/pdf/cs/0003029v1,2024-04-28,
cs/0003030v1,"Problem solving in ID-logic with aggregates: some experiments","The goal of the LP+ project at the K.U.Leuven is to design an expressive logic, suitable for declarative knowledge representation, and to develop intelligent systems based on Logic Programming technology for solving computational problems using the declarative specifications. The ID-logic is an integration of typed classical logic and a definition logic. Different abductive solvers for this language are being developed. This paper is a report of the integration of high order aggregates into ID-logic and the consequences on the solver SLDNFA.",2000-03-08T15:39:14Z,http://arxiv.org/pdf/cs/0003030v1,2024-04-28,
cs/0003031v1,"Optimal Belief Revision","We propose a new approach to belief revision that provides a way to change knowledge bases with a minimum of effort. We call this way of revising belief states optimal belief revision. Our revision method gives special attention to the fact that most belief revision processes are directed to a specific informational objective. This approach to belief change is founded on notions such as optimal context and accessibility. For the sentential model of belief states we provide both a formal description of contexts as sub-theories determined by three parameters and a method to construct contexts. Next, we introduce an accessibility ordering for belief sets, which we then use for selecting the best (optimal) contexts with respect to the processing effort involved in the revision. Then, for finitely axiomatizable knowledge bases, we characterize a finite accessibility ranking from which the accessibility ordering for the entire base is generated and show how to determine the ranking of an arbitrary sentence in the language. Finally, we define the adjustment of the accessibility ranking of a revised base of a belief set.",2000-03-08T15:54:50Z,http://arxiv.org/pdf/cs/0003031v1,2024-04-28,
cs/0003032v1,"cc-Golog: Towards More Realistic Logic-Based Robot Controllers","High-level robot controllers in realistic domains typically deal with processes which operate concurrently, change the world continuously, and where the execution of actions is event-driven as in ``charge the batteries as soon as the voltage level is low''. While non-logic-based robot control languages are well suited to express such scenarios, they fare poorly when it comes to projecting, in a conspicuous way, how the world evolves when actions are executed. On the other hand, a logic-based control language like \congolog, based on the situation calculus, is well-suited for the latter. However, it has problems expressing event-driven behavior. In this paper, we show how these problems can be overcome by first extending the situation calculus to support continuous change and event-driven behavior and then presenting \ccgolog, a variant of \congolog which is based on the extended situation calculus. One benefit of \ccgolog is that it narrows the gap in expressiveness compared to non-logic-based control languages while preserving a semantically well-founded projection mechanism.",2000-03-08T16:14:08Z,http://arxiv.org/pdf/cs/0003032v1,2024-04-28,
cs/0003033v1,"Smodels: A System for Answer Set Programming","The Smodels system implements the stable model semantics for normal logic programs. It handles a subclass of programs which contain no function symbols and are domain-restricted but supports extensions including built-in functions as well as cardinality and weight constraints. On top of this core engine more involved systems can be built. As an example, we have implemented total and partial stable model computation for disjunctive logic programs. An interesting application method is based on answer set programming, i.e., encoding an application problem as a set of rules so that its solutions are captured by the stable models of the rules. Smodels has been applied to a number of areas including planning, model checking, reachability analysis, product configuration, dynamic constraint satisfaction, and feature interaction.",2000-03-08T23:25:51Z,http://arxiv.org/pdf/cs/0003033v1,2024-04-28,
cs/0003034v2,"E-RES: A System for Reasoning about Actions, Events and Observations","E-RES is a system that implements the Language E, a logic for reasoning about narratives of action occurrences and observations. E's semantics is model-theoretic, but this implementation is based on a sound and complete reformulation of E in terms of argumentation, and uses general computational techniques of argumentation frameworks. The system derives sceptical non-monotonic consequences of a given reformulated theory which exactly correspond to consequences entailed by E's model-theory. The computation relies on a complimentary ability of the system to derive credulous non-monotonic consequences together with a set of supporting assumptions which is sufficient for the (credulous) conclusion to hold. E-RES allows theories to contain general action laws, statements about action occurrences, observations and statements of ramifications (or universal laws). It is able to derive consequences both forward and backward in time. This paper gives a short overview of the theoretical basis of E-RES and illustrates its use on a variety of examples. Currently, E-RES is being extended so that the system can be used for planning.",2000-03-08T16:18:52Z,http://arxiv.org/pdf/cs/0003034v2,2024-04-28,
cs/0003037v1,"QUIP - A Tool for Computing Nonmonotonic Reasoning Tasks","In this paper, we outline the prototype of an automated inference tool, called QUIP, which provides a uniform implementation for several nonmonotonic reasoning formalisms. The theoretical basis of QUIP is derived from well-known results about the computational complexity of nonmonotonic logics and exploits a representation of the different reasoning tasks in terms of quantified boolean formulae.",2000-03-08T17:18:08Z,http://arxiv.org/pdf/cs/0003037v1,2024-04-28,
cs/0003038v1,"A Splitting Set Theorem for Epistemic Specifications","Over the past decade a considerable amount of research has been done to expand logic programming languages to handle incomplete information. One such language is the language of epistemic specifications. As is usual with logic programming languages, the problem of answering queries is intractable in the general case. For extended disjunctive logic programs, an idea that has proven useful in simplifying the investigation of answer sets is the use of splitting sets. In this paper we will present an extended definition of splitting sets that will be applicable to epistemic specifications. Furthermore, an extension of the splitting set theorem will be presented. Also, a characterization of stratified epistemic specifications will be given in terms of splitting sets. This characterization leads us to an algorithmic method of computing world views of a subclass of epistemic logic programs.",2000-03-08T20:40:31Z,http://arxiv.org/pdf/cs/0003038v1,2024-04-28,
cs/0003039v1,"DES: a Challenge Problem for Nonmonotonic Reasoning Systems","The US Data Encryption Standard, DES for short, is put forward as an interesting benchmark problem for nonmonotonic reasoning systems because (i) it provides a set of test cases of industrial relevance which shares features of randomly generated problems and real-world problems, (ii) the representation of DES using normal logic programs with the stable model semantics is simple and easy to understand, and (iii) this subclass of logic programs can be seen as an interesting special case for many other formalizations of nonmonotonic reasoning. In this paper we present two encodings of DES as logic programs: a direct one out of the standard specifications and an optimized one extending the work of Massacci and Marraro. The computational properties of the encodings are studied by using them for DES key search with the Smodels system as the implementation of the stable model semantics. Results indicate that the encodings and Smodels are quite competitive: they outperform state-of-the-art SAT-checkers working with an optimized encoding of DES into SAT and are comparable with a SAT-checker that is customized and tuned for the optimized SAT encoding.",2000-03-08T21:49:57Z,http://arxiv.org/pdf/cs/0003039v1,2024-04-28,
cs/0003042v1,"Fages' Theorem and Answer Set Programming","We generalize a theorem by Francois Fages that describes the relationship between the completion semantics and the answer set semantics for logic programs with negation as failure. The study of this relationship is important in connection with the emergence of answer set programming. Whenever the two semantics are equivalent, answer sets can be computed by a satisfiability solver, and the use of answer set solvers such as smodels and dlv is unnecessary. A logic programming representation of the blocks world due to Ilkka Niemelae is discussed as an example.",2000-03-09T00:28:21Z,http://arxiv.org/pdf/cs/0003042v1,2024-04-28,
cs/0003044v1,"On the tractable counting of theory models and its application to belief revision and truth maintenance","We introduced decomposable negation normal form (DNNF) recently as a tractable form of propositional theories, and provided a number of powerful logical operations that can be performed on it in polynomial time. We also presented an algorithm for compiling any conjunctive normal form (CNF) into DNNF and provided a structure-based guarantee on its space and time complexity. We present in this paper a linear-time algorithm for converting an ordered binary decision diagram (OBDD) representation of a propositional theory into an equivalent DNNF, showing that DNNFs scale as well as OBDDs. We also identify a subclass of DNNF which we call deterministic DNNF, d-DNNF, and show that the previous complexity guarantees on compiling DNNF continue to hold for this stricter subclass, which has stronger properties. In particular, we present a new operation on d-DNNF which allows us to count its models under the assertion, retraction and flipping of every literal by traversing the d-DNNF twice. That is, after such traversal, we can test in constant-time: the entailment of any literal by the d-DNNF, and the consistency of the d-DNNF under the retraction or flipping of any literal. We demonstrate the significance of these new operations by showing how they allow us to implement linear-time, complete truth maintenance systems and linear-time, complete belief revision systems for two important classes of propositional theories.",2000-03-09T08:58:15Z,http://arxiv.org/pdf/cs/0003044v1,2024-04-28,
cs/0003047v1,"BDD-based reasoning in the fluent calculus - first results","The paper reports on first preliminary results and insights gained in a project aiming at implementing the fluent calculus using methods and techniques based on binary decision diagrams. After reporting on an initial experiment showing promising results we discuss our findings concerning various techniques and heuristics used to speed up the reasoning process.",2000-03-09T17:18:12Z,http://arxiv.org/pdf/cs/0003047v1,2024-04-28,
cs/0003049v1,"Planning with Incomplete Information","Planning is a natural domain of application for frameworks of reasoning about actions and change. In this paper we study how one such framework, the Language E, can form the basis for planning under (possibly) incomplete information. We define two types of plans: weak and safe plans, and propose a planner, called the E-Planner, which is often able to extend an initial weak plan into a safe plan even though the (explicit) information available is incomplete, e.g. for cases where the initial state is not completely known. The E-Planner is based upon a reformulation of the Language E in argumentation terms and a natural proof theory resulting from the reformulation. It uses an extension of this proof theory by means of abduction for the generation of plans and adopts argumentation-based techniques for extending weak plans into safe plans. We provide representative examples illustrating the behaviour of the E-Planner, in particular for cases where the status of fluents is incompletely known.",2000-03-09T22:30:27Z,http://arxiv.org/pdf/cs/0003049v1,2024-04-28,
cs/0003051v1,"Local Diagnosis","In an earlier work, we have presented operations of belief change which only affect the relevant part of a belief base. In this paper, we propose the application of the same strategy to the problem of model-based diangosis. We first isolate the subset of the system description which is relevant for a given observation and then solve the diagnosis problem for this subset.",2000-03-10T22:54:55Z,http://arxiv.org/pdf/cs/0003051v1,2024-04-28,
cs/0003052v3,"A Consistency-Based Model for Belief Change: Preliminary Report","We present a general, consistency-based framework for belief change. Informally, in revising K by A, we begin with A and incorporate as much of K as consistently possible. Formally, a knowledge base K and sentence A are expressed, via renaming propositions in K, in separate languages. Using a maximization process, we assume the languages are the same insofar as consistently possible. Lastly, we express the resultant knowledge base in a single language. There may be more than one way in which A can be so extended by K: in choice revision, one such ``extension'' represents the revised state; alternately revision consists of the intersection of all such extensions.   The most general formulation of our approach is flexible enough to express other approaches to revision and update, the merging of knowledge bases, and the incorporation of static and dynamic integrity constraints. Our framework differs from work based on ordinal conditional functions, notably with respect to iterated revision. We argue that the approach is well-suited for implementation: the choice revision operator gives better complexity results than general revision; the approach can be expressed in terms of a finite knowledge base; and the scope of a revision can be restricted to just those propositions mentioned in the sentence for revision A.",2000-03-11T06:29:02Z,http://arxiv.org/pdf/cs/0003052v3,2024-04-28,
cs/0003059v1,"SATEN: An Object-Oriented Web-Based Revision and Extraction Engine","SATEN is an object-oriented web-based extraction and belief revision engine. It runs on any computer via a Java 1.1 enabled browser such as Netscape 4. SATEN performs belief revision based on the AGM approach. The extraction and belief revision reasoning engines operate on a user specified ranking of information. One of the features of SATEN is that it can be used to integrate mutually inconsistent commensuate rankings into a consistent ranking.",2000-03-14T04:58:18Z,http://arxiv.org/pdf/cs/0003059v1,2024-04-28,
cs/0003061v1,"dcs: An Implementation of DATALOG with Constraints","Answer-set programming (ASP) has emerged recently as a viable programming paradigm. We describe here an ASP system, DATALOG with constraints or DC, based on non-monotonic logic. Informally, DC theories consist of propositional clauses (constraints) and of Horn rules. The semantics is a simple and natural extension of the semantics of the propositional logic. However, thanks to the presence of Horn rules in the system, modeling of transitive closure becomes straightforward. We describe the syntax, use and implementation of DC and provide experimental results.",2000-03-14T18:06:38Z,http://arxiv.org/pdf/cs/0003061v1,2024-04-28,
cs/0003077v1,"DATALOG with constraints - an answer-set programming system","Answer-set programming (ASP) has emerged recently as a viable programming paradigm well attuned to search problems in AI, constraint satisfaction and combinatorics. Propositional logic is, arguably, the simplest ASP system with an intuitive semantics supporting direct modeling of problem constraints. However, for some applications, especially those requiring that transitive closure be computed, it requires additional variables and results in large theories. Consequently, it may not be a practical computational tool for such problems. On the other hand, ASP systems based on nonmonotonic logics, such as stable logic programming, can handle transitive closure computation efficiently and, in general, yield very concise theories as problem representations. Their semantics is, however, more complex. Searching for the middle ground, in this paper we introduce a new nonmonotonic logic, DATALOG with constraints or DC. Informally, DC theories consist of propositional clauses (constraints) and of Horn rules. The semantics is a simple and natural extension of the semantics of the propositional logic. However, thanks to the presence of Horn rules in the system, modeling of transitive closure becomes straightforward. We describe the syntax and semantics of DC, and study its properties. We discuss an implementation of DC and present results of experimental study of the effectiveness of DC, comparing it with CSAT, a satisfiability checker and SMODELS implementation of stable logic programming. Our results show that DC is competitive with the other two approaches, in case of many search problems, often yielding much more efficient solutions.",2000-03-24T19:09:59Z,http://arxiv.org/pdf/cs/0003077v1,2024-04-28,
cs/0003080v1,"Some Remarks on Boolean Constraint Propagation","We study here the well-known propagation rules for Boolean constraints. First we propose a simple notion of completeness for sets of such rules and establish a completeness result. Then we show an equivalence in an appropriate sense between Boolean constraint propagation and unit propagation, a form of resolution for propositional logic.   Subsequently we characterize one set of such rules by means of the notion of hyper-arc consistency introduced in (Mohr and Masini 1988). Also, we clarify the status of a similar, though different, set of rules introduced in (Simonis 1989a) and more fully in (Codognet and Diaz 1996).",2000-03-28T11:49:37Z,http://arxiv.org/pdf/cs/0003080v1,2024-04-28,
cs/0005031v3,"Conditional Plausibility Measures and Bayesian Networks","A general notion of algebraic conditional plausibility measures is defined. Probability measures, ranking functions, possibility measures, and (under the appropriate definitions) sets of probability measures can all be viewed as defining algebraic conditional plausibility measures. It is shown that algebraic conditional plausibility measures can be represented using Bayesian networks.",2000-05-30T19:05:21Z,http://arxiv.org/pdf/cs/0005031v3,2024-04-28,
cs/0006043v1,"Constraint compiling into rules formalism constraint compiling into rules formalism for dynamic CSPs computing","In this paper we present a rule based formalism for filtering variables domains of constraints. This formalism is well adapted for solving dynamic CSP. We take diagnosis as an instance problem to illustrate the use of these rules. A diagnosis problem is seen like finding all the minimal sets of constraints to be relaxed in the constraint network that models the device to be diagnosed",2000-06-30T10:25:06Z,http://arxiv.org/pdf/cs/0006043v1,2024-04-28,
cs/0007004v1,"Brainstorm/J: a Java Framework for Intelligent Agents","Despite the effort of many researchers in the area of multi-agent systems (MAS) for designing and programming agents, a few years ago the research community began to take into account that common features among different MAS exists. Based on these common features, several tools have tackled the problem of agent development on specific application domains or specific types of agents. As a consequence, their scope is restricted to a subset of the huge application domain of MAS. In this paper we propose a generic infrastructure for programming agents whose name is Brainstorm/J. The infrastructure has been implemented as an object oriented framework. As a consequence, our approach supports a broader scope of MAS applications than previous efforts, being flexible and reusable.",2000-07-04T16:31:40Z,http://arxiv.org/pdf/cs/0007004v1,2024-04-28,
cs/0010037v1,"On the relationship between fuzzy logic and four-valued relevance logic","In fuzzy propositional logic, to a proposition a partial truth in [0,1] is assigned. It is well known that under certain circumstances, fuzzy logic collapses to classical logic. In this paper, we will show that under dual conditions, fuzzy logic collapses to four-valued (relevance) logic, where propositions have truth-value true, false, unknown, or contradiction. As a consequence, fuzzy entailment may be considered as ``in between'' four-valued (relevance) entailment and classical entailment.",2000-10-31T14:14:26Z,http://arxiv.org/pdf/cs/0010037v1,2024-04-28,
cs/0011012v3,"Causes and Explanations: A Structural-Model Approach, Part I: Causes","We propose a new definition of actual cause, using structural equations to model counterfactuals. We show that the definition yields a plausible and elegant account of causation that handles well examples which have caused problems for other definitions and resolves major difficulties in the traditional account.",2000-11-07T23:21:38Z,http://arxiv.org/pdf/cs/0011012v3,2024-04-28,
cs/0011030v1,"Logic Programming Approaches for Representing and Solving Constraint Satisfaction Problems: A Comparison","Many logic programming based approaches can be used to describe and solve combinatorial search problems. On the one hand there is constraint logic programming which computes a solution as an answer substitution to a query containing the variables of the constraint satisfaction problem. On the other hand there are systems based on stable model semantics, abductive systems, and first order logic model generators which compute solutions as models of some theory. This paper compares these different approaches from the point of view of knowledge representation (how declarative are the programs) and from the point of view of performance (how good are they at solving typical problems).",2000-11-21T13:56:21Z,http://arxiv.org/pdf/cs/0011030v1,2024-04-28,
cs/0105022v1,"Multi-Channel Parallel Adaptation Theory for Rule Discovery","In this paper, we introduce a new machine learning theory based on multi-channel parallel adaptation for rule discovery. This theory is distinguished from the familiar parallel-distributed adaptation theory of neural networks in terms of channel-based convergence to the target rules. We show how to realize this theory in a learning system named CFRule. CFRule is a parallel weight-based model, but it departs from traditional neural computing in that its internal knowledge is comprehensible. Furthermore, when the model converges upon training, each channel converges to a target rule. The model adaptation rule is derived by multi-level parallel weight optimization based on gradient descent. Since, however, gradient descent only guarantees local optimization, a multi-channel regression-based optimization strategy is developed to effectively deal with this problem. Formally, we prove that the CFRule model can explicitly and precisely encode any given rule set. Also, we prove a property related to asynchronous parallel convergence, which is a critical element of the multi-channel parallel adaptation theory for rule learning. Thanks to the quantizability nature of the CFRule model, rules can be extracted completely and soundly via a threshold-based mechanism. Finally, the practical application of the theory is demonstrated in DNA promoter recognition and hepatitis prognosis prediction.",2001-05-11T14:17:42Z,http://arxiv.org/pdf/cs/0105022v1,2024-04-28,
cs/0106006v1,"A Constraint-Driven System for Contract Assembly","We present an approach for modelling the structure and coarse content of legal documents with a view to providing automated support for the drafting of contracts and contract database retrieval. The approach is designed to be applicable where contract drafting is based on model-form contracts or on existing examples of a similar type. The main features of the approach are: (1) the representation addresses the structure and the interrelationships between the constituent parts of contracts, but not the text of the document itself; (2) the representation of documents is separated from the mechanisms that manipulate it; and (3) the drafting process is subject to a collection of explicitly stated constraints that govern the structure of the documents. We describe the representation of document instances and of 'generic documents', which are data structures used to drive the creation of new document instances, and we show extracts from a sample session to illustrate the features of a prototype system implemented in MacProlog.",2001-06-07T14:27:30Z,http://arxiv.org/pdf/cs/0106006v1,2024-04-28,
cs/0106007v1,"Modelling Contractual Arguments","One influential approach to assessing the ""goodness"" of arguments is offered by the Pragma-Dialectical school (p-d) (Eemeren & Grootendorst 1992). This can be compared with Rhetorical Structure Theory (RST) (Mann & Thompson 1988), an approach that originates in discourse analysis. In p-d terms an argument is good if it avoids committing a fallacy, whereas in RST terms an argument is good if it is coherent. RST has been criticised (Snoeck Henkemans 1997) for providing only a partially functional account of argument, and similar criticisms have been raised in the Natural Language Generation (NLG) community-particularly by Moore & Pollack (1992)- with regards to its account of intentionality in text in general. Mann and Thompson themselves note that although RST can be successfully applied to a wide range of texts from diverse domains, it fails to characterise some types of text, most notably legal contracts. There is ongoing research in the Artificial Intelligence and Law community exploring the potential for providing electronic support to contract negotiators, focusing on long-term, complex engineering agreements (see for example Daskalopulu & Sergot 1997). This paper provides a brief introduction to RST and illustrates its shortcomings with respect to contractual text. An alternative approach for modelling argument structure is presented which not only caters for contractual text, but also overcomes the aforementioned limitations of RST.",2001-06-07T14:40:04Z,http://arxiv.org/pdf/cs/0106007v1,2024-04-28,
cs/0106025v1,"Information Integration and Computational Logic","Information Integration is a young and exciting field with enormous research and commercial significance in the new world of the Information Society. It stands at the crossroad of Databases and Artificial Intelligence requiring novel techniques that bring together different methods from these fields. Information from disparate heterogeneous sources often with no a-priori common schema needs to be synthesized in a flexible, transparent and intelligent way in order to respond to the demands of a query thus enabling a more informed decision by the user or application program. The field although relatively young has already found many practical applications particularly for integrating information over the World Wide Web. This paper gives a brief introduction of the field highlighting some of the main current and future research issues and application areas. It attempts to evaluate the current and potential role of Computational Logic in this and suggests some of the problems where logic-based techniques could be used.",2001-06-11T20:00:04Z,http://arxiv.org/pdf/cs/0106025v1,2024-04-28,
cs/0107002v1,"Enhancing Constraint Propagation with Composition Operators","Constraint propagation is a general algorithmic approach for pruning the search space of a CSP. In a uniform way, K. R. Apt has defined a computation as an iteration of reduction functions over a domain. He has also demonstrated the need for integrating static properties of reduction functions (commutativity and semi-commutativity) to design specialized algorithms such as AC3 and DAC. We introduce here a set of operators for modeling compositions of reduction functions. Two of the major goals are to tackle parallel computations, and dynamic behaviours (such as slow convergence).",2001-07-02T08:08:39Z,http://arxiv.org/pdf/cs/0107002v1,2024-04-28,
cs/0109006v1,"On Properties of Update Sequences Based on Causal Rejection","We consider an approach to update nonmonotonic knowledge bases represented as extended logic programs under answer set semantics. New information is incorporated into the current knowledge base subject to a causal rejection principle enforcing that, in case of conflicts, more recent rules are preferred and older rules are overridden. Such a rejection principle is also exploited in other approaches to update logic programs, e.g., in dynamic logic programming by Alferes et al. We give a thorough analysis of properties of our approach, to get a better understanding of the causal rejection principle. We review postulates for update and revision operators from the area of theory change and nonmonotonic reasoning, and some new properties are considered as well. We then consider refinements of our semantics which incorporate a notion of minimality of change. As well, we investigate the relationship to other approaches, showing that our approach is semantically equivalent to inheritance programs by Buccafurri et al. and that it coincides with certain classes of dynamic logic programs, for which we provide characterizations in terms of graph conditions. Therefore, most of our results about properties of causal rejection principle apply to these approaches as well. Finally, we deal with computational complexity of our approach, and outline how the update semantics and its refinements can be implemented on top of existing logic programming engines.",2001-09-05T09:19:34Z,http://arxiv.org/pdf/cs/0109006v1,2024-04-28,
cs/0111060v1,"Gradient-based Reinforcement Planning in Policy-Search Methods","We introduce a learning method called ``gradient-based reinforcement planning'' (GREP). Unlike traditional DP methods that improve their policy backwards in time, GREP is a gradient-based method that plans ahead and improves its policy before it actually acts in the environment. We derive formulas for the exact policy gradient that maximizes the expected future reward and confirm our ideas with numerical experiments.",2001-11-28T13:43:13Z,http://arxiv.org/pdf/cs/0111060v1,2024-04-28,
cs/0112015v1,"Rational Competitive Analysis","Much work in computer science has adopted competitive analysis as a tool for decision making under uncertainty. In this work we extend competitive analysis to the context of multi-agent systems. Unlike classical competitive analysis where the behavior of an agent's environment is taken to be arbitrary, we consider the case where an agent's environment consists of other agents. These agents will usually obey some (minimal) rationality constraints. This leads to the definition of rational competitive analysis. We introduce the concept of rational competitive analysis, and initiate the study of competitive analysis for multi-agent systems. We also discuss the application of rational competitive analysis to the context of bidding games, as well as to the classical one-way trading problem.",2001-12-13T00:46:10Z,http://arxiv.org/pdf/cs/0112015v1,2024-04-28,
cs/0201022v2,"A theory of experiment","This article aims at clarifying the language and practice of scientific experiment, mainly by hooking observability on calculability.",2002-01-23T19:53:08Z,http://arxiv.org/pdf/cs/0201022v2,2024-04-28,
cs/0202021v1,"Nonmonotonic Reasoning, Preferential Models and Cumulative Logics","Many systems that exhibit nonmonotonic behavior have been described and studied already in the literature. The general notion of nonmonotonic reasoning, though, has almost always been described only negatively, by the property it does not enjoy, i.e. monotonicity. We study here general patterns of nonmonotonic reasoning and try to isolate properties that could help us map the field of nonmonotonic reasoning by reference to positive properties. We concentrate on a number of families of nonmonotonic consequence relations, defined in the style of Gentzen. Both proof-theoretic and semantic points of view are developed in parallel. The former point of view was pioneered by D. Gabbay, while the latter has been advocated by Y. Shoham in. Five such families are defined and characterized by representation theorems, relating the two points of view. One of the families of interest, that of preferential relations, turns out to have been studied by E. Adams. The ""preferential"" models proposed here are a much stronger tool than Adams' probabilistic semantics. The basic language used in this paper is that of propositional logic. The extension of our results to first order predicate calculi and the study of the computational complexity of the decision problems described in this paper will be treated in another paper.",2002-02-18T10:29:54Z,http://arxiv.org/pdf/cs/0202021v1,2024-04-28,
cs/0202022v1,"What does a conditional knowledge base entail?","This paper presents a logical approach to nonmonotonic reasoning based on the notion of a nonmonotonic consequence relation. A conditional knowledge base, consisting of a set of conditional assertions of the type ""if ... then ..."", represents the explicit defeasible knowledge an agent has about the way the world generally behaves. We look for a plausible definition of the set of all conditional assertions entailed by a conditional knowledge base. In a previous paper, S. Kraus and the authors defined and studied ""preferential"" consequence relations. They noticed that not all preferential relations could be considered as reasonable inference procedures. This paper studies a more restricted class of consequence relations, ""rational"" relations. It is argued that any reasonable nonmonotonic inference procedure should define a rational relation. It is shown that the rational relations are exactly those that may be represented by a ""ranked"" preferential model, or by a (non-standard) probabilistic model. The rational closure of a conditional knowledge base is defined and shown to provide an attractive answer to the question of the title. Global properties of this closure operation are proved: it is a cumulative operation. It is also computationally tractable. This paper assumes the underlying language is propositional.",2002-02-18T12:43:18Z,http://arxiv.org/pdf/cs/0202022v1,2024-04-28,
cs/0202024v1,"A note on Darwiche and Pearl","It is shown that Darwiche and Pearl's postulates imply an interesting property, not noticed by the authors.",2002-02-18T15:23:06Z,http://arxiv.org/pdf/cs/0202024v1,2024-04-28,
cs/0202025v1,"Distance Semantics for Belief Revision","A vast and interesting family of natural semantics for belief revision is defined. Suppose one is given a distance d between any two models. One may then define the revision of a theory K by a formula a as the theory defined by the set of all those models of a that are closest, by d, to the set of models of K. This family is characterized by a set of rationality postulates that extends the AGM postulates. The new postulates describe properties of iterated revisions.",2002-02-18T15:36:46Z,http://arxiv.org/pdf/cs/0202025v1,2024-04-28,
cs/0202026v1,"Preferred History Semantics for Iterated Updates","We give a semantics to iterated update by a preference relation on possible developments. An iterated update is a sequence of formulas, giving (incomplete) information about successive states of the world. A development is a sequence of models, describing a possible trajectory through time. We assume a principle of inertia and prefer those developments, which are compatible with the information, and avoid unnecessary changes. The logical properties of the updates defined in this way are considered, and a representation result is proved.",2002-02-18T15:53:56Z,http://arxiv.org/pdf/cs/0202026v1,2024-04-28,
cs/0202031v1,"Nonmonotonic inference operations","A. Tarski proposed the study of infinitary consequence operations as the central topic of mathematical logic. He considered monotonicity to be a property of all such operations. In this paper, we weaken the monotonicity requirement and consider more general operations, inference operations. These operations describe the nonmonotonic logics both humans and machines seem to be using when infering defeasible information from incomplete knowledge. We single out a number of interesting families of inference operations. This study of infinitary inference operations is inspired by the results of Kraus, Lehmann and Magidor on finitary nonmonotonic operations, but this paper is self-contained.",2002-02-20T14:19:42Z,http://arxiv.org/pdf/cs/0202031v1,2024-04-28,
cs/0202033v1,"The logical meaning of Expansion","The Expansion property considered by researchers in Social Choice is shown to correspond to a logical property of nonmonotonic consequence relations that is the {\em pure}, i.e., not involving connectives, version of a previously known weak rationality condition. The assumption that the union of two definable sets of models is definable is needed for the soundness part of the result.",2002-02-20T14:50:49Z,http://arxiv.org/pdf/cs/0202033v1,2024-04-28,
cs/0203002v1,"Another perspective on Default Reasoning","The lexicographic closure of any given finite set D of normal defaults is defined. A conditional assertion ""if a then b"" is in this lexicographic closure if, given the defaults D and the fact a, one would conclude b. The lexicographic closure is essentially a rational extension of D, and of its rational closure, defined in a previous paper. It provides a logic of normal defaults that is different from the one proposed by R. Reiter and that is rich enough not to require the consideration of non-normal defaults. A large number of examples are provided to show that the lexicographic closure corresponds to the basic intuitions behind Reiter's logic of defaults.",2002-03-01T11:06:49Z,http://arxiv.org/pdf/cs/0203002v1,2024-04-28,
cs/0203003v1,"Deductive Nonmonotonic Inference Operations: Antitonic Representations","We provide a characterization of those nonmonotonic inference operations C for which C(X) may be described as the set of all logical consequences of X together with some set of additional assumptions S(X) that depends anti-monotonically on X (i.e., X is a subset of Y implies that S(Y) is a subset of S(X)). The operations represented are exactly characterized in terms of properties most of which have been studied in Freund-Lehmann(cs.AI/0202031). Similar characterizations of right-absorbing and cumulative operations are also provided. For cumulative operations, our results fit in closely with those of Freund. We then discuss extending finitary operations to infinitary operations in a canonical way and discuss co-compactness properties. Our results provide a satisfactory notion of pseudo-compactness, generalizing to deductive nonmonotonic operations the notion of compactness for monotonic operations. They also provide an alternative, more elegant and more general, proof of the existence of an infinitary deductive extension for any finitary deductive operation (Theorem 7.9 of Freund-Lehmann).",2002-03-01T11:20:59Z,http://arxiv.org/pdf/cs/0203003v1,2024-04-28,
cs/0203004v1,"Stereotypical Reasoning: Logical Properties","Stereotypical reasoning assumes that the situation at hand is one of a kind and that it enjoys the properties generally associated with that kind of situation. It is one of the most basic forms of nonmonotonic reasoning. A formal model for stereotypical reasoning is proposed and the logical properties of this form of reasoning are studied. Stereotypical reasoning is shown to be cumulative under weak assumptions.",2002-03-04T08:57:54Z,http://arxiv.org/pdf/cs/0203004v1,2024-04-28,
cs/0203005v2,"A Framework for Compiling Preferences in Logic Programs","We introduce a methodology and framework for expressing general preference information in logic programming under the answer set semantics. An ordered logic program is an extended logic program in which rules are named by unique terms, and in which preferences among rules are given by a set of atoms of form s < t where s and t are names. An ordered logic program is transformed into a second, regular, extended logic program wherein the preferences are respected, in that the answer sets obtained in the transformed program correspond with the preferred answer sets of the original program. Our approach allows the specification of dynamic orderings, in which preferences can appear arbitrarily within a program. Static orderings (in which preferences are external to a logic program) are a trivial restriction of the general dynamic case. First, we develop a specific approach to reasoning with preferences, wherein the preference ordering specifies the order in which rules are to be applied. We then demonstrate the wide range of applicability of our framework by showing how other approaches, among them that of Brewka and Eiter, can be captured within our framework. Since the result of each of these transformations is an extended logic program, we can make use of existing implementations, such as dlv and smodels. To this end, we have developed a publicly available compiler as a front-end for these programming systems.",2002-03-04T13:00:41Z,http://arxiv.org/pdf/cs/0203005v2,2024-04-28,
cs/0203007v1,"Two results for proiritized logic programming","Prioritized default reasoning has illustrated its rich expressiveness and flexibility in knowledge representation and reasoning. However, many important aspects of prioritized default reasoning have yet to be thoroughly explored. In this paper, we investigate two properties of prioritized logic programs in the context of answer set semantics. Specifically, we reveal a close relationship between mutual defeasibility and uniqueness of the answer set for a prioritized logic program. We then explore how the splitting technique for extended logic programs can be extended to prioritized logic programs. We prove splitting theorems that can be used to simplify the evaluation of a prioritized logic program under certain conditions.",2002-03-05T00:28:04Z,http://arxiv.org/pdf/cs/0203007v1,2024-04-28,
cs/0204032v1,"Belief Revision and Rational Inference","The (extended) AGM postulates for belief revision seem to deal with the revision of a given theory K by an arbitrary formula, but not to constrain the revisions of two different theories by the same formula. A new postulate is proposed and compared with other similar postulates that have been proposed in the literature. The AGM revisions that satisfy this new postulate stand in one-to-one correspondence with the rational, consistency-preserving relations. This correspondence is described explicitly. Two viewpoints on iterative revisions are distinguished and discussed.",2002-04-14T09:22:42Z,http://arxiv.org/pdf/cs/0204032v1,2024-04-28,
cs/0205014v1,"Ultimate approximations in nonmonotonic knowledge representation systems","We study fixpoints of operators on lattices. To this end we introduce the notion of an approximation of an operator. We order approximations by means of a precision ordering. We show that each lattice operator O has a unique most precise or ultimate approximation. We demonstrate that fixpoints of this ultimate approximation provide useful insights into fixpoints of the operator O.   We apply our theory to logic programming and introduce the ultimate Kripke-Kleene, well-founded and stable semantics. We show that the ultimate Kripke-Kleene and well-founded semantics are more precise then their standard counterparts We argue that ultimate semantics for logic programming have attractive epistemological properties and that, while in general they are computationally more complex than the standard semantics, for many classes of theories, their complexity is no worse.",2002-05-11T20:44:16Z,http://arxiv.org/pdf/cs/0205014v1,2024-04-28,
cs/0206003v1,"Handling Defeasibilities in Action Domains","Representing defeasibility is an important issue in common sense reasoning. In reasoning about action and change, this issue becomes more difficult because domain and action related defeasible information may conflict with general inertia rules. Furthermore, different types of defeasible information may also interfere with each other during the reasoning. In this paper, we develop a prioritized logic programming approach to handle defeasibilities in reasoning about action. In particular, we propose three action languages {\cal AT}^{0}, {\cal AT}^{1} and {\cal AT}^{2} which handle three types of defeasibilities in action domains named defeasible constraints, defeasible observations and actions with defeasible and abnormal effects respectively. Each language with a higher superscript can be viewed as an extension of the language with a lower superscript. These action languages inherit the simple syntax of {\cal A} language but their semantics is developed in terms of transition systems where transition functions are defined based on prioritized logic programs. By illustrating various examples, we show that our approach eventually provides a powerful mechanism to handle various defeasibilities in temporal prediction and postdiction. We also investigate semantic properties of these three action languages and characterize classes of action domains that present more desirable solutions in reasoning about action within the underlying action languages.",2002-06-03T06:20:21Z,http://arxiv.org/pdf/cs/0206003v1,2024-04-28,
cs/0206041v2,"Anticipatory Guidance of Plot","An anticipatory system for guiding plot development in interactive narratives is described. The executable model is a finite automaton that provides the implemented system with a look-ahead. The identification of undesirable future states in the model is used to guide the player, in a transparent manner. In this way, too radical twists of the plot can be avoided. Since the player participates in the development of the plot, such guidance can have many forms, depending on the environment of the player, on the behavior of the other players, and on the means of player interaction. We present a design method for interactive narratives which produces designs suitable for the implementation of anticipatory mechanisms. Use of the method is illustrated by application to our interactive computer game Kaktus.",2002-06-26T09:17:13Z,http://arxiv.org/pdf/cs/0206041v2,2024-04-28,
cs/0207021v1,"Abduction, ASP and Open Logic Programs","Open logic programs and open entailment have been recently proposed as an abstract framework for the verification of incomplete specifications based upon normal logic programs and the stable model semantics. There are obvious analogies between open predicates and abducible predicates. However, despite superficial similarities, there are features of open programs that have no immediate counterpart in the framework of abduction and viceversa. Similarly, open programs cannot be immediately simulated with answer set programming (ASP). In this paper we start a thorough investigation of the relationships between open inference, abduction and ASP. We shall prove that open programs generalize the other two frameworks. The generalized framework suggests interesting extensions of abduction under the generalized stable model semantics. In some cases, we will be able to reduce open inference to abduction and ASP, thereby estimating its computational complexity. At the same time, the aforementioned reduction opens the way to new applications of abduction and ASP.",2002-07-07T09:55:00Z,http://arxiv.org/pdf/cs/0207021v1,2024-04-28,
cs/0207023v2,"Domain-Dependent Knowledge in Answer Set Planning","In this paper we consider three different kinds of domain-dependent control knowledge (temporal, procedural and HTN-based) that are useful in planning. Our approach is declarative and relies on the language of logic programming with answer set semantics (AnsProlog*). AnsProlog* is designed to plan without control knowledge. We show how temporal, procedural and HTN-based control knowledge can be incorporated into AnsProlog* by the modular addition of a small number of domain-dependent rules, without the need to modify the planner. We formally prove the correctness of our planner, both in the absence and presence of the control knowledge. Finally, we perform some initial experimentation that demonstrates the potential reduction in planning time that can be achieved when procedural domain knowledge is used to solve planning problems with large plan length.",2002-07-08T00:31:54Z,http://arxiv.org/pdf/cs/0207023v2,2024-04-28,
cs/0207025v1,"""Minimal defence"": a refinement of the preferred semantics for argumentation frameworks","Dung's abstract framework for argumentation enables a study of the interactions between arguments based solely on an ``attack'' binary relation on the set of arguments. Various ways to solve conflicts between contradictory pieces of information have been proposed in the context of argumentation, nonmonotonic reasoning or logic programming, and can be captured by appropriate semantics within Dung's framework. A common feature of these semantics is that one can always maximize in some sense the set of acceptable arguments. We propose in this paper to extend Dung's framework in order to allow for the representation of what we call ``restricted'' arguments: these arguments should only be used if absolutely necessary, that is, in order to support other arguments that would otherwise be defeated. We modify Dung's preferred semantics accordingly: a set of arguments becomes acceptable only if it contains a minimum of restricted arguments, for a maximum of unrestricted arguments.",2002-07-08T13:30:16Z,http://arxiv.org/pdf/cs/0207025v1,2024-04-28,
cs/0207029v2,"Two Representations for Iterative Non-prioritized Change","We address a general representation problem for belief change, and describe two interrelated representations for iterative non-prioritized change: a logical representation in terms of persistent epistemic states, and a constructive representation in terms of flocks of bases.",2002-07-09T12:32:45Z,http://arxiv.org/pdf/cs/0207029v2,2024-04-28,
cs/0207030v2,"Collective Argumentation","An extension of an abstract argumentation framework, called collective argumentation, is introduced in which the attack relation is defined directly among sets of arguments. The extension turns out to be suitable, in particular, for representing semantics of disjunctive logic programs. Two special kinds of collective argumentation are considered in which the opponents can share their arguments.",2002-07-09T12:42:24Z,http://arxiv.org/pdf/cs/0207030v2,2024-04-28,
cs/0207042v1,"Logic Programming with Ordered Disjunction","Logic programs with ordered disjunction (LPODs) combine ideas underlying Qualitative Choice Logic (Brewka et al. KR 2002) and answer set programming. Logic programming under answer set semantics is extended with a new connective called ordered disjunction. The new connective allows us to represent alternative, ranked options for problem solutions in the heads of rules: A \times B intuitively means: if possible A, but if A is not possible then at least B. The semantics of logic programs with ordered disjunction is based on a preference relation on answer sets. LPODs are useful for applications in design and configuration and can serve as a basis for qualitative decision making.",2002-07-11T11:03:34Z,http://arxiv.org/pdf/cs/0207042v1,2024-04-28,
cs/0207045v1,"Compilation of Propositional Weighted Bases","In this paper, we investigate the extent to which knowledge compilation can be used to improve inference from propositional weighted bases. We present a general notion of compilation of a weighted base that is parametrized by any equivalence--preserving compilation function. Both negative and positive results are presented. On the one hand, complexity results are identified, showing that the inference problem from a compiled weighted base is as difficult as in the general case, when the prime implicates, Horn cover or renamable Horn cover classes are targeted. On the other hand, we show that the inference problem becomes tractable whenever DNNF-compilations are used and clausal queries are considered. Moreover, we show that the set of all preferred models of a DNNF-compilation of a weighted base can be computed in time polynomial in the output size. Finally, we sketch how our results can be used in model-based diagnosis in order to compute the most probable diagnoses of a system.",2002-07-11T16:11:40Z,http://arxiv.org/pdf/cs/0207045v1,2024-04-28,
cs/0207056v1,"Modeling Complex Domains of Actions and Change","This paper studies the problem of modeling complex domains of actions and change within high-level action description languages. We investigate two main issues of concern: (a) can we represent complex domains that capture together different problems such as ramifications, non-determinism and concurrency of actions, at a high-level, close to the given natural ontology of the problem domain and (b) what features of such a representation can affect, and how, its computational behaviour. The paper describes the main problems faced in this representation task and presents the results of an empirical study, carried out through a series of controlled experiments, to analyze the computational performance of reasoning in these representations. The experiments compare different representations obtained, for example, by changing the basic ontology of the domain or by varying the degree of use of indirect effect laws through domain constraints. This study has helped to expose the main sources of computational difficulty in the reasoning and suggest some methodological guidelines for representing complex domains. Although our work has been carried out within one particular high-level description language, we believe that the results, especially those that relate to the problems of representation, are independent of the specific modeling language.",2002-07-13T12:00:16Z,http://arxiv.org/pdf/cs/0207056v1,2024-04-28,
cs/0207059v1,"Value Based Argumentation Frameworks","This paper introduces the notion of value-based argumentation frameworks, an extension of the standard argumentation frameworks proposed by Dung, which are able toshow how rational decision is possible in cases where arguments derive their force from the social values their acceptance would promote.",2002-07-15T11:30:16Z,http://arxiv.org/pdf/cs/0207059v1,2024-04-28,
cs/0207060v1,"Preferred well-founded semantics for logic programming by alternating fixpoints: Preliminary report","We analyze the problem of defining well-founded semantics for ordered logic programs within a general framework based on alternating fixpoint theory. We start by showing that generalizations of existing answer set approaches to preference are too weak in the setting of well-founded semantics. We then specify some informal yet intuitive criteria and propose a semantical framework for preference handling that is more suitable for defining well-founded semantics for ordered logic programs. The suitability of the new approach is convinced by the fact that many attractive properties are satisfied by our semantics. In particular, our semantics is still correct with respect to various existing answer sets semantics while it successfully overcomes the weakness of their generalization to well-founded semantics. Finally, we indicate how an existing preferred well-founded semantics can be captured within our semantical framework.",2002-07-15T13:30:24Z,http://arxiv.org/pdf/cs/0207060v1,2024-04-28,
cs/0207065v1,"Embedding Default Logic in Propositional Argumentation Systems","In this paper we present a transformation of finite propositional default theories into so-called propositional argumentation systems. This transformation allows to characterize all notions of Reiter's default logic in the framework of argumentation systems. As a consequence, computing extensions, or determining wether a given formula belongs to one extension or all extensions can be answered without leaving the field of classical propositional logic. The transformation proposed is linear in the number of defaults.",2002-07-16T15:16:07Z,http://arxiv.org/pdf/cs/0207065v1,2024-04-28,
cs/0207067v1,"On the existence and multiplicity of extensions in dialectical argumentation","In the present paper, the existence and multiplicity problems of extensions are addressed. The focus is on extension of the stable type. The main result of the paper is an elegant characterization of the existence and multiplicity of extensions in terms of the notion of dialectical justification, a close cousin of the notion of admissibility. The characterization is given in the context of the particular logic for dialectical argumentation DEFLOG. The results are of direct relevance for several well-established models of defeasible reasoning (like default logic, logic programming and argumentation frameworks), since elsewhere dialectical argumentation has been shown to have close formal connections with these models.",2002-07-17T12:09:45Z,http://arxiv.org/pdf/cs/0207067v1,2024-04-28,
cs/0207075v1,"Nonmonotonic Probabilistic Logics between Model-Theoretic Probabilistic Logic and Probabilistic Logic under Coherence","Recently, it has been shown that probabilistic entailment under coherence is weaker than model-theoretic probabilistic entailment. Moreover, probabilistic entailment under coherence is a generalization of default entailment in System P. In this paper, we continue this line of research by presenting probabilistic generalizations of more sophisticated notions of classical default entailment that lie between model-theoretic probabilistic entailment and probabilistic entailment under coherence. That is, the new formalisms properly generalize their counterparts in classical default reasoning, they are weaker than model-theoretic probabilistic entailment, and they are stronger than probabilistic entailment under coherence. The new formalisms are useful especially for handling probabilistic inconsistencies related to conditioning on zero events. They can also be applied for probabilistic belief revision. More generally, in the same spirit as a similar previous paper, this paper sheds light on exciting new formalisms for probabilistic reasoning beyond the well-known standard ones.",2002-07-22T01:44:25Z,http://arxiv.org/pdf/cs/0207075v1,2024-04-28,
cs/0207083v1,"Evaluating Defaults","We seek to find normative criteria of adequacy for nonmonotonic logic similar to the criterion of validity for deductive logic. Rather than stipulating that the conclusion of an inference be true in all models in which the premises are true, we require that the conclusion of a nonmonotonic inference be true in ``almost all'' models of a certain sort in which the premises are true. This ``certain sort'' specification picks out the models that are relevant to the inference, taking into account factors such as specificity and vagueness, and previous inferences. The frequencies characterizing the relevant models reflect known frequencies in our actual world. The criteria of adequacy for a default inference can be extended by thresholding to criteria of adequacy for an extension. We show that this avoids the implausibilities that might otherwise result from the chaining of default inferences. The model proportions, when construed in terms of frequencies, provide a verifiable grounding of default rules, and can become the basis for generating default rules from statistics.",2002-07-24T23:05:29Z,http://arxiv.org/pdf/cs/0207083v1,2024-04-28,
cs/0208017v1,"Linking Makinson and Kraus-Lehmann-Magidor preferential entailments","About ten years ago, various notions of preferential entailment have been introduced. The main reference is a paper by Kraus, Lehmann and Magidor (KLM), one of the main competitor being a more general version defined by Makinson (MAK). These two versions have already been compared, but it is time to revisit these comparisons. Here are our three main results: (1) These two notions are equivalent, provided that we restrict our attention, as done in KLM, to the cases where the entailment respects logical equivalence (on the left and on the right). (2) A serious simplification of the description of the fundamental cases in which MAK is equivalent to KLM, including a natural passage in both ways. (3) The two previous results are given for preferential entailments more general than considered in some of the original texts, but they apply also to the original definitions and, for this particular case also, the models can be simplified.",2002-08-08T17:08:46Z,http://arxiv.org/pdf/cs/0208017v1,2024-04-28,
cs/0208019v1,"Knowledge Representation","This work analyses main features that should be present in knowledge representation. It suggests a model for representation and a way to implement this model in software. Representation takes care of both low-level sensor information and high-level concepts.",2002-08-12T22:34:47Z,http://arxiv.org/pdf/cs/0208019v1,2024-04-28,
cs/0208034v3,"Causes and Explanations: A Structural-Model Approach. Part II: Explanations","We propose new definitions of (causal) explanation, using structural equations to model counterfactuals. The definition is based on the notion of actual cause, as defined and motivated in a companion paper. Essentially, an explanation is a fact that is not known for certain but, if found to be true, would constitute an actual cause of the fact to be explained, regardless of the agent's initial uncertainty. We show that the definition handles well a number of problematic examples from the literature.",2002-08-20T23:08:49Z,http://arxiv.org/pdf/cs/0208034v3,2024-04-28,
cs/0209019v1,"Reasoning about Evolving Nonmonotonic Knowledge Bases","Recently, several approaches to updating knowledge bases modeled as extended logic programs have been introduced, ranging from basic methods to incorporate (sequences of) sets of rules into a logic program, to more elaborate methods which use an update policy for specifying how updates must be incorporated. In this paper, we introduce a framework for reasoning about evolving knowledge bases, which are represented as extended logic programs and maintained by an update policy. We first describe a formal model which captures various update approaches, and we define a logical language for expressing properties of evolving knowledge bases. We then investigate semantical and computational properties of our framework, where we focus on properties of knowledge states with respect to the canonical reasoning task of whether a given formula holds on a given evolving knowledge base. In particular, we present finitary characterizations of the evolution for certain classes of framework instances, which can be exploited for obtaining decidability results. In more detail, we characterize the complexity of reasoning for some meaningful classes of evolving knowledge bases, ranging from polynomial to double exponential space complexity.",2002-09-16T19:23:19Z,http://arxiv.org/pdf/cs/0209019v1,2024-04-28,
cs/0209022v2,"A Comparison of Different Cognitive Paradigms Using Simple Animats in a Virtual Laboratory, with Implications to the Notion of Cognition","In this thesis I present a virtual laboratory which implements five different models for controlling animats: a rule-based system, a behaviour-based system, a concept-based system, a neural network, and a Braitenberg architecture. Through different experiments, I compare the performance of the models and conclude that there is no ""best"" model, since different models are better for different things in different contexts.   The models I chose, although quite simple, represent different approaches for studying cognition. Using the results as an empirical philosophical aid,   I note that there is no ""best"" approach for studying cognition, since different approaches have all advantages and disadvantages, because they study different aspects of cognition from different contexts. This has implications for current debates on ""proper"" approaches for cognition: all approaches are a bit proper, but none will be ""proper enough"". I draw remarks on the notion of cognition abstracting from all the approaches used to study it, and propose a simple classification for different types of cognition.",2002-09-19T16:35:55Z,http://arxiv.org/pdf/cs/0209022v2,2024-04-28,
cs/0210004v1,"Revising Partially Ordered Beliefs","This paper deals with the revision of partially ordered beliefs. It proposes a semantic representation of epistemic states by partial pre-orders on interpretations and a syntactic representation by partially ordered belief bases. Two revision operations, the revision stemming from the history of observations and the possibilistic revision, defined when the epistemic state is represented by a total pre-order, are generalized, at a semantic level, to the case of a partial pre-order on interpretations, and at a syntactic level, to the case of a partially ordered belief base. The equivalence between the two representations is shown for the two revision operations.",2002-10-03T09:29:54Z,http://arxiv.org/pdf/cs/0210004v1,2024-04-28,
cs/0211008v4,"Can the whole brain be simpler than its ""parts""?","This is the first in a series of connected papers discussing the problem of a dynamically reconfigurable universal learning neurocomputer that could serve as a computational model for the whole human brain. The whole series is entitled ""The Brain Zero Project. My Brain as a Dynamically Reconfigurable Universal Learning Neurocomputer."" (For more information visit the website www.brain0.com.) This introductory paper is concerned with general methodology. Its main goal is to explain why it is critically important for both neural modeling and cognitive modeling to pay much attention to the basic requirements of the whole brain as a complex computing system. The author argues that it can be easier to develop an adequate computational model for the whole ""unprogrammed"" (untrained) human brain than to find adequate formal representations of some nontrivial parts of brain's performance. (In the same way as, for example, it is easier to describe the behavior of a complex analytical function than the behavior of its real and/or imaginary part.) The ""curse of dimensionality"" that plagues purely phenomenological (""brainless"") cognitive theories is a natural penalty for an attempt to represent insufficiently large parts of brain's performance in a state space of insufficiently high dimensionality. A ""partial"" modeler encounters ""Catch 22."" An attempt to simplify a cognitive problem by artificially reducing its dimensionality makes the problem more difficult.",2002-11-09T17:16:18Z,http://arxiv.org/pdf/cs/0211008v4,2024-04-28,
cs/0211027v1,"Adaptive Development of Koncepts in Virtual Animats: Insights into the Development of Knowledge","As a part of our effort for studying the evolution and development of cognition, we present results derived from synthetic experimentations in a virtual laboratory where animats develop koncepts adaptively and ground their meaning through action. We introduce the term ""koncept"" to avoid confusions and ambiguity derived from the wide use of the word ""concept"". We present the models which our animats use for abstracting koncepts from perceptions, plastically adapt koncepts, and associate koncepts with actions. On a more philosophical vein, we suggest that knowledge is a property of a cognitive system, not an element, and therefore observer-dependent.",2002-11-21T18:13:31Z,http://arxiv.org/pdf/cs/0211027v1,2024-04-28,
cs/0211038v1,"Dynamic Adjustment of the Motivation Degree in an Action Selection Mechanism","This paper presents a model for dynamic adjustment of the motivation degree, using a reinforcement learning approach, in an action selection mechanism previously developed by the authors. The learning takes place in the modification of a parameter of the model of combination of internal and external stimuli. Experiments that show the claimed properties are presented, using a VR simulation developed for such purposes. The importance of adaptation by learning in action selection is also discussed.",2002-11-27T10:35:50Z,http://arxiv.org/pdf/cs/0211038v1,2024-04-28,
cs/0211039v1,"Action Selection Properties in a Software Simulated Agent","This article analyses the properties of the Internal Behaviour network, an action selection mechanism previously proposed by the authors, with the aid of a simulation developed for such ends. A brief review of the Internal Behaviour network is followed by the explanation of the implementation of the simulation. Then, experiments are presented and discussed analysing the properties of the action selection in the proposed model.",2002-11-27T10:42:31Z,http://arxiv.org/pdf/cs/0211039v1,2024-04-28,
cs/0211040v1,"A Model for Combination of External and Internal Stimuli in the Action Selection of an Autonomous Agent","This paper proposes a model for combination of external and internal stimuli for the action selection in an autonomous agent, based in an action selection mechanism previously proposed by the authors. This combination model includes additive and multiplicative elements, which allows to incorporate new properties, which enhance the action selection. A given parameter a, which is part of the proposed model, allows to regulate the degree of dependence of the observed external behaviour from the internal states of the entity.",2002-11-27T10:45:50Z,http://arxiv.org/pdf/cs/0211040v1,2024-04-28,
cs/0212025v1,"Searching for Plannable Domains can Speed up Reinforcement Learning","Reinforcement learning (RL) involves sequential decision making in uncertain environments. The aim of the decision-making agent is to maximize the benefit of acting in its environment over an extended period of time. Finding an optimal policy in RL may be very slow. To speed up learning, one often used solution is the integration of planning, for example, Sutton's Dyna algorithm, or various other methods using macro-actions.   Here we suggest to separate plannable, i.e., close to deterministic parts of the world, and focus planning efforts in this domain. A novel reinforcement learning method called plannable RL (pRL) is proposed here. pRL builds a simple model, which is used to search for macro actions. The simplicity of the model makes planning computationally inexpensive. It is shown that pRL finds an optimal policy, and that plannable macro actions found by pRL are near-optimal. In turn, it is unnecessary to try large numbers of macro actions, which enables fast learning. The utility of pRL is demonstrated by computer simulations.",2002-12-10T22:15:25Z,http://arxiv.org/pdf/cs/0212025v1,2024-04-28,
cs/0301006v1,"Temporal plannability by variance of the episode length","Optimization of decision problems in stochastic environments is usually concerned with maximizing the probability of achieving the goal and minimizing the expected episode length. For interacting agents in time-critical applications, learning of the possibility of scheduling of subtasks (events) or the full task is an additional relevant issue. Besides, there exist highly stochastic problems where the actual trajectories show great variety from episode to episode, but completing the task takes almost the same amount of time. The identification of sub-problems of this nature may promote e.g., planning, scheduling and segmenting Markov decision processes. In this work, formulae for the average duration as well as the standard deviation of the duration of events are derived. The emerging Bellman-type equation is a simple extension of Sobel's work (1982). Methods of dynamic programming as well as methods of reinforcement learning can be applied for our extension. Computer demonstration on a toy problem serve to highlight the principle.",2003-01-09T12:39:03Z,http://arxiv.org/pdf/cs/0301006v1,2024-04-28,
cs/0301010v2,"Comparisons and Computation of Well-founded Semantics for Disjunctive Logic Programs","Much work has been done on extending the well-founded semantics to general disjunctive logic programs and various approaches have been proposed. However, these semantics are different from each other and no consensus is reached about which semantics is the most intended. In this paper we look at disjunctive well-founded reasoning from different angles. We show that there is an intuitive form of the well-founded reasoning in disjunctive logic programming which can be characterized by slightly modifying some exisitng approaches to defining disjunctive well-founded semantics, including program transformations, argumentation, unfounded sets (and resolution-like procedure). We also provide a bottom-up procedure for this semantics. The significance of our work is not only in clarifying the relationship among different approaches, but also shed some light on what is an intended well-founded semantics for disjunctive logic programs.",2003-01-14T08:14:43Z,http://arxiv.org/pdf/cs/0301010v2,2024-04-28,
cs/0301023v1,"A semantic framework for preference handling in answer set programming","We provide a semantic framework for preference handling in answer set programming. To this end, we introduce preference preserving consequence operators. The resulting fixpoint characterizations provide us with a uniform semantic framework for characterizing preference handling in existing approaches. Although our approach is extensible to other semantics by means of an alternating fixpoint theory, we focus here on the elaboration of preferences under answer set semantics. Alternatively, we show how these approaches can be characterized by the concept of order preservation. These uniform semantic characterizations provide us with new insights about interrelationships and moreover about ways of implementation.",2003-01-23T09:09:31Z,http://arxiv.org/pdf/cs/0301023v1,2024-04-28,
cs/0302029v1,"Defeasible Logic Programming: An Argumentative Approach","The work reported here introduces Defeasible Logic Programming (DeLP), a formalism that combines results of Logic Programming and Defeasible Argumentation. DeLP provides the possibility of representing information in the form of weak rules in a declarative manner, and a defeasible argumentation inference mechanism for warranting the entailed conclusions.   In DeLP an argumentation formalism will be used for deciding between contradictory goals. Queries will be supported by arguments that could be defeated by other arguments. A query q will succeed when there is an argument A for q that is warranted, ie, the argument A that supports q is found undefeated by a warrant procedure that implements a dialectical analysis.   The defeasible argumentation basis of DeLP allows to build applications that deal with incomplete and contradictory information in dynamic domains. Thus, the resulting approach is suitable for representing agent's knowledge and for providing an argumentation based reasoning mechanism to agents.",2003-02-20T00:48:06Z,http://arxiv.org/pdf/cs/0302029v1,2024-04-28,
cs/0302036v2,"Constraint-based analysis of composite solvers","Cooperative constraint solving is an area of constraint programming that studies the interaction between constraint solvers with the aim of discovering the interaction patterns that amplify the positive qualities of individual solvers. Automatisation and formalisation of such studies is an important issue of cooperative constraint solving.   In this paper we present a constraint-based analysis of composite solvers that integrates reasoning about the individual solvers and the processed data. The idea is to approximate this reasoning by resolution of set constraints on the finite sets representing the predicates that express all the necessary properties. We illustrate application of our analysis to two important cooperation patterns: deterministic choice and loop.",2003-02-25T14:33:08Z,http://arxiv.org/pdf/cs/0302036v2,2024-04-28,
cs/0302039v1,"Kalman-filtering using local interactions","There is a growing interest in using Kalman-filter models for brain modelling. In turn, it is of considerable importance to represent Kalman-filter in connectionist forms with local Hebbian learning rules. To our best knowledge, Kalman-filter has not been given such local representation. It seems that the main obstacle is the dynamic adaptation of the Kalman-gain. Here, a connectionist representation is presented, which is derived by means of the recursive prediction error method. We show that this method gives rise to attractive local learning rules and can adapt the Kalman-gain.",2003-02-28T18:32:26Z,http://arxiv.org/pdf/cs/0302039v1,2024-04-28,
cs/0303006v1,"On the Notion of Cognition","We discuss philosophical issues concerning the notion of cognition basing ourselves in experimental results in cognitive sciences, especially in computer simulations of cognitive systems. There have been debates on the ""proper"" approach for studying cognition, but we have realized that all approaches can be in theory equivalent. Different approaches model different properties of cognitive systems from different perspectives, so we can only learn from all of them. We also integrate ideas from several perspectives for enhancing the notion of cognition, such that it can contain other definitions of cognition as special cases. This allows us to propose a simple classification of different types of cognition.",2003-03-10T18:20:28Z,http://arxiv.org/pdf/cs/0303006v1,2024-04-28,
cs/0303009v2,"Unfolding Partiality and Disjunctions in Stable Model Semantics","The paper studies an implementation methodology for partial and disjunctive stable models where partiality and disjunctions are unfolded from a logic program so that an implementation of stable models for normal (disjunction-free) programs can be used as the core inference engine. The unfolding is done in two separate steps. Firstly, it is shown that partial stable models can be captured by total stable models using a simple linear and modular program transformation. Hence, reasoning tasks concerning partial stable models can be solved using an implementation of total stable models. Disjunctive partial stable models have been lacking implementations which now become available as the translation handles also the disjunctive case. Secondly, it is shown how total stable models of disjunctive programs can be determined by computing stable models for normal programs. Hence, an implementation of stable models of normal programs can be used as a core engine for implementing disjunctive programs. The feasibility of the approach is demonstrated by constructing a system for computing stable models of disjunctive programs using the smodels system as the core engine. The performance of the resulting system is compared to that of dlv which is a state-of-the-art special purpose system for disjunctive programs.",2003-03-14T14:29:32Z,http://arxiv.org/pdf/cs/0303009v2,2024-04-28,
cs/0303018v1,"Multi-target particle filtering for the probability hypothesis density","When tracking a large number of targets, it is often computationally expensive to represent the full joint distribution over target states. In cases where the targets move independently, each target can instead be tracked with a separate filter. However, this leads to a model-data association problem. Another approach to solve the problem with computational complexity is to track only the first moment of the joint distribution, the probability hypothesis density (PHD). The integral of this distribution over any area S is the expected number of targets within S. Since no record of object identity is kept, the model-data association problem is avoided.   The contribution of this paper is a particle filter implementation of the PHD filter mentioned above. This PHD particle filter is applied to tracking of multiple vehicles in terrain, a non-linear tracking problem. Experiments show that the filter can track a changing number of vehicles robustly, achieving near-real-time performance.",2003-03-20T13:48:04Z,http://arxiv.org/pdf/cs/0303018v1,2024-04-28,
cs/0305001v1,"A Framework for Searching AND/OR Graphs with Cycles","Search in cyclic AND/OR graphs was traditionally known to be an unsolved problem. In the recent past several important studies have been reported in this domain. In this paper, we have taken a fresh look at the problem. First, a new and comprehensive theoretical framework for cyclic AND/OR graphs has been presented, which was found missing in the recent literature. Based on this framework, two best-first search algorithms, S1 and S2, have been developed. S1 does uninformed search and is a simple modification of the Bottom-up algorithm by Martelli and Montanari. S2 performs a heuristically guided search and replicates the modification in Bottom-up's successors, namely HS and AO*. Both S1 and S2 solve the problem of searching AND/OR graphs in presence of cycles. We then present a detailed analysis for the correctness and complexity results of S1 and S2, using the proposed framework. We have observed through experiments that S1 and S2 output correct results in all cases.",2003-05-01T04:48:29Z,http://arxiv.org/pdf/cs/0305001v1,2024-04-28,
cs/0305019v1,"On rho in a Decision-Theoretic Apparatus of Dempster-Shafer Theory","Thomas M. Strat has developed a decision-theoretic apparatus for Dempster-Shafer theory (Decision analysis using belief functions, Intern. J. Approx. Reason. 4(5/6), 391-417, 1990). In this apparatus, expected utility intervals are constructed for different choices. The choice with the highest expected utility is preferable to others. However, to find the preferred choice when the expected utility interval of one choice is included in that of another, it is necessary to interpolate a discerning point in the intervals. This is done by the parameter rho, defined as the probability that the ambiguity about the utility of every nonsingleton focal element will turn out as favorable as possible. If there are several different decision makers, we might sometimes be more interested in having the highest expected utility among the decision makers rather than only trying to maximize our own expected utility regardless of choices made by other decision makers. The preference of each choice is then determined by the probability of yielding the highest expected utility. This probability is equal to the maximal interval length of rho under which an alternative is preferred. We must here take into account not only the choices already made by other decision makers but also the rational choices we can assume to be made by later decision makers. In Strats apparatus, an assumption, unwarranted by the evidence at hand, has to be made about the value of rho. We demonstrate that no such assumption is necessary. It is sufficient to assume a uniform probability distribution for rho to be able to discern the most preferable choice. We discuss when this approach is justifiable.",2003-05-16T15:07:09Z,http://arxiv.org/pdf/cs/0305019v1,2024-04-28,
cs/0305044v2,"Updating beliefs with incomplete observations","Currently, there is renewed interest in the problem, raised by Shafer in 1985, of updating probabilities when observations are incomplete. This is a fundamental problem in general, and of particular interest for Bayesian networks. Recently, Grunwald and Halpern have shown that commonly used updating strategies fail in this case, except under very special assumptions. In this paper we propose a new method for updating probabilities with incomplete observations. Our approach is deliberately conservative: we make no assumptions about the so-called incompleteness mechanism that associates complete with incomplete observations. We model our ignorance about this mechanism by a vacuous lower prevision, a tool from the theory of imprecise probabilities, and we use only coherence arguments to turn prior into posterior probabilities. In general, this new approach to updating produces lower and upper posterior probabilities and expectations, as well as partially determinate decisions. This is a logical consequence of the existing ignorance about the incompleteness mechanism. We apply the new approach to the problem of classification of new evidence in probabilistic expert systems, where it leads to a new, so-called conservative updating rule. In the special case of Bayesian networks constructed using expert knowledge, we provide an exact algorithm for classification based on our updating rule, which has linear-time complexity for a class of networks wider than polytrees. This result is then extended to the more general framework of credal networks, where computations are often much harder than with Bayesian nets. Using an example, we show that our rule appears to provide a solid basis for reliable updating with incomplete observations, when no strong assumptions about the incompleteness mechanism are justified.",2003-05-27T11:05:52Z,http://arxiv.org/pdf/cs/0305044v2,2024-04-28,
cs/0306124v1,"Updating Probabilities","As examples such as the Monty Hall puzzle show, applying conditioning to update a probability distribution on a ``naive space'', which does not take into account the protocol used, can often lead to counterintuitive results. Here we examine why. A criterion known as CAR (``coarsening at random'') in the statistical literature characterizes when ``naive'' conditioning in a naive space works. We show that the CAR condition holds rather infrequently, and we provide a procedural characterization of it, by giving a randomized algorithm that generates all and only distributions for which CAR holds. This substantially extends previous characterizations of CAR. We also consider more generalized notions of update such as Jeffrey conditioning and minimizing relative entropy (MRE). We give a generalization of the CAR condition that characterizes when Jeffrey conditioning leads to appropriate answers, and show that there exist some very simple settings in which MRE essentially never gives the right results. This generalizes and interconnects previous results obtained in the literature on CAR and MRE.",2003-06-23T22:24:05Z,http://arxiv.org/pdf/cs/0306124v1,2024-04-28,
cs/0306135v1,"Pruning Isomorphic Structural Sub-problems in Configuration","Configuring consists in simulating the realization of a complex product from a catalog of component parts, using known relations between types, and picking values for object attributes. This highly combinatorial problem in the field of constraint programming has been addressed with a variety of approaches since the foundation system R1(McDermott82). An inherent difficulty in solving configuration problems is the existence of many isomorphisms among interpretations. We describe a formalism independent approach to improve the detection of isomorphisms by configurators, which does not require to adapt the problem model. To achieve this, we exploit the properties of a characteristic subset of configuration problems, called the structural sub-problem, which canonical solutions can be produced or tested at a limited cost. In this paper we present an algorithm for testing the canonicity of configurations, that can be added as a symmetry breaking constraint to any configurator. The cost and efficiency of this canonicity test are given.",2003-06-27T11:25:17Z,http://arxiv.org/pdf/cs/0306135v1,2024-04-28,
cs/0307010v2,"Probabilistic Reasoning as Information Compression by Multiple Alignment, Unification and Search: An Introduction and Overview","This article introduces the idea that probabilistic reasoning (PR) may be understood as ""information compression by multiple alignment, unification and search"" (ICMAUS). In this context, multiple alignment has a meaning which is similar to but distinct from its meaning in bio-informatics, while unification means a simple merging of matching patterns, a meaning which is related to but simpler than the meaning of that term in logic.   A software model, SP61, has been developed for the discovery and formation of 'good' multiple alignments, evaluated in terms of information compression. The model is described in outline.   Using examples from the SP61 model, this article describes in outline how the ICMAUS framework can model various kinds of PR including: PR in best-match pattern recognition and information retrieval; one-step 'deductive' and 'abductive' PR; inheritance of attributes in a class hierarchy; chains of reasoning (probabilistic decision networks and decision trees, and PR with 'rules'); geometric analogy problems; nonmonotonic reasoning and reasoning with default values; modelling the function of a Bayesian network.",2003-07-04T16:34:45Z,http://arxiv.org/pdf/cs/0307010v2,2024-04-28,
cs/0307025v1,"Information Compression by Multiple Alignment, Unification and Search as a Unifying Principle in Computing and Cognition","This article presents an overview of the idea that ""information compression by multiple alignment, unification and search"" (ICMAUS) may serve as a unifying principle in computing (including mathematics and logic) and in such aspects of human cognition as the analysis and production of natural language, fuzzy pattern recognition and best-match information retrieval, concept hierarchies with inheritance of attributes, probabilistic reasoning, and unsupervised inductive learning. The ICMAUS concepts are described together with an outline of the SP61 software model in which the ICMAUS concepts are currently realised. A range of examples is presented, illustrated with output from the SP61 model.",2003-07-10T15:32:31Z,http://arxiv.org/pdf/cs/0307025v1,2024-04-28,
cs/0307048v2,"Integrating cardinal direction relations and other orientation relations in Qualitative Spatial Reasoning","We propose a calculus integrating two calculi well-known in Qualitative Spatial Reasoning (QSR): Frank's projection-based cardinal direction calculus, and a coarser version of Freksa's relative orientation calculus. An original constraint propagation procedure is presented, which implements the interaction between the two integrated calculi. The importance of taking into account the interaction is shown with a real example providing an inconsistent knowledge base, whose inconsistency (a) cannot be detected by reasoning separately about each of the two components of the knowledge, just because, taken separately, each is consistent, but (b) is detected by the proposed algorithm, thanks to the interaction knowledge propagated from each of the two compnents to the other.",2003-07-21T13:03:19Z,http://arxiv.org/pdf/cs/0307048v2,2024-04-28,
cs/0307050v1,"A ternary Relation Algebra of directed lines","We define a ternary Relation Algebra (RA) of relative position relations on two-dimensional directed lines (d-lines for short). A d-line has two degrees of freedom (DFs): a rotational DF (RDF), and a translational DF (TDF). The representation of the RDF of a d-line will be handled by an RA of 2D orientations, CYC_t, known in the literature. A second algebra, TA_t, which will handle the TDF of a d-line, will be defined. The two algebras, CYC_t and TA_t, will constitute, respectively, the translational and the rotational components of the RA, PA_t, of relative position relations on d-lines: the PA_t atoms will consist of those pairs <t,r> of a TA_t atom and a CYC_t atom that are compatible. We present in detail the RA PA_t, with its converse table, its rotation table and its composition tables. We show that a (polynomial) constraint propagation algorithm, known in the literature, is complete for a subset of PA_t relations including almost all of the atomic relations. We will discuss the application scope of the RA, which includes incidence geometry, GIS (Geographic Information Systems), shape representation, localisation in (multi-)robot navigation, and the representation of motion prepositions in NLP (Natural Language Processing). We then compare the RA to existing ones, such as an algebra for reasoning about rectangles parallel to the axes of an (orthogonal) coordinate system, a ``spatial Odyssey'' of Allen's interval algebra, and an algebra for reasoning about 2D segments.",2003-07-21T16:01:11Z,http://arxiv.org/pdf/cs/0307050v1,2024-04-28,
cs/0307056v1,"From Statistical Knowledge Bases to Degrees of Belief","An intelligent agent will often be uncertain about various properties of its environment, and when acting in that environment it will frequently need to quantify its uncertainty. For example, if the agent wishes to employ the expected-utility paradigm of decision theory to guide its actions, it will need to assign degrees of belief (subjective probabilities) to various assertions. Of course, these degrees of belief should not be arbitrary, but rather should be based on the information available to the agent. This paper describes one approach for inducing degrees of belief from very rich knowledge bases, that can include information about particular individuals, statistical correlations, physical laws, and default rules. We call our approach the random-worlds method. The method is based on the principle of indifference: it treats all of the worlds the agent considers possible as being equally likely. It is able to integrate qualitative default reasoning with quantitative probabilistic reasoning by providing a language in which both types of information can be easily expressed. Our results show that a number of desiderata that arise in direct inference (reasoning from statistical information to conclusions about individuals) and default reasoning follow directly {from} the semantics of random worlds. For example, random worlds captures important patterns of reasoning such as specificity, inheritance, indifference to irrelevant information, and default assumptions of independence. Furthermore, the expressive power of the language used and the intuitive semantics of random worlds allow the method to deal with problems that are beyond the scope of many other non-deductive reasoning systems.",2003-07-24T21:32:09Z,http://arxiv.org/pdf/cs/0307056v1,2024-04-28,
cs/0307063v1,"An Alternative to RDF-Based Languages for the Representation and Processing of Ontologies in the Semantic Web","This paper describes an approach to the representation and processing of ontologies in the Semantic Web, based on the ICMAUS theory of computation and AI. This approach has strengths that complement those of languages based on the Resource Description Framework (RDF) such as RDF Schema and DAML+OIL. The main benefits of the ICMAUS approach are simplicity and comprehensibility in the representation of ontologies, an ability to cope with errors and uncertainties in knowledge, and a versatile reasoning system with capabilities in the kinds of probabilistic reasoning that seem to be required in the Semantic Web.",2003-07-29T11:13:41Z,http://arxiv.org/pdf/cs/0307063v1,2024-04-28,
cs/0308002v3,"Quantifying and Visualizing Attribute Interactions","Interactions are patterns between several attributes in data that cannot be inferred from any subset of these attributes. While mutual information is a well-established approach to evaluating the interactions between two attributes, we surveyed its generalizations as to quantify interactions between several attributes. We have chosen McGill's interaction information, which has been independently rediscovered a number of times under various names in various disciplines, because of its many intuitively appealing properties. We apply interaction information to visually present the most important interactions of the data. Visualization of interactions has provided insight into the structure of data on a number of domains, identifying redundant attributes and opportunities for constructing new features, discovering unexpected regularities in data, and have helped during construction of predictive models; we illustrate the methods on numerous examples. A machine learning method that disregards interactions may get caught in two traps: myopia is caused by learning algorithms assuming independence in spite of interactions, whereas fragmentation arises from assuming an interaction in spite of independence.",2003-08-01T10:50:07Z,http://arxiv.org/pdf/cs/0308002v3,2024-04-28,
cs/0309025v1,"Evidential Force Aggregation","In this paper we develop an evidential force aggregation method intended for classification of evidential intelligence into recognized force structures. We assume that the intelligence has already been partitioned into clusters and use the classification method individually in each cluster. The classification is based on a measure of fitness between template and fused intelligence that makes it possible to handle intelligence reports with multiple nonspecific and uncertain propositions. With this measure we can aggregate on a level-by-level basis, starting from general intelligence to achieve a complete force structure with recognized units on all hierarchical levels.",2003-09-15T07:20:48Z,http://arxiv.org/pdf/cs/0309025v1,2024-04-28,
cs/0310023v1,"Application of Kullback-Leibler Metric to Speech Recognition","Article discusses the application of Kullback-Leibler divergence to the recognition of speech signals and suggests three algorithms implementing this divergence criterion: correlation algorithm, spectral algorithm and filter algorithm. Discussion covers an approach to the problem of speech variability and is illustrated with the results of experimental modeling of speech signals. The article gives a number of recommendations on the choice of appropriate model parameters and provides a comparison to some other methods of speech recognition.",2003-10-13T16:17:51Z,http://arxiv.org/pdf/cs/0310023v1,2024-04-28,
cs/0310044v1,"The Algebra of Utility Inference","Richard Cox [1] set the axiomatic foundations of probable inference and the algebra of propositions. He showed that consistency within these axioms requires certain rules for updating belief. In this paper we use the analogy between probability and utility introduced in [2] to propose an axiomatic foundation for utility inference and the algebra of preferences. We show that consistency within these axioms requires certain rules for updating preference. We discuss a class of utility functions that stems from the axioms of utility inference and show that this class is the basic building block for any general multiattribute utility function. We use this class of utility functions together with the algebra of preferences to construct utility functions represented by logical operations on the attributes.",2003-10-23T01:13:20Z,http://arxiv.org/pdf/cs/0310044v1,2024-04-28,
cs/0310045v1,"An information theory for preferences","Recent literature in the last Maximum Entropy workshop introduced an analogy between cumulative probability distributions and normalized utility functions. Based on this analogy, a utility density function can de defined as the derivative of a normalized utility function. A utility density function is non-negative and integrates to unity. These two properties form the basis of a correspondence between utility and probability. A natural application of this analogy is a maximum entropy principle to assign maximum entropy utility values. Maximum entropy utility interprets many of the common utility functions based on the preference information needed for their assignment, and helps assign utility values based on partial preference information. This paper reviews maximum entropy utility and introduces further results that stem from the duality between probability and utility.",2003-10-23T01:34:44Z,http://arxiv.org/pdf/cs/0310045v1,2024-04-28,
cs/0310047v1,"Abductive Logic Programs with Penalization: Semantics, Complexity and Implementation","Abduction, first proposed in the setting of classical logics, has been studied with growing interest in the logic programming area during the last years.   In this paper we study abduction with penalization in the logic programming framework. This form of abductive reasoning, which has not been previously analyzed in logic programming, turns out to represent several relevant problems, including optimization problems, very naturally. We define a formal model for abduction with penalization over logic programs, which extends the abductive framework proposed by Kakas and Mancarella. We address knowledge representation issues, encoding a number of problems in our abductive framework. In particular, we consider some relevant problems, taken from different domains, ranging from optimization theory to diagnosis and planning; their encodings turn out to be simple and elegant in our formalism. We thoroughly analyze the computational complexity of the main problems arising in the context of abduction with penalization from logic programs. Finally, we implement a system supporting the proposed abductive framework on top of the DLV engine. To this end, we design a translation from abduction problems with penalties into logic programs with weak constraints. We prove that this approach is sound and complete.",2003-10-24T18:03:06Z,http://arxiv.org/pdf/cs/0310047v1,2024-04-28,
cs/0310061v1,"Local-search techniques for propositional logic extended with cardinality constraints","We study local-search satisfiability solvers for propositional logic extended with cardinality atoms, that is, expressions that provide explicit ways to model constraints on cardinalities of sets. Adding cardinality atoms to the language of propositional logic facilitates modeling search problems and often results in concise encodings. We propose two ``native'' local-search solvers for theories in the extended language. We also describe techniques to reduce the problem to standard propositional satisfiability and allow us to use off-the-shelf SAT solvers. We study these methods experimentally. Our general finding is that native solvers designed specifically for the extended language perform better than indirect methods relying on SAT solvers.",2003-10-31T16:29:02Z,http://arxiv.org/pdf/cs/0310061v1,2024-04-28,
cs/0310062v1,"WSAT(cc) - a fast local-search ASP solver","We describe WSAT(cc), a local-search solver for computing models of theories in the language of propositional logic extended by cardinality atoms. WSAT(cc) is a processing back-end for the logic PS+, a recently proposed formalism for answer-set programming.",2003-10-31T16:46:07Z,http://arxiv.org/pdf/cs/0310062v1,2024-04-28,
cs/0311004v1,"Utility-Probability Duality","This paper presents duality between probability distributions and utility functions.",2003-11-06T07:33:23Z,http://arxiv.org/pdf/cs/0311004v1,2024-04-28,
cs/0311007v1,"Parametric Connectives in Disjunctive Logic Programming","Disjunctive Logic Programming (\DLP) is an advanced formalism for Knowledge Representation and Reasoning (KRR). \DLP is very expressive in a precise mathematical sense: it allows to express every property of finite structures that is decidable in the complexity class $\SigmaP{2}$ ($\NP^{\NP}$). Importantly, the \DLP encodings are often simple and natural.   In this paper, we single out some limitations of \DLP for KRR, which cannot naturally express problems where the size of the disjunction is not known ``a priori'' (like N-Coloring), but it is part of the input. To overcome these limitations, we further enhance the knowledge modelling abilities of \DLP, by extending this language by {\em Parametric Connectives (OR and AND)}. These connectives allow us to represent compactly the disjunction/conjunction of a set of atoms having a given property. We formally define the semantics of the new language, named $DLP^{\bigvee,\bigwedge}$ and we show the usefulness of the new constructs on relevant knowledge-based problems. We address implementation issues and discuss related works.",2003-11-07T15:57:07Z,http://arxiv.org/pdf/cs/0311007v1,2024-04-28,
cs/0311024v1,"Logic-Based Specification Languages for Intelligent Software Agents","The research field of Agent-Oriented Software Engineering (AOSE) aims to find abstractions, languages, methodologies and toolkits for modeling, verifying, validating and prototyping complex applications conceptualized as Multiagent Systems (MASs). A very lively research sub-field studies how formal methods can be used for AOSE. This paper presents a detailed survey of six logic-based executable agent specification languages that have been chosen for their potential to be integrated in our ARPEGGIO project, an open framework for specifying and prototyping a MAS. The six languages are ConGoLog, Agent-0, the IMPACT agent programming language, DyLog, Concurrent METATEM and Ehhf. For each executable language, the logic foundations are described and an example of use is shown. A comparison of the six languages and a survey of similar approaches complete the paper, together with considerations of the advantages of using logic-based languages in MAS modeling and prototyping.",2003-11-20T10:10:25Z,http://arxiv.org/pdf/cs/0311024v1,2024-04-28,
cs/0311026v1,"Great Expectations. Part I: On the Customizability of Generalized Expected Utility","We propose a generalization of expected utility that we call generalized EU (GEU), where a decision maker's beliefs are represented by plausibility measures, and the decision maker's tastes are represented by general (i.e.,not necessarily real-valued) utility functions. We show that every agent, ``rational'' or not, can be modeled as a GEU maximizer. We then show that we can customize GEU by selectively imposing just the constraints we want. In particular, we show how each of Savage's postulates corresponds to constraints on GEU.",2003-11-20T17:36:53Z,http://arxiv.org/pdf/cs/0311026v1,2024-04-28,
cs/0311027v1,"Great Expectations. Part II: Generalized Expected Utility as a Universal Decision Rule","Many different rules for decision making have been introduced in the literature. We show that a notion of generalized expected utility proposed in Part I of this paper is a universal decision rule, in the sense that it can represent essentially all other decision rules.",2003-11-20T17:39:22Z,http://arxiv.org/pdf/cs/0311027v1,2024-04-28,
cs/0311045v1,"Unsupervised Grammar Induction in a Framework of Information Compression by Multiple Alignment, Unification and Search","This paper describes a novel approach to grammar induction that has been developed within a framework designed to integrate learning with other aspects of computing, AI, mathematics and logic. This framework, called ""information compression by multiple alignment, unification and search"" (ICMAUS), is founded on principles of Minimum Length Encoding pioneered by Solomonoff and others. Most of the paper describes SP70, a computer model of the ICMAUS framework that incorporates processes for unsupervised learning of grammars. An example is presented to show how the model can infer a plausible grammar from appropriate input. Limitations of the current model and how they may be overcome are briefly discussed.",2003-11-27T11:18:59Z,http://arxiv.org/pdf/cs/0311045v1,2024-04-28,
cs/0311051v1,"Integrating existing cone-shaped and projection-based cardinal direction relations and a TCSP-like decidable generalisation","We consider the integration of existing cone-shaped and projection-based calculi of cardinal direction relations, well-known in QSR. The more general, integrating language we consider is based on convex constraints of the qualitative form $r(x,y)$, $r$ being a cone-shaped or projection-based cardinal direction atomic relation, or of the quantitative form $(\alpha ,\beta)(x,y)$, with $\alpha ,\beta\in [0,2\pi)$ and $(\beta -\alpha)\in [0,\pi ]$: the meaning of the quantitative constraint, in particular, is that point $x$ belongs to the (convex) cone-shaped area rooted at $y$, and bounded by angles $\alpha$ and $\beta$. The general form of a constraint is a disjunction of the form $[r_1\vee...\vee r_{n_1}\vee (\alpha_1,\beta_1)\vee...\vee (\alpha _{n_2},\beta_{n_2})](x,y)$, with $r_i(x,y)$, $i=1... n_1$, and $(\alpha _i,\beta_i)(x,y)$, $i=1... n_2$, being convex constraints as described above: the meaning of such a general constraint is that, for some $i=1... n_1$, $r_i(x,y)$ holds, or, for some $i=1... n_2$, $(\alpha_i,\beta_i)(x,y)$ holds. A conjunction of such general constraints is a $\tcsp$-like CSP, which we will refer to as an $\scsp$ (Spatial Constraint Satisfaction Problem). An effective solution search algorithm for an $\scsp$ will be described, which uses (1) constraint propagation, based on a composition operation to be defined, as the filtering method during the search, and (2) the Simplex algorithm, guaranteeing completeness, at the leaves of the search tree. The approach is particularly suited for large-scale high-level vision, such as, e.g., satellite-like surveillance of a geographic area.",2003-11-28T04:06:56Z,http://arxiv.org/pdf/cs/0311051v1,2024-04-28,
cs/0312020v1,"Modeling Object Oriented Constraint Programs in Z","Object oriented constraint programs (OOCPs) emerge as a leading evolution of constraint programming and artificial intelligence, first applied to a range of industrial applications called configuration problems. The rich variety of technical approaches to solving configuration problems (CLP(FD), CC(FD), DCSP, Terminological systems, constraint programs with set variables ...) is a source of difficulty. No universally accepted formal language exists for communicating about OOCPs, which makes the comparison of systems difficult. We present here a Z based specification of OOCPs which avoids the falltrap of hidden object semantics. The object system is part of the specification, and captures all of the most advanced notions from the object oriented modeling standard UML. The paper illustrates these issues and the conciseness and precision of Z by the specification of a working OOCP that solves an historical AI problem : parsing a context free grammar. Being written in Z, an OOCP specification also supports formal proofs. The whole builds the foundation of an adaptative and evolving framework for communicating about constrained object models and programs.",2003-12-12T10:15:38Z,http://arxiv.org/pdf/cs/0312020v1,2024-04-28,
cs/0312040v1,"Diagnostic reasoning with A-Prolog","In this paper we suggest an architecture for a software agent which operates a physical device and is capable of making observations and of testing and repairing the device's components. We present simplified definitions of the notions of symptom, candidate diagnosis, and diagnosis which are based on the theory of action language ${\cal AL}$. The definitions allow one to give a simple account of the agent's behavior in which many of the agent's tasks are reduced to computing stable models of logic programs.",2003-12-18T13:38:49Z,http://arxiv.org/pdf/cs/0312040v1,2024-04-28,
cs/0312045v1,"Weight Constraints as Nested Expressions","We compare two recent extensions of the answer set (stable model) semantics of logic programs. One of them, due to Lifschitz, Tang and Turner, allows the bodies and heads of rules to contain nested expressions. The other, due to Niemela and Simons, uses weight constraints. We show that there is a simple, modular translation from the language of weight constraints into the language of nested expressions that preserves the program's answer sets. Nested expressions can be eliminated from the result of this translation in favor of additional atoms. The translation makes it possible to compute answer sets for some programs with weight constraints using satisfiability solvers, and to prove the strong equivalence of programs with weight constraints using the logic of here-and there.",2003-12-19T16:00:43Z,http://arxiv.org/pdf/cs/0312045v1,2024-04-28,
cs/0312053v1,"On the Expressibility of Stable Logic Programming","(We apologize for pidgin LaTeX) Schlipf \cite{sch91} proved that Stable Logic Programming (SLP) solves all $\mathit{NP}$ decision problems. We extend Schlipf's result to prove that SLP solves all search problems in the class $\mathit{NP}$. Moreover, we do this in a uniform way as defined in \cite{mt99}. Specifically, we show that there is a single $\mathrm{DATALOG}^{\neg}$ program $P_{\mathit{Trg}}$ such that given any Turing machine $M$, any polynomial $p$ with non-negative integer coefficients and any input $\sigma$ of size $n$ over a fixed alphabet $\Sigma$, there is an extensional database $\mathit{edb}_{M,p,\sigma}$ such that there is a one-to-one correspondence between the stable models of $\mathit{edb}_{M,p,\sigma} \cup P_{\mathit{Trg}}$ and the accepting computations of the machine $M$ that reach the final state in at most $p(n)$ steps. Moreover, $\mathit{edb}_{M,p,\sigma}$ can be computed in polynomial time from $p$, $\sigma$ and the description of $M$ and the decoding of such accepting computations from its corresponding stable model of $\mathit{edb}_{M,p,\sigma} \cup P_{\mathit{Trg}}$ can be computed in linear time. A similar statement holds for Default Logic with respect to $\Sigma_2^\mathrm{P}$-search problems\footnote{The proof of this result involves additional technical complications and will be a subject of another publication.}.",2003-12-22T18:34:21Z,http://arxiv.org/pdf/cs/0312053v1,2024-04-28,
cs/0401009v1,"Unifying Computing and Cognition: The SP Theory and its Applications","This book develops the conjecture that all kinds of information processing in computers and in brains may usefully be understood as ""information compression by multiple alignment, unification and search"". This ""SP theory"", which has been under development since 1987, provides a unified view of such things as the workings of a universal Turing machine, the nature of 'knowledge', the interpretation and production of natural language, pattern recognition and best-match information retrieval, several kinds of probabilistic reasoning, planning and problem solving, unsupervised learning, and a range of concepts in mathematics and logic. The theory also provides a basis for the design of an 'SP' computer with several potential advantages compared with traditional digital computers.",2004-01-13T16:16:07Z,http://arxiv.org/pdf/cs/0401009v1,2024-04-28,
cs/0402033v1,"Recycling Computed Answers in Rewrite Systems for Abduction","In rule-based systems, goal-oriented computations correspond naturally to the possible ways that an observation may be explained. In some applications, we need to compute explanations for a series of observations with the same domain. The question whether previously computed answers can be recycled arises. A yes answer could result in substantial savings of repeated computations. For systems based on classic logic, the answer is YES. For nonmonotonic systems however, one tends to believe that the answer should be NO, since recycling is a form of adding information. In this paper, we show that computed answers can always be recycled, in a nontrivial way, for the class of rewrite procedures that we proposed earlier for logic programs with negation. We present some experimental results on an encoding of the logistics domain.",2004-02-16T06:15:05Z,http://arxiv.org/pdf/cs/0402033v1,2024-04-28,
cs/0402035v1,"Memory As A Monadic Control Construct In Problem-Solving","Recent advances in programming languages study and design have established a standard way of grounding computational systems representation in category theory. These formal results led to a better understanding of issues of control and side-effects in functional and imperative languages. This framework can be successfully applied to the investigation of the performance of Artificial Intelligence (AI) inference and cognitive systems. In this paper, we delineate a categorical formalisation of memory as a control structure driving performance in inference systems. Abstracting away control mechanisms from three widely used representations of memory in cognitive systems (scripts, production rules and clusters) we explain how categorical triples capture the interaction between learning and problem-solving.",2004-02-16T17:02:29Z,http://arxiv.org/pdf/cs/0402035v1,2024-04-28,
cs/0402057v2,"Integrating Defeasible Argumentation and Machine Learning Techniques","The field of machine learning (ML) is concerned with the question of how to construct algorithms that automatically improve with experience. In recent years many successful ML applications have been developed, such as datamining programs, information-filtering systems, etc. Although ML algorithms allow the detection and extraction of interesting patterns of data for several kinds of problems, most of these algorithms are based on quantitative reasoning, as they rely on training data in order to infer so-called target functions.   In the last years defeasible argumentation has proven to be a sound setting to formalize common-sense qualitative reasoning. This approach can be combined with other inference techniques, such as those provided by machine learning theory.   In this paper we outline different alternatives for combining defeasible argumentation and machine learning techniques. We suggest how different aspects of a generic argument-based framework can be integrated with other ML-based approaches.",2004-02-25T18:02:29Z,http://arxiv.org/pdf/cs/0402057v2,2024-04-28,
cs/0403002v2,"Epistemic Foundation of Stable Model Semantics","Stable model semantics has become a very popular approach for the management of negation in logic programming. This approach relies mainly on the closed world assumption to complete the available knowledge and its formulation has its basis in the so-called Gelfond-Lifschitz transformation.   The primary goal of this work is to present an alternative and epistemic-based characterization of stable model semantics, to the Gelfond-Lifschitz transformation. In particular, we show that stable model semantics can be defined entirely as an extension of the Kripke-Kleene semantics. Indeed, we show that the closed world assumption can be seen as an additional source of `falsehood' to be added cumulatively to the Kripke-Kleene semantics. Our approach is purely algebraic and can abstract from the particular formalism of choice as it is based on monotone operators (under the knowledge order) over bilattices only.",2004-03-02T15:45:29Z,http://arxiv.org/pdf/cs/0403002v2,2024-04-28,
cs/0403006v1,"The role of behavior modifiers in representation development","We address the problem of the development of representations and their relationship to the environment. We study a software agent which develops in a network a representation of its simple environment which captures and integrates the relationships between agent and environment through a closure mechanism. The inclusion of a variable behavior modifier allows better representation development. This can be confirmed with an internal description of the closure mechanism, and with an external description of the properties of the representation network.",2004-03-05T12:53:57Z,http://arxiv.org/pdf/cs/0403006v1,2024-04-28,
cs/0404011v1,"Parametric external predicates for the DLV System","This document describes syntax, semantics and implementation guidelines in order to enrich the DLV system with the possibility to make external C function calls. This feature is realized by the introduction of parametric external predicates, whose extension is not specified through a logic program but implicitly computed through external code.",2004-04-05T17:15:45Z,http://arxiv.org/pdf/cs/0404011v1,2024-04-28,
cs/0404012v1,"Toward the Implementation of Functions in the DLV System (Preliminary Technical Report)","This document describes the functions as they are treated in the DLV system. We give first the language, then specify the main implementation issues.",2004-04-05T17:23:07Z,http://arxiv.org/pdf/cs/0404012v1,2024-04-28,
cs/0404051v1,"Knowledge And The Action Description Language A","We introduce Ak, an extension of the action description language A (Gelfond and Lifschitz, 1993) to handle actions which affect knowledge. We use sensing actions to increase an agent's knowledge of the world and non-deterministic actions to remove knowledge. We include complex plans involving conditionals and loops in our query language for hypothetical reasoning. We also present a translation of Ak domain descriptions into epistemic logic programs.",2004-04-24T14:16:04Z,http://arxiv.org/pdf/cs/0404051v1,2024-04-28,
cs/0405008v1,"A Comparative Study of Fuzzy Classification Methods on Breast Cancer Data","In this paper, we examine the performance of four fuzzy rule generation methods on Wisconsin breast cancer data. The first method generates fuzzy if then rules using the mean and the standard deviation of attribute values. The second approach generates fuzzy if then rules using the histogram of attributes values. The third procedure generates fuzzy if then rules with certainty of each attribute into homogeneous fuzzy sets. In the fourth approach, only overlapping areas are partitioned. The first two approaches generate a single fuzzy if then rule for each class by specifying the membership function of each antecedent fuzzy set using the information about attribute values of training patterns. The other two approaches are based on fuzzy grids with homogeneous fuzzy partitions of each attribute. The performance of each approach is evaluated on breast cancer data sets. Simulation results show that the Modified grid approach has a high classification rate of 99.73 %.",2004-05-04T23:02:53Z,http://arxiv.org/pdf/cs/0405008v1,2024-04-28,
cs/0405009v1,"Intelligent Systems: Architectures and Perspectives","The integration of different learning and adaptation techniques to overcome individual limitations and to achieve synergetic effects through the hybridization or fusion of these techniques has, in recent years, contributed to a large number of new intelligent system designs. Computational intelligence is an innovative framework for constructing intelligent hybrid architectures involving Neural Networks (NN), Fuzzy Inference Systems (FIS), Probabilistic Reasoning (PR) and derivative free optimization techniques such as Evolutionary Computation (EC). Most of these hybridization approaches, however, follow an ad hoc design methodology, justified by success in certain application domains. Due to the lack of a common framework it often remains difficult to compare the various hybrid systems conceptually and to evaluate their performance comparatively. This chapter introduces the different generic architectures for integrating intelligent systems. The designing aspects and perspectives of different hybrid archirectures like NN-FIS, EC-FIS, EC-NN, FIS-PR and NN-FIS-EC systems are presented. Some conclusions are also provided towards the end.",2004-05-04T23:48:39Z,http://arxiv.org/pdf/cs/0405009v1,2024-04-28,
cs/0405010v1,"A Neuro-Fuzzy Approach for Modelling Electricity Demand in Victoria","Neuro-fuzzy systems have attracted growing interest of researchers in various scientific and engineering areas due to the increasing need of intelligent systems. This paper evaluates the use of two popular soft computing techniques and conventional statistical approach based on Box--Jenkins autoregressive integrated moving average (ARIMA) model to predict electricity demand in the State of Victoria, Australia. The soft computing methods considered are an evolving fuzzy neural network (EFuNN) and an artificial neural network (ANN) trained using scaled conjugate gradient algorithm (CGA) and backpropagation (BP) algorithm. The forecast accuracy is compared with the forecasts used by Victorian Power Exchange (VPX) and the actual energy demand. To evaluate, we considered load demand patterns for 10 consecutive months taken every 30 min for training the different prediction models. Test results show that the neuro-fuzzy system performed better than neural networks, ARIMA model and the VPX forecasts.",2004-05-05T00:27:53Z,http://arxiv.org/pdf/cs/0405010v1,2024-04-28,
cs/0405011v1,"Neuro Fuzzy Systems: Sate-of-the-Art Modeling Techniques","Fusion of Artificial Neural Networks (ANN) and Fuzzy Inference Systems (FIS) have attracted the growing interest of researchers in various scientific and engineering areas due to the growing need of adaptive intelligent systems to solve the real world problems. ANN learns from scratch by adjusting the interconnections between layers. FIS is a popular computing framework based on the concept of fuzzy set theory, fuzzy if-then rules, and fuzzy reasoning. The advantages of a combination of ANN and FIS are obvious. There are several approaches to integrate ANN and FIS and very often it depends on the application. We broadly classify the integration of ANN and FIS into three categories namely concurrent model, cooperative model and fully fused model. This paper starts with a discussion of the features of each model and generalize the advantages and deficiencies of each model. We further focus the review on the different types of fused neuro-fuzzy systems and citing the advantages and disadvantages of each model.",2004-05-05T00:32:52Z,http://arxiv.org/pdf/cs/0405011v1,2024-04-28,
cs/0405012v1,"Is Neural Network a Reliable Forecaster on Earth? A MARS Query!","Long-term rainfall prediction is a challenging task especially in the modern world where we are facing the major environmental problem of global warming. In general, climate and rainfall are highly non-linear phenomena in nature exhibiting what is known as the butterfly effect. While some regions of the world are noticing a systematic decrease in annual rainfall, others notice increases in flooding and severe storms. The global nature of this phenomenon is very complicated and requires sophisticated computer modeling and simulation to predict accurately. In this paper, we report a performance analysis for Multivariate Adaptive Regression Splines (MARS)and artificial neural networks for one month ahead prediction of rainfall. To evaluate the prediction efficiency, we made use of 87 years of rainfall data in Kerala state, the southern part of the Indian peninsula situated at latitude -longitude pairs (8o29'N - 76o57' E). We used an artificial neural network trained using the scaled conjugate gradient algorithm. The neural network and MARS were trained with 40 years of rainfall data. For performance evaluation, network predicted outputs were compared with the actual rainfall data. Simulation results reveal that MARS is a good forecasting tool and performed better than the considered neural network.",2004-05-05T00:36:17Z,http://arxiv.org/pdf/cs/0405012v1,2024-04-28,
cs/0405013v1,"DCT Based Texture Classification Using Soft Computing Approach","Classification of texture pattern is one of the most important problems in pattern recognition. In this paper, we present a classification method based on the Discrete Cosine Transform (DCT) coefficients of texture image. As DCT works on gray level image, the color scheme of each image is transformed into gray levels. For classifying the images using DCT we used two popular soft computing techniques namely neurocomputing and neuro-fuzzy computing. We used a feedforward neural network trained using the backpropagation learning and an evolving fuzzy neural network to classify the textures. The soft computing models were trained using 80% of the texture data and remaining was used for testing and validation purposes. A performance comparison was made among the soft computing models for the texture classification problem. We also analyzed the effects of prolonged training of neural networks. It is observed that the proposed neuro-fuzzy model performed better than neural network.",2004-05-05T00:44:12Z,http://arxiv.org/pdf/cs/0405013v1,2024-04-28,
cs/0405014v1,"Estimating Genome Reversal Distance by Genetic Algorithm","Sorting by reversals is an important problem in inferring the evolutionary relationship between two genomes. The problem of sorting unsigned permutation has been proven to be NP-hard. The best guaranteed error bounded is the 3/2- approximation algorithm. However, the problem of sorting signed permutation can be solved easily. Fast algorithms have been developed both for finding the sorting sequence and finding the reversal distance of signed permutation. In this paper, we present a way to view the problem of sorting unsigned permutation as signed permutation. And the problem can then be seen as searching an optimal signed permutation in all n2 corresponding signed permutations. We use genetic algorithm to conduct the search. Our experimental result shows that the proposed method outperform the 3/2-approximation algorithm.",2004-05-05T00:57:34Z,http://arxiv.org/pdf/cs/0405014v1,2024-04-28,
cs/0405016v1,"Intrusion Detection Systems Using Adaptive Regression Splines","Past few years have witnessed a growing recognition of intelligent techniques for the construction of efficient and reliable intrusion detection systems. Due to increasing incidents of cyber attacks, building effective intrusion detection systems (IDS) are essential for protecting information systems security, and yet it remains an elusive goal and a great challenge. In this paper, we report a performance analysis between Multivariate Adaptive Regression Splines (MARS), neural networks and support vector machines. The MARS procedure builds flexible regression models by fitting separate splines to distinct intervals of the predictor variables. A brief comparison of different neural network learning algorithms is also given.",2004-05-05T02:22:16Z,http://arxiv.org/pdf/cs/0405016v1,2024-04-28,
cs/0405017v1,"Data Mining Approach for Analyzing Call Center Performance","The aim of our research was to apply well-known data mining techniques (such as linear neural networks, multi-layered perceptrons, probabilistic neural networks, classification and regression trees, support vector machines and finally a hybrid decision tree neural network approach) to the problem of predicting the quality of service in call centers; based on the performance data actually collected in a call center of a large insurance company. Our aim was two-fold. First, to compare the performance of models built using the above-mentioned techniques and, second, to analyze the characteristics of the input sensitivity in order to better understand the relationship between the perform-ance evaluation process and the actual performance and in this way help improve the performance of call centers. In this paper we summarize our findings.",2004-05-05T02:27:43Z,http://arxiv.org/pdf/cs/0405017v1,2024-04-28,
cs/0405018v1,"Modeling Chaotic Behavior of Stock Indices Using Intelligent Paradigms","The use of intelligent systems for stock market predictions has been widely established. In this paper, we investigate how the seemingly chaotic behavior of stock markets could be well represented using several connectionist paradigms and soft computing techniques. To demonstrate the different techniques, we considered Nasdaq-100 index of Nasdaq Stock MarketS and the S&P CNX NIFTY stock index. We analyzed 7 year's Nasdaq 100 main index values and 4 year's NIFTY index values. This paper investigates the development of a reliable and efficient technique to model the seemingly chaotic behavior of stock markets. We considered an artificial neural network trained using Levenberg-Marquardt algorithm, Support Vector Machine (SVM), Takagi-Sugeno neuro-fuzzy model and a Difference Boosting Neural Network (DBNN). This paper briefly explains how the different connectionist paradigms could be formulated using different learning methods and then investigates whether they can provide the required level of performance, which are sufficiently good and robust so as to provide a reliable forecast model for stock market indices. Experiment results reveal that all the connectionist paradigms considered could represent the stock indices behavior very accurately.",2004-05-05T02:38:25Z,http://arxiv.org/pdf/cs/0405018v1,2024-04-28,
cs/0405019v1,"Hybrid Fuzzy-Linear Programming Approach for Multi Criteria Decision Making Problems","The purpose of this paper is to point to the usefulness of applying a linear mathematical formulation of fuzzy multiple criteria objective decision methods in organising business activities. In this respect fuzzy parameters of linear programming are modelled by preference-based membership functions. This paper begins with an introduction and some related research followed by some fundamentals of fuzzy set theory and technical concepts of fuzzy multiple objective decision models. Further a real case study of a manufacturing plant and the implementation of the proposed technique is presented. Empirical results clearly show the superiority of the fuzzy technique in optimising individual objective functions when compared to non-fuzzy approach. Furthermore, for the problem considered, the optimal solution helps to infer that by incorporating fuzziness in a linear programming model either in constraints, or both in objective functions and constraints, provides a similar (or even better) level of satisfaction for obtained results compared to non-fuzzy linear programming.",2004-05-05T02:44:41Z,http://arxiv.org/pdf/cs/0405019v1,2024-04-28,
cs/0405024v1,"Meta-Learning Evolutionary Artificial Neural Networks","In this paper, we present MLEANN (Meta-Learning Evolutionary Artificial Neural Network), an automatic computational framework for the adaptive optimization of artificial neural networks wherein the neural network architecture, activation function, connection weights; learning algorithm and its parameters are adapted according to the problem. We explored the performance of MLEANN and conventionally designed artificial neural networks for function approximation problems. To evaluate the comparative performance, we used three different well-known chaotic time series. We also present the state of the art popular neural network learning algorithms and some experimentation results related to convergence speed and generalization performance. We explored the performance of backpropagation algorithm; conjugate gradient algorithm, quasi-Newton algorithm and Levenberg-Marquardt algorithm for the three chaotic time series. Performances of the different learning algorithms were evaluated when the activation functions and architecture were changed. We further present the theoretical background, algorithm, design strategy and further demonstrate how effective and inevitable is the proposed MLEANN framework to design a neural network, which is smaller, faster and with a better generalization performance.",2004-05-06T13:44:20Z,http://arxiv.org/pdf/cs/0405024v1,2024-04-28,
cs/0405025v1,"The Largest Compatible Subset Problem for Phylogenetic Data","The phylogenetic tree construction is to infer the evolutionary relationship between species from the experimental data. However, the experimental data are often imperfect and conflicting each others. Therefore, it is important to extract the motif from the imperfect data. The largest compatible subset problem is that, given a set of experimental data, we want to discard the minimum such that the remaining is compatible. The largest compatible subset problem can be viewed as the vertex cover problem in the graph theory that has been proven to be NP-hard. In this paper, we propose a hybrid Evolutionary Computing (EC) method for this problem. The proposed method combines the EC approach and the algorithmic approach for special structured graphs. As a result, the complexity of the problem is dramatically reduced. Experiments were performed on randomly generated graphs with different edge densities. The vertex covers produced by the proposed method were then compared to the vertex covers produced by a 2-approximation algorithm. The experimental results showed that the proposed method consistently outperformed a classical 2- approximation algorithm. Furthermore, a significant improvement was found when the graph density was small.",2004-05-06T13:52:23Z,http://arxiv.org/pdf/cs/0405025v1,2024-04-28,
cs/0405026v1,"A Concurrent Fuzzy-Neural Network Approach for Decision Support Systems","Decision-making is a process of choosing among alternative courses of action for solving complicated problems where multi-criteria objectives are involved. The past few years have witnessed a growing recognition of Soft Computing technologies that underlie the conception, design and utilization of intelligent systems. Several works have been done where engineers and scientists have applied intelligent techniques and heuristics to obtain optimal decisions from imprecise information. In this paper, we present a concurrent fuzzy-neural network approach combining unsupervised and supervised learning techniques to develop the Tactical Air Combat Decision Support System (TACDSS). Experiment results clearly demonstrate the efficiency of the proposed technique.",2004-05-06T13:58:41Z,http://arxiv.org/pdf/cs/0405026v1,2024-04-28,
cs/0405028v1,"Analysis of Hybrid Soft and Hard Computing Techniques for Forex Monitoring Systems","In a universe with a single currency, there would be no foreign exchange market, no foreign exchange rates, and no foreign exchange. Over the past twenty-five years, the way the market has performed those tasks has changed enormously. The need for intelligent monitoring systems has become a necessity to keep track of the complex forex market. The vast currency market is a foreign concept to the average individual. However, once it is broken down into simple terms, the average individual can begin to understand the foreign exchange market and use it as a financial instrument for future investing. In this paper, we attempt to compare the performance of hybrid soft computing and hard computing techniques to predict the average monthly forex rates one month ahead. The soft computing models considered are a neural network trained by the scaled conjugate gradient algorithm and a neuro-fuzzy model implementing a Takagi-Sugeno fuzzy inference system. We also considered Multivariate Adaptive Regression Splines (MARS), Classification and Regression Trees (CART) and a hybrid CART-MARS technique. We considered the exchange rates of Australian dollar with respect to US dollar, Singapore dollar, New Zealand dollar, Japanese yen and United Kingdom pounds. The models were trained using 70% of the data and remaining was used for testing and validation purposes. It is observed that the proposed hybrid models could predict the forex rates more accurately than all the techniques when applied individually. Empirical results also reveal that the hybrid hard computing approach also improved some of our previous work using a neuro-fuzzy approach.",2004-05-07T00:10:07Z,http://arxiv.org/pdf/cs/0405028v1,2024-04-28,
cs/0405030v1,"Business Intelligence from Web Usage Mining","The rapid e-commerce growth has made both business community and customers face a new situation. Due to intense competition on one hand and the customer's option to choose from several alternatives business community has realized the necessity of intelligent marketing strategies and relationship management. Web usage mining attempts to discover useful knowledge from the secondary data obtained from the interactions of the users with the Web. Web usage mining has become very critical for effective Web site management, creating adaptive Web sites, business and support services, personalization, network traffic flow analysis and so on. In this paper, we present the important concepts of Web usage mining and its various practical applications. We further present a novel approach 'intelligent-miner' (i-Miner) to optimize the concurrent architecture of a fuzzy clustering algorithm (to discover web data clusters) and a fuzzy inference system to analyze the Web site visitor trends. A hybrid evolutionary fuzzy clustering algorithm is proposed in this paper to optimally segregate similar user interests. The clustered data is then used to analyze the trends using a Takagi-Sugeno fuzzy inference system learned using a combination of evolutionary algorithm and neural network learning. Proposed approach is compared with self-organizing maps (to discover patterns) and several function approximation techniques like neural networks, linear genetic programming and Takagi-Sugeno fuzzy inference system (to analyze the clusters). The results are graphically illustrated and the practical significance is discussed in detail. Empirical results clearly show that the proposed Web usage-mining framework is efficient.",2004-05-06T23:54:39Z,http://arxiv.org/pdf/cs/0405030v1,2024-04-28,
cs/0405031v1,"Adaptation of Mamdani Fuzzy Inference System Using Neuro - Genetic Approach for Tactical Air Combat Decision Support System","Normally a decision support system is build to solve problem where multi-criteria decisions are involved. The knowledge base is the vital part of the decision support containing the information or data that is used in decision-making process. This is the field where engineers and scientists have applied several intelligent techniques and heuristics to obtain optimal decisions from imprecise information. In this paper, we present a hybrid neuro-genetic learning approach for the adaptation a Mamdani fuzzy inference system for the Tactical Air Combat Decision Support System (TACDSS). Some simulation results demonstrating the difference of the learning techniques and are also provided.",2004-05-06T23:58:46Z,http://arxiv.org/pdf/cs/0405031v1,2024-04-28,
cs/0405032v1,"EvoNF: A Framework for Optimization of Fuzzy Inference Systems Using Neural Network Learning and Evolutionary Computation","Several adaptation techniques have been investigated to optimize fuzzy inference systems. Neural network learning algorithms have been used to determine the parameters of fuzzy inference system. Such models are often called as integrated neuro-fuzzy models. In an integrated neuro-fuzzy model there is no guarantee that the neural network learning algorithm converges and the tuning of fuzzy inference system will be successful. Success of evolutionary search procedures for optimization of fuzzy inference system is well proven and established in many application areas. In this paper, we will explore how the optimization of fuzzy inference systems could be further improved using a meta-heuristic approach combining neural network learning and evolutionary computation. The proposed technique could be considered as a methodology to integrate neural networks, fuzzy inference systems and evolutionary search procedures. We present the theoretical frameworks and some experimental results to demonstrate the efficiency of the proposed technique.",2004-05-07T00:01:54Z,http://arxiv.org/pdf/cs/0405032v1,2024-04-28,
cs/0405033v1,"Optimization of Evolutionary Neural Networks Using Hybrid Learning Algorithms","Evolutionary artificial neural networks (EANNs) refer to a special class of artificial neural networks (ANNs) in which evolution is another fundamental form of adaptation in addition to learning. Evolutionary algorithms are used to adapt the connection weights, network architecture and learning algorithms according to the problem environment. Even though evolutionary algorithms are well known as efficient global search algorithms, very often they miss the best local solutions in the complex solution space. In this paper, we propose a hybrid meta-heuristic learning approach combining evolutionary learning and local search methods (using 1st and 2nd order error information) to improve the learning and faster convergence obtained using a direct evolutionary approach. The proposed technique is tested on three different chaotic time series and the test results are compared with some popular neuro-fuzzy systems and a recently developed cutting angle method of global optimization. Empirical results reveal that the proposed technique is efficient in spite of the computational complexity.",2004-05-07T00:08:16Z,http://arxiv.org/pdf/cs/0405033v1,2024-04-28,
cs/0405049v1,"Export Behaviour Modeling Using EvoNF Approach","The academic literature suggests that the extent of exporting by multinational corporation subsidiaries (MCS) depends on their product manufactured, resources, tax protection, customers and markets, involvement strategy, financial independence and suppliers' relationship with a multinational corporation (MNC). The aim of this paper is to model the complex export pattern behaviour using a Takagi-Sugeno fuzzy inference system in order to determine the actual volume of MCS export output (sales exported). The proposed fuzzy inference system is optimised by using neural network learning and evolutionary computation. Empirical results clearly show that the proposed approach could model the export behaviour reasonable well compared to a direct neural network approach.",2004-05-16T03:24:55Z,http://arxiv.org/pdf/cs/0405049v1,2024-04-28,
cs/0405050v1,"Traffic Accident Analysis Using Decision Trees and Neural Networks","The costs of fatalities and injuries due to traffic accident have a great impact on society. This paper presents our research to model the severity of injury resulting from traffic accidents using artificial neural networks and decision trees. We have applied them to an actual data set obtained from the National Automotive Sampling System (NASS) General Estimates System (GES). Experiment results reveal that in all the cases the decision tree outperforms the neural network. Our research analysis also shows that the three most important factors in fatal injury are: driver's seat belt usage, light condition of the roadway, and driver's alcohol usage.",2004-05-16T03:33:20Z,http://arxiv.org/pdf/cs/0405050v1,2024-04-28,
cs/0405051v1,"Short Term Load Forecasting Models in Czech Republic Using Soft Computing Paradigms","This paper presents a comparative study of six soft computing models namely multilayer perceptron networks, Elman recurrent neural network, radial basis function network, Hopfield model, fuzzy inference system and hybrid fuzzy neural network for the hourly electricity demand forecast of Czech Republic. The soft computing models were trained and tested using the actual hourly load data for seven years. A comparison of the proposed techniques is presented for predicting 2 day ahead demands for electricity. Simulation results indicate that hybrid fuzzy neural network and radial basis function networks are the best candidates for the analysis and forecasting of electricity demand.",2004-05-16T03:44:06Z,http://arxiv.org/pdf/cs/0405051v1,2024-04-28,
cs/0405052v1,"Decision Support Systems Using Intelligent Paradigms","Decision-making is a process of choosing among alternative courses of action for solving complicated problems where multi-criteria objectives are involved. The past few years have witnessed a growing recognition of Soft Computing (SC) technologies that underlie the conception, design and utilization of intelligent systems. In this paper, we present different SC paradigms involving an artificial neural network trained using the scaled conjugate gradient algorithm, two different fuzzy inference methods optimised using neural network learning/evolutionary algorithms and regression trees for developing intelligent decision support systems. We demonstrate the efficiency of the different algorithms by developing a decision support system for a Tactical Air Combat Environment (TACE). Some empirical comparisons between the different algorithms are also provided.",2004-05-16T03:50:05Z,http://arxiv.org/pdf/cs/0405052v1,2024-04-28,
cs/0405071v1,"Regression with respect to sensing actions and partial states","In this paper, we present a state-based regression function for planning domains where an agent does not have complete information and may have sensing actions. We consider binary domains and employ the 0-approximation [Son & Baral 2001] to define the regression function. In binary domains, the use of 0-approximation means using 3-valued states. Although planning using this approach is incomplete with respect to the full semantics, we adopt it to have a lower complexity. We prove the soundness and completeness of our regression formulation with respect to the definition of progression. More specifically, we show that (i) a plan obtained through regression for a planning problem is indeed a progression solution of that planning problem, and that (ii) for each plan found through progression, using regression one obtains that plan or an equivalent one. We then develop a conditional planner that utilizes our regression function. We prove the soundness and completeness of our planning algorithm and present experimental results with respect to several well known planning problems in the literature.",2004-05-21T12:43:19Z,http://arxiv.org/pdf/cs/0405071v1,2024-04-28,
cs/0405090v1,"Propositional Defeasible Logic has Linear Complexity","Defeasible logic is a rule-based nonmonotonic logic, with both strict and defeasible rules, and a priority relation on rules. We show that inference in the propositional form of the logic can be performed in linear time. This contrasts markedly with most other propositional nonmonotonic logics, in which inference is intractable.",2004-05-24T15:45:59Z,http://arxiv.org/pdf/cs/0405090v1,2024-04-28,
cs/0405106v1,"Pruning Search Space in Defeasible Argumentation","Defeasible argumentation has experienced a considerable growth in AI in the last decade. Theoretical results have been combined with development of practical applications in AI & Law, Case-Based Reasoning and various knowledge-based systems. However, the dialectical process associated with inference is computationally expensive. This paper focuses on speeding up this inference process by pruning the involved search space. Our approach is twofold. On one hand, we identify distinguished literals for computing defeat. On the other hand, we restrict ourselves to a subset of all possible conflicting arguments by introducing dialectical constraints.",2004-05-27T18:43:39Z,http://arxiv.org/pdf/cs/0405106v1,2024-04-28,
cs/0405113v2,"A proposal to design expert system for the calculations in the domain of QFT","Main purposes of the paper are followings: 1) To show examples of the calculations in domain of QFT via ``derivative rules'' of an expert system; 2) To consider advantages and disadvantage that technology of the calculations; 3) To reflect about how one would develop new physical theories, what knowledge would be useful in their investigations and how this problem can be connected with designing an expert system.",2004-05-31T10:50:23Z,http://arxiv.org/pdf/cs/0405113v2,2024-04-28,
cs/0406038v1,"A New Approach to Draw Detection by Move Repetition in Computer Chess Programming","We will try to tackle both the theoretical and practical aspects of a very important problem in chess programming as stated in the title of this article - the issue of draw detection by move repetition. The standard approach that has so far been employed in most chess programs is based on utilising positional matrices in original and compressed format as well as on the implementation of the so-called bitboard format.   The new approach that we will be trying to introduce is based on using variant strings generated by the search algorithm (searcher) during the tree expansion in decision making. We hope to prove that this approach is more efficient than the standard treatment of the issue, especially in positions with few pieces (endgames). To illustrate what we have in mind a machine language routine that implements our theoretical assumptions is attached. The routine is part of the Axon chess program, developed by the authors. Axon, in its current incarnation, plays chess at master strength (ca. 2400-2450 Elo, based on both Axon vs computer programs and Axon vs human masters in over 3000 games altogether).",2004-06-21T13:42:03Z,http://arxiv.org/pdf/cs/0406038v1,2024-04-28,
cs/0407008v1,"Autogenic Training With Natural Language Processing Modules: A Recent Tool For Certain Neuro Cognitive Studies","Learning to respond to voice-text input involves the subject's ability in understanding the phonetic and text based contents and his/her ability to communicate based on his/her experience. The neuro-cognitive facility of the subject has to support two important domains in order to make the learning process complete. In many cases, though the understanding is complete, the response is partial. This is one valid reason why we need to support the information from the subject with scalable techniques such as Natural Language Processing (NLP) for abstraction of the contents from the output. This paper explores the feasibility of using NLP modules interlaced with Neural Networks to perform the required task in autogenic training related to medical applications.",2004-07-02T20:15:02Z,http://arxiv.org/pdf/cs/0407008v1,2024-04-28,
cs/0407037v1,"Generalized Evolutionary Algorithm based on Tsallis Statistics","Generalized evolutionary algorithm based on Tsallis canonical distribution is proposed. The algorithm uses Tsallis generalized canonical distribution to weigh the configurations for `selection' instead of Gibbs-Boltzmann distribution. Our simulation results show that for an appropriate choice of non-extensive index that is offered by Tsallis statistics, evolutionary algorithms based on this generalization outperform algorithms based on Gibbs-Boltzmann distribution.",2004-07-16T06:08:22Z,http://arxiv.org/pdf/cs/0407037v1,2024-04-28,
cs/0407040v1,"Decomposition Based Search - A theoretical and experimental evaluation","In this paper we present and evaluate a search strategy called Decomposition Based Search (DBS) which is based on two steps: subproblem generation and subproblem solution. The generation of subproblems is done through value ranking and domain splitting. Subdomains are explored so as to generate, according to the heuristic chosen, promising subproblems first.   We show that two well known search strategies, Limited Discrepancy Search (LDS) and Iterative Broadening (IB), can be seen as special cases of DBS. First we present a tuning of DBS that visits the same search nodes as IB, but avoids restarts. Then we compare both theoretically and computationally DBS and LDS using the same heuristic. We prove that DBS has a higher probability of being successful than LDS on a comparable number of nodes, under realistic assumptions. Experiments on a constraint satisfaction problem and an optimization problem show that DBS is indeed very effective if compared to LDS.",2004-07-16T13:38:19Z,http://arxiv.org/pdf/cs/0407040v1,2024-04-28,
cs/0407042v1,"Postponing Branching Decisions","Solution techniques for Constraint Satisfaction and Optimisation Problems often make use of backtrack search methods, exploiting variable and value ordering heuristics. In this paper, we propose and analyse a very simple method to apply in case the value ordering heuristic produces ties: postponing the branching decision. To this end, we group together values in a tie, branch on this sub-domain, and defer the decision among them to lower levels of the search tree. We show theoretically and experimentally that this simple modification can dramatically improve the efficiency of the search strategy. Although in practise similar methods may have been applied already, to our knowledge, no empirical or theoretical study has been proposed in the literature to identify when and to what extent this strategy should be used.",2004-07-16T14:37:11Z,http://arxiv.org/pdf/cs/0407042v1,2024-04-28,
cs/0407044v1,"Reduced cost-based ranking for generating promising subproblems","In this paper, we propose an effective search procedure that interleaves two steps: subproblem generation and subproblem solution. We mainly focus on the first part. It consists of a variable domain value ranking based on reduced costs. Exploiting the ranking, we generate, in a Limited Discrepancy Search tree, the most promising subproblems first. An interesting result is that reduced costs provide a very precise ranking that allows to almost always find the optimal solution in the first generated subproblem, even if its dimension is significantly smaller than that of the original problem. Concerning the proof of optimality, we exploit a way to increase the lower bound for subproblems at higher discrepancies. We show experimental results on the TSP and its time constrained variant to show the effectiveness of the proposed approach, but the technique could be generalized for other problems.",2004-07-16T14:53:21Z,http://arxiv.org/pdf/cs/0407044v1,2024-04-28,
cs/0408010v5,"A Simple Proportional Conflict Redistribution Rule","One proposes a first alternative rule of combination to WAO (Weighted Average Operator) proposed recently by Josang, Daniel and Vannoorenberghe, called Proportional Conflict Redistribution rule (denoted PCR1). PCR1 and WAO are particular cases of WO (the Weighted Operator) because the conflicting mass is redistributed with respect to some weighting factors. In this first PCR rule, the proportionalization is done for each non-empty set with respect to the non-zero sum of its corresponding mass matrix - instead of its mass column average as in WAO, but the results are the same as Ph. Smets has pointed out. Also, we extend WAO (which herein gives no solution) for the degenerate case when all column sums of all non-empty sets are zero, and then the conflicting mass is transferred to the non-empty disjunctive form of all non-empty sets together; but if this disjunctive form happens to be empty, then one considers an open world (i.e. the frame of discernment might contain new hypotheses) and thus all conflicting mass is transferred to the empty set. In addition to WAO, we propose a general formula for PCR1 (WAO for non-degenerate cases).",2004-08-03T16:08:37Z,http://arxiv.org/pdf/cs/0408010v5,2024-04-28,
cs/0408021v2,"An Algorithm for Quasi-Associative and Quasi-Markovian Rules of Combination in Information Fusion","In this paper one proposes a simple algorithm of combining the fusion rules, those rules which first use the conjunctive rule and then the transfer of conflicting mass to the non-empty sets, in such a way that they gain the property of associativity and fulfill the Markovian requirement for dynamic fusion. Also, a new rule, SDL-improved, is presented.",2004-08-08T19:41:23Z,http://arxiv.org/pdf/cs/0408021v2,2024-04-28,
cs/0408044v1,"FLUX: A Logic Programming Method for Reasoning Agents","FLUX is a programming method for the design of agents that reason logically about their actions and sensor information in the presence of incomplete knowledge. The core of FLUX is a system of Constraint Handling Rules, which enables agents to maintain an internal model of their environment by which they control their own behavior. The general action representation formalism of the fluent calculus provides the formal semantics for the constraint solver. FLUX exhibits excellent computational behavior due to both a carefully restricted expressiveness and the inference paradigm of progression.",2004-08-19T14:47:51Z,http://arxiv.org/pdf/cs/0408044v1,2024-04-28,
cs/0408055v1,"Cauchy Annealing Schedule: An Annealing Schedule for Boltzmann Selection Scheme in Evolutionary Algorithms","Boltzmann selection is an important selection mechanism in evolutionary algorithms as it has theoretical properties which help in theoretical analysis. However, Boltzmann selection is not used in practice because a good annealing schedule for the `inverse temperature' parameter is lacking. In this paper we propose a Cauchy annealing schedule for Boltzmann selection scheme based on a hypothesis that selection-strength should increase as evolutionary process goes on and distance between two selection strengths should decrease for the process to converge. To formalize these aspects, we develop formalism for selection mechanisms using fitness distributions and give an appropriate measure for selection-strength. In this paper, we prove an important result, by which we derive an annealing schedule called Cauchy annealing schedule. We demonstrate the novelty of proposed annealing schedule using simulations in the framework of genetic algorithms.",2004-08-24T11:21:06Z,http://arxiv.org/pdf/cs/0408055v1,2024-04-28,
cs/0408064v3,"Proportional Conflict Redistribution Rules for Information Fusion","In this paper we propose five versions of a Proportional Conflict Redistribution rule (PCR) for information fusion together with several examples. From PCR1 to PCR2, PCR3, PCR4, PCR5 one increases the complexity of the rules and also the exactitude of the redistribution of conflicting masses. PCR1 restricted from the hyper-power set to the power set and without degenerate cases gives the same result as the Weighted Average Operator (WAO) proposed recently by J{\o}sang, Daniel and Vannoorenberghe but does not satisfy the neutrality property of vacuous belief assignment. That's why improved PCR rules are proposed in this paper. PCR4 is an improvement of minC and Dempster's rules. The PCR rules redistribute the conflicting mass, after the conjunctive rule has been applied, proportionally with some functions depending on the masses assigned to their corresponding columns in the mass matrix. There are infinitely many ways these functions (weighting factors) can be chosen depending on the complexity one wants to deal with in specific applications and fusion systems. Any fusion combination rule is at some degree ad-hoc.",2004-08-28T03:08:39Z,http://arxiv.org/pdf/cs/0408064v3,2024-04-28,
cs/0409007v1,"The Generalized Pignistic Transformation","This paper presents in detail the generalized pignistic transformation (GPT) succinctly developed in the Dezert-Smarandache Theory (DSmT) framework as a tool for decision process. The GPT allows to provide a subjective probability measure from any generalized basic belief assignment given by any corpus of evidence. We mainly focus our presentation on the 3D case and provide the complete result obtained by the GPT and its validation drawn from the probability theory.",2004-09-06T17:47:06Z,http://arxiv.org/pdf/cs/0409007v1,2024-04-28,
cs/0409040v3,"Unification of Fusion Theories","Since no fusion theory neither rule fully satisfy all needed applications, the author proposes a Unification of Fusion Theories and a combination of fusion rules in solving problems/applications. For each particular application, one selects the most appropriate model, rule(s), and algorithm of implementation. We are working in the unification of the fusion theories and rules, which looks like a cooking recipe, better we'd say like a logical chart for a computer programmer, but we don't see another method to comprise/unify all things. The unification scenario presented herein, which is now in an incipient form, should periodically be updated incorporating new discoveries from the fusion and engineering research.",2004-09-23T02:02:44Z,http://arxiv.org/pdf/cs/0409040v3,2024-04-28,
cs/0410014v1,"Normal forms for Answer Sets Programming","Normal forms for logic programs under stable/answer set semantics are introduced. We argue that these forms can simplify the study of program properties, mainly consistency. The first normal form, called the {\em kernel} of the program, is useful for studying existence and number of answer sets. A kernel program is composed of the atoms which are undefined in the Well-founded semantics, which are those that directly affect the existence of answer sets. The body of rules is composed of negative literals only. Thus, the kernel form tends to be significantly more compact than other formulations. Also, it is possible to check consistency of kernel programs in terms of colorings of the Extended Dependency Graph program representation which we previously developed. The second normal form is called {\em 3-kernel.} A 3-kernel program is composed of the atoms which are undefined in the Well-founded semantics. Rules in 3-kernel programs have at most two conditions, and each rule either belongs to a cycle, or defines a connection between cycles. 3-kernel programs may have positive conditions. The 3-kernel normal form is very useful for the static analysis of program consistency, i.e., the syntactic characterization of existence of answer sets. This result can be obtained thanks to a novel graph-like representation of programs, called Cycle Graph which presented in the companion article \cite{Cos04b}.",2004-10-06T15:01:50Z,http://arxiv.org/pdf/cs/0410014v1,2024-04-28,
cs/0410033v2,"An In-Depth Look at Information Fusion Rules & the Unification of Fusion Theories","This paper may look like a glossary of the fusion rules and we also introduce new ones presenting their formulas and examples: Conjunctive, Disjunctive, Exclusive Disjunctive, Mixed Conjunctive-Disjunctive rules, Conditional rule, Dempster's, Yager's, Smets' TBM rule, Dubois-Prade's, Dezert-Smarandache classical and hybrid rules, Murphy's average rule, Inagaki-Lefevre-Colot-Vannoorenberghe Unified Combination rules [and, as particular cases: Iganaki's parameterized rule, Weighting Average Operator, minC (M. Daniel), and newly Proportional Conflict Redistribution rules (Smarandache-Dezert) among which PCR5 is the most exact way of redistribution of the conflicting mass to non-empty sets following the path of the conjunctive rule], Zhang's Center Combination rule, Convolutive x-Averaging, Consensus Operator (Josang), Cautious Rule (Smets), ?-junctions rules (Smets), etc. and three new T-norm & T-conorm rules adjusted from fuzzy and neutrosophic sets to information fusion (Tchamova-Smarandache). Introducing the degree of union and degree of inclusion with respect to the cardinal of sets not with the fuzzy set point of view, besides that of intersection, many fusion rules can be improved. There are corner cases where each rule might have difficulties working or may not get an expected result.",2004-10-14T22:53:46Z,http://arxiv.org/pdf/cs/0410033v2,2024-04-28,
cs/0410049v1,"Intransitivity and Vagueness","There are many examples in the literature that suggest that indistinguishability is intransitive, despite the fact that the indistinguishability relation is typically taken to be an equivalence relation (and thus transitive). It is shown that if the uncertainty perception and the question of when an agent reports that two things are indistinguishable are both carefully modeled, the problems disappear, and indistinguishability can indeed be taken to be an equivalence relation. Moreover, this model also suggests a logic of vagueness that seems to solve many of the problems related to vagueness discussed in the philosophical literature. In particular, it is shown here how the logic can handle the sorites paradox.",2004-10-19T17:31:11Z,http://arxiv.org/pdf/cs/0410049v1,2024-04-28,
cs/0410050v1,"Sleeping Beauty Reconsidered: Conditioning and Reflection in Asynchronous Systems","A careful analysis of conditioning in the Sleeping Beauty problem is done, using the formal model for reasoning about knowledge and probability developed by Halpern and Tuttle. While the Sleeping Beauty problem has been viewed as revealing problems with conditioning in the presence of imperfect recall, the analysis done here reveals that the problems are not so much due to imperfect recall as to asynchrony. The implications of this analysis for van Fraassen's Reflection Principle and Savage's Sure-Thing Principle are considered.",2004-10-19T17:31:44Z,http://arxiv.org/pdf/cs/0410050v1,2024-04-28,
cs/0411015v1,"Bounded Input Bounded Predefined Control Bounded Output","The paper is an attempt to generalize a methodology, which is similar to the bounded-input bounded-output method currently widely used for the system stability studies. The presented earlier methodology allows decomposition of input space into bounded subspaces and defining for each subspace its bounding surface. It also defines a corresponding predefined control, which maps any point of a bounded input into a desired bounded output subspace. This methodology was improved by providing a mechanism for the fast defining a bounded surface. This paper presents enhanced bounded-input bounded-predefined-control bounded-output approach, which provides adaptability feature to the control and allows transferring of a controlled system along a suboptimal trajectory.",2004-11-08T01:52:58Z,http://arxiv.org/pdf/cs/0411015v1,2024-04-28,
cs/0411034v2,"Generating Conditional Probabilities for Bayesian Networks: Easing the Knowledge Acquisition Problem","The number of probability distributions required to populate a conditional probability table (CPT) in a Bayesian network, grows exponentially with the number of parent-nodes associated with that table. If the table is to be populated through knowledge elicited from a domain expert then the sheer magnitude of the task forms a considerable cognitive barrier. In this paper we devise an algorithm to populate the CPT while easing the extent of knowledge acquisition. The input to the algorithm consists of a set of weights that quantify the relative strengths of the influences of the parent-nodes on the child-node, and a set of probability distributions the number of which grows only linearly with the number of associated parent-nodes. These are elicited from the domain expert. The set of probabilities are obtained by taking into consideration the heuristics that experts use while arriving at probabilistic estimations. The algorithm is used to populate the CPT by computing appropriate weighted sums of the elicited distributions. We invoke the methods of information geometry to demonstrate how these weighted sums capture the expert's judgemental strategy.",2004-11-12T00:42:55Z,http://arxiv.org/pdf/cs/0411034v2,2024-04-28,
cs/0411071v1,"Comparing Multi-Target Trackers on Different Force Unit Levels","Consider the problem of tracking a set of moving targets. Apart from the tracking result, it is often important to know where the tracking fails, either to steer sensors to that part of the state-space, or to inform a human operator about the status and quality of the obtained information. An intuitive quality measure is the correlation between two tracking results based on uncorrelated observations. In the case of Bayesian trackers such a correlation measure could be the Kullback-Leibler difference.   We focus on a scenario with a large number of military units moving in some terrain. The units are observed by several types of sensors and ""meta-sensors"" with force aggregation capabilities. The sensors register units of different size. Two separate multi-target probability hypothesis density (PHD) particle filters are used to track some type of units (e.g., companies) and their sub-units (e.g., platoons), respectively, based on observations of units of those sizes. Each observation is used in one filter only.   Although the state-space may well be the same in both filters, the posterior PHD distributions are not directly comparable -- one unit might correspond to three or four spatially distributed sub-units. Therefore, we introduce a mapping function between distributions for different unit size, based on doctrine knowledge of unit configuration.   The mapped distributions can now be compared -- locally or globally -- using some measure, which gives the correlation between two PHD distributions in a bounded volume of the state-space. To locate areas where the tracking fails, a discretized quality map of the state-space can be generated by applying the measure locally to different parts of the space.",2004-11-19T13:12:40Z,http://arxiv.org/pdf/cs/0411071v1,2024-04-28,
cs/0411072v1,"Extremal optimization for sensor report pre-processing","We describe the recently introduced extremal optimization algorithm and apply it to target detection and association problems arising in pre-processing for multi-target tracking.   Here we consider the problem of pre-processing for multiple target tracking when the number of sensor reports received is very large and arrives in large bursts. In this case, it is sometimes necessary to pre-process reports before sending them to tracking modules in the fusion system. The pre-processing step associates reports to known tracks (or initializes new tracks for reports on objects that have not been seen before). It could also be used as a pre-process step before clustering, e.g., in order to test how many clusters to use.   The pre-processing is done by solving an approximate version of the original problem. In this approximation, not all pair-wise conflicts are calculated. The approximation relies on knowing how many such pair-wise conflicts that are necessary to compute. To determine this, results on phase-transitions occurring when coloring (or clustering) large random instances of a particular graph ensemble are used.",2004-11-19T13:37:40Z,http://arxiv.org/pdf/cs/0411072v1,2024-04-28,
cs/0412091v1,"The Combination of Paradoxical, Uncertain, and Imprecise Sources of Information based on DSmT and Neutro-Fuzzy Inference","The management and combination of uncertain, imprecise, fuzzy and even paradoxical or high conflicting sources of information has always been, and still remains today, of primal importance for the development of reliable modern information systems involving artificial reasoning. In this chapter, we present a survey of our recent theory of plausible and paradoxical reasoning, known as Dezert-Smarandache Theory (DSmT) in the literature, developed for dealing with imprecise, uncertain and paradoxical sources of information. We focus our presentation here rather on the foundations of DSmT, and on the two important new rules of combination, than on browsing specific applications of DSmT available in literature. Several simple examples are given throughout the presentation to show the efficiency and the generality of this new approach. The last part of this chapter concerns the presentation of the neutrosophic logic, the neutro-fuzzy inference and its connection with DSmT. Fuzzy logic and neutrosophic logic are useful tools in decision making after fusioning the information using the DSm hybrid rule of combination of masses.",2004-12-19T14:56:11Z,http://arxiv.org/pdf/cs/0412091v1,2024-04-28,
cs/0501068v1,"Learning to automatically detect features for mobile robots using second-order Hidden Markov Models","In this paper, we propose a new method based on Hidden Markov Models to interpret temporal sequences of sensor data from mobile robots to automatically detect features. Hidden Markov Models have been used for a long time in pattern recognition, especially in speech recognition. Their main advantages over other methods (such as neural networks) are their ability to model noisy temporal signals of variable length. We show in this paper that this approach is well suited for interpretation of temporal sequences of mobile-robot sensor data. We present two distinct experiments and results: the first one in an indoor environment where a mobile robot learns to detect features like open doors or T-intersections, the second one in an outdoor environment where a different mobile robot has to identify situations like climbing a hill or crossing a rock.",2005-01-24T11:05:36Z,http://arxiv.org/pdf/cs/0501068v1,2024-04-28,
cs/0501072v1,"Inferring knowledge from a large semantic network","In this paper, we present a rich semantic network based on a differential analysis. We then detail implemented measures that take into account common and differential features between words. In a last section, we describe some industrial applications.",2005-01-25T16:09:11Z,http://arxiv.org/pdf/cs/0501072v1,2024-04-28,
cs/0501084v1,"Towards Automated Integration of Guess and Check Programs in Answer Set Programming: A Meta-Interpreter and Applications","Answer set programming (ASP) with disjunction offers a powerful tool for declaratively representing and solving hard problems. Many NP-complete problems can be encoded in the answer set semantics of logic programs in a very concise and intuitive way, where the encoding reflects the typical ""guess and check"" nature of NP problems: The property is encoded in a way such that polynomial size certificates for it correspond to stable models of a program. However, the problem-solving capacity of full disjunctive logic programs (DLPs) is beyond NP, and captures a class of problems at the second level of the polynomial hierarchy. While these problems also have a clear ""guess and check"" structure, finding an encoding in a DLP reflecting this structure may sometimes be a non-obvious task, in particular if the ""check"" itself is a coNP-complete problem; usually, such problems are solved by interleaving separate guess and check programs, where the check is expressed by inconsistency of the check program. In this paper, we present general transformations of head-cycle free (extended) disjunctive logic programs into stratified and positive (extended) disjunctive logic programs based on meta-interpretation techniques. The answer sets of the original and the transformed program are in simple correspondence, and, moreover, inconsistency of the original program is indicated by a designated answer set of the transformed program. Our transformations facilitate the integration of separate ""guess"" and ""check"" programs, which are often easy to obtain, automatically into a single disjunctive logic program. Our results complement recent results on meta-interpretation in ASP, and extend methods and techniques for a declarative ""guess and check"" problem solving paradigm through ASP.",2005-01-28T20:19:12Z,http://arxiv.org/pdf/cs/0501084v1,2024-04-28,
cs/0501086v1,"Clever Search: A WordNet Based Wrapper for Internet Search Engines","This paper presents an approach to enhance search engines with information about word senses available in WordNet. The approach exploits information about the conceptual relations within the lexical-semantic net. In the wrapper for search engines presented, WordNet information is used to specify user's request or to classify the results of a publicly available web search engine, like google, yahoo, etc.",2005-01-31T16:00:22Z,http://arxiv.org/pdf/cs/0501086v1,2024-04-28,
cs/0501089v1,"Issues in Exploiting GermaNet as a Resource in Real Applications","This paper reports about experiments with GermaNet as a resource within domain specific document analysis. The main question to be answered is: How is the coverage of GermaNet in a specific domain? We report about results of a field test of GermaNet for analyses of autopsy protocols and present a sketch about the integration of GermaNet inside XDOC. Our remarks will contribute to a GermaNet user's wish list.",2005-01-31T10:07:52Z,http://arxiv.org/pdf/cs/0501089v1,2024-04-28,
cs/0501093v1,"Transforming Business Rules Into Natural Language Text","The aim of the project presented in this paper is to design a system for an NLG architecture, which supports the documentation process of eBusiness models. A major task is to enrich the formal description of an eBusiness model with additional information needed in an NLG task.",2005-01-31T07:59:14Z,http://arxiv.org/pdf/cs/0501093v1,2024-04-28,
cs/0501094v2,"Corpus based Enrichment of GermaNet Verb Frames","Lexical semantic resources, like WordNet, are often used in real applications of natural language document processing. For example, we integrated GermaNet in our document suite XDOC of processing of German forensic autopsy protocols. In addition to the hypernymy and synonymy relation, we want to adapt GermaNet's verb frames for our analysis. In this paper we outline an approach for the domain related enrichment of GermaNet verb frames by corpus based syntactic and co-occurred data analyses of real documents.",2005-01-31T08:36:39Z,http://arxiv.org/pdf/cs/0501094v2,2024-04-28,
cs/0501095v1,"Context Related Derivation of Word Senses","Real applications of natural language document processing are very often confronted with domain specific lexical gaps during the analysis of documents of a new domain. This paper describes an approach for the derivation of domain specific concepts for the extension of an existing ontology. As resources we need an initial ontology and a partially processed corpus of a domain. We exploit the specific characteristic of the sublanguage in the corpus. Our approach is based on syntactical structures (noun phrases) and compound analyses to extract information required for the extension of GermaNet's lexical resources.",2005-01-31T09:25:29Z,http://arxiv.org/pdf/cs/0501095v1,2024-04-28,
cs/0501096v1,"Transforming and Enriching Documents for the Semantic Web","We suggest to employ techniques from Natural Language Processing (NLP) and Knowledge Representation (KR) to transform existing documents into documents amenable for the Semantic Web. Semantic Web documents have at least part of their semantics and pragmatics marked up explicitly in both a machine processable as well as human readable manner. XML and its related standards (XSLT, RDF, Topic Maps etc.) are the unifying platform for the tools and methodologies developed for different application scenarios.",2005-01-31T09:48:46Z,http://arxiv.org/pdf/cs/0501096v1,2024-04-28,
cs/0502060v1,"Perspectives for Strong Artificial Life","This text introduces the twin deadlocks of strong artificial life. Conceptualization of life is a deadlock both because of the existence of a continuum between the inert and the living, and because we only know one instance of life. Computationalism is a second deadlock since it remains a matter of faith. Nevertheless, artificial life realizations quickly progress and recent constructions embed an always growing set of the intuitive properties of life. This growing gap between theory and realizations should sooner or later crystallize in some kind of paradigm shift and then give clues to break the twin deadlocks.",2005-02-13T18:20:48Z,http://arxiv.org/pdf/cs/0502060v1,2024-04-28,
cs/0504064v1,"Neural-Network Techniques for Visual Mining Clinical Electroencephalograms","In this chapter we describe new neural-network techniques developed for visual mining clinical electroencephalograms (EEGs), the weak electrical potentials invoked by brain activity. These techniques exploit fruitful ideas of Group Method of Data Handling (GMDH). Section 2 briefly describes the standard neural-network techniques which are able to learn well-suited classification modes from data presented by relevant features. Section 3 introduces an evolving cascade neural network technique which adds new input nodes as well as new neurons to the network while the training error decreases. This algorithm is applied to recognize artifacts in the clinical EEGs. Section 4 presents the GMDH-type polynomial networks learnt from data. We applied this technique to distinguish the EEGs recorded from an Alzheimer and a healthy patient as well as recognize EEG artifacts. Section 5 describes the new neural-network technique developed to induce multi-class concepts from data. We used this technique for inducing a 16-class concept from the large-scale clinical EEG data. Finally we discuss perspectives of applying the neural-network techniques to clinical EEGs.",2005-04-14T10:27:55Z,http://arxiv.org/pdf/cs/0504064v1,2024-04-28,
cs/0504065v1,"Estimating Classification Uncertainty of Bayesian Decision Tree Technique on Financial Data","Bayesian averaging over classification models allows the uncertainty of classification outcomes to be evaluated, which is of crucial importance for making reliable decisions in applications such as financial in which risks have to be estimated. The uncertainty of classification is determined by a trade-off between the amount of data available for training, the diversity of a classifier ensemble and the required performance. The interpretability of classification models can also give useful information for experts responsible for making reliable classifications. For this reason Decision Trees (DTs) seem to be attractive classification models. The required diversity of the DT ensemble can be achieved by using the Bayesian model averaging all possible DTs. In practice, the Bayesian approach can be implemented on the base of a Markov Chain Monte Carlo (MCMC) technique of random sampling from the posterior distribution. For sampling large DTs, the MCMC method is extended by Reversible Jump technique which allows inducing DTs under given priors. For the case when the prior information on the DT size is unavailable, the sweeping technique defining the prior implicitly reveals a better performance. Within this Chapter we explore the classification uncertainty of the Bayesian MCMC techniques on some datasets from the StatLog Repository and real financial data. The classification uncertainty is compared within an Uncertainty Envelope technique dealing with the class posterior distribution and a given confidence probability. This technique provides realistic estimates of the classification uncertainty which can be easily interpreted in statistical terms with the aim of risk evaluation.",2005-04-14T10:30:54Z,http://arxiv.org/pdf/cs/0504065v1,2024-04-28,
cs/0504066v1,"Comparison of the Bayesian and Randomised Decision Tree Ensembles within an Uncertainty Envelope Technique","Multiple Classifier Systems (MCSs) allow evaluation of the uncertainty of classification outcomes that is of crucial importance for safety critical applications. The uncertainty of classification is determined by a trade-off between the amount of data available for training, the classifier diversity and the required performance. The interpretability of MCSs can also give useful information for experts responsible for making reliable classifications. For this reason Decision Trees (DTs) seem to be attractive classification models for experts. The required diversity of MCSs exploiting such classification models can be achieved by using two techniques, the Bayesian model averaging and the randomised DT ensemble. Both techniques have revealed promising results when applied to real-world problems. In this paper we experimentally compare the classification uncertainty of the Bayesian model averaging with a restarting strategy and the randomised DT ensemble on a synthetic dataset and some domain problems commonly used in the machine learning community. To make the Bayesian DT averaging feasible, we use a Markov Chain Monte Carlo technique. The classification uncertainty is evaluated within an Uncertainty Envelope technique dealing with the class posterior distribution and a given confidence probability. Exploring a full posterior distribution, this technique produces realistic estimates which can be easily interpreted in statistical terms. In our experiments we found out that the Bayesian DTs are superior to the randomised DT ensembles within the Uncertainty Envelope technique.",2005-04-14T10:33:33Z,http://arxiv.org/pdf/cs/0504066v1,2024-04-28,
cs/0504071v1,"Proceedings of the Pacific Knowledge Acquisition Workshop 2004","Artificial intelligence (AI) research has evolved over the last few decades and knowledge acquisition research is at the core of AI research. PKAW-04 is one of three international knowledge acquisition workshops held in the Pacific-Rim, Canada and Europe over the last two decades. PKAW-04 has a strong emphasis on incremental knowledge acquisition, machine learning, neural nets and active mining.   The proceedings contain 19 papers that were selected by the program committee among 24 submitted papers. All papers were peer reviewed by at least two reviewers. The papers in these proceedings cover the methods and tools as well as the applications related to develop expert systems or knowledge based systems.",2005-04-14T13:14:53Z,http://arxiv.org/pdf/cs/0504071v1,2024-04-28,
cs/0505018v1,"Temporal and Spatial Data Mining with Second-Order Hidden Models","In the frame of designing a knowledge discovery system, we have developed stochastic models based on high-order hidden Markov models. These models are capable to map sequences of data into a Markov chain in which the transitions between the states depend on the \texttt{n} previous states according to the order of the model. We study the process of achieving information extraction fromspatial and temporal data by means of an unsupervised classification. We use therefore a French national database related to the land use of a region, named Teruti, which describes the land use both in the spatial and temporal domain. Land-use categories (wheat, corn, forest, ...) are logged every year on each site regularly spaced in the region. They constitute a temporal sequence of images in which we look for spatial and temporal dependencies. The temporal segmentation of the data is done by means of a second-order Hidden Markov Model (\hmmd) that appears to have very good capabilities to locate stationary segments, as shown in our previous work in speech recognition. Thespatial classification is performed by defining a fractal scanning ofthe images with the help of a Hilbert-Peano curve that introduces atotal order on the sites, preserving the relation ofneighborhood between the sites. We show that the \hmmd performs aclassification that is meaningful for the agronomists.Spatial and temporal classification may be achieved simultaneously by means of a 2 levels \hmmd that measures the \aposteriori probability to map a temporal sequence of images onto a set of hidden classes.",2005-05-09T06:54:57Z,http://arxiv.org/pdf/cs/0505018v1,2024-04-28,
cs/0505081v1,"An ontological approach to the construction of problem-solving models","Our ongoing work aims at defining an ontology-centered approach for building expertise models for the CommonKADS methodology. This approach (which we have named ""OntoKADS"") is founded on a core problem-solving ontology which distinguishes between two conceptualization levels: at an object level, a set of concepts enable us to define classes of problem-solving situations, and at a meta level, a set of meta-concepts represent modeling primitives. In this article, our presentation of OntoKADS will focus on the core ontology and, in particular, on roles - the primitive situated at the interface between domain knowledge and reasoning, and whose ontological status is still much debated. We first propose a coherent, global, ontological framework which enables us to account for this primitive. We then show how this novel characterization of the primitive allows definition of new rules for the construction of expertise models.",2005-05-30T13:42:02Z,http://arxiv.org/pdf/cs/0505081v1,2024-04-28,
cs/0506031v1,"A Constrained Object Model for Configuration Based Workflow Composition","Automatic or assisted workflow composition is a field of intense research for applications to the world wide web or to business process modeling. Workflow composition is traditionally addressed in various ways, generally via theorem proving techniques. Recent research observed that building a composite workflow bears strong relationships with finite model search, and that some workflow languages can be defined as constrained object metamodels . This lead to consider the viability of applying configuration techniques to this problem, which was proven feasible. Constrained based configuration expects a constrained object model as input. The purpose of this document is to formally specify the constrained object model involved in ongoing experiments and research using the Z specification language.",2005-06-09T14:57:53Z,http://arxiv.org/pdf/cs/0506031v1,2024-04-28,
cs/0507010v1,"A Study for the Feature Core of Dynamic Reduct","To the reduct problems of decision system, the paper proposes the notion of dynamic core according to the dynamic reduct model. It describes various formal definitions of dynamic core, and discusses some properties about dynamic core. All of these show that dynamic core possesses the essential characters of the feature core.",2005-07-05T13:02:02Z,http://arxiv.org/pdf/cs/0507010v1,2024-04-28,
cs/0507023v1,"Two-dimensional cellular automata and the analysis of correlated time series","Correlated time series are time series that, by virtue of the underlying process to which they refer, are expected to influence each other strongly. We introduce a novel approach to handle such time series, one that models their interaction as a two-dimensional cellular automaton and therefore allows them to be treated as a single entity. We apply our approach to the problems of filling gaps and predicting values in rainfall time series. Computational results show that the new approach compares favorably to Kalman smoothing and filtering.",2005-07-08T12:47:38Z,http://arxiv.org/pdf/cs/0507023v1,2024-04-28,
cs/0507029v1,"ATNoSFERES revisited","ATNoSFERES is a Pittsburgh style Learning Classifier System (LCS) in which the rules are represented as edges of an Augmented Transition Network. Genotypes are strings of tokens of a stack-based language, whose execution builds the labeled graph. The original ATNoSFERES, using a bitstring to represent the language tokens, has been favorably compared in previous work to several Michigan style LCSs architectures in the context of Non Markov problems. Several modifications of ATNoSFERES are proposed here: the most important one conceptually being a representational change: each token is now represented by an integer, hence the genotype is a string of integers; several other modifications of the underlying grammar language are also proposed. The resulting ATNoSFERES-II is validated on several standard animat Non Markov problems, on which it outperforms all previously published results in the LCS literature. The reasons for these improvement are carefully analyzed, and some assumptions are proposed on the underlying mechanisms in order to explain these good results.",2005-07-11T13:11:25Z,http://arxiv.org/pdf/cs/0507029v1,2024-04-28,
cs/0508132v1,"Planning with Preferences using Logic Programming","We present a declarative language, PP, for the high-level specification of preferences between possible solutions (or trajectories) of a planning problem. This novel language allows users to elegantly express non-trivial, multi-dimensional preferences and priorities over such preferences. The semantics of PP allows the identification of most preferred trajectories for a given goal. We also provide an answer set programming implementation of planning problems with PP preferences.",2005-08-31T14:50:22Z,http://arxiv.org/pdf/cs/0508132v1,2024-04-28,
cs/0509011v1,"Clustering Mixed Numeric and Categorical Data: A Cluster Ensemble Approach","Clustering is a widely used technique in data mining applications for discovering patterns in underlying data. Most traditional clustering algorithms are limited to handling datasets that contain either numeric or categorical attributes. However, datasets with mixed types of attributes are common in real life data mining applications. In this paper, we propose a novel divide-and-conquer technique to solve this problem. First, the original mixed dataset is divided into two sub-datasets: the pure categorical dataset and the pure numeric dataset. Next, existing well established clustering algorithms designed for different types of datasets are employed to produce corresponding clusters. Last, the clustering results on the categorical and numeric dataset are combined as a categorical dataset, on which the categorical data clustering algorithm is used to get the final clusters. Our contribution in this paper is to provide an algorithm framework for the mixed attributes clustering problem, in which existing clustering algorithms can be easily integrated, the capabilities of different kinds of clustering algorithms and characteristics of different types of datasets could be fully exploited. Comparisons with other clustering algorithms on real life datasets illustrate the superiority of our approach.",2005-09-05T02:47:12Z,http://arxiv.org/pdf/cs/0509011v1,2024-04-28,
cs/0509033v1,"K-Histograms: An Efficient Clustering Algorithm for Categorical Dataset","Clustering categorical data is an integral part of data mining and has attracted much attention recently. In this paper, we present k-histogram, a new efficient algorithm for clustering categorical data. The k-histogram algorithm extends the k-means algorithm to categorical domain by replacing the means of clusters with histograms, and dynamically updates histograms in the clustering process. Experimental results on real datasets show that k-histogram algorithm can produce better clustering results than k-modes algorithm, the one related with our work most closely.",2005-09-13T06:33:08Z,http://arxiv.org/pdf/cs/0509033v1,2024-04-28,
cs/0510050v1,"Integration of the DOLCE top-level ontology into the OntoSpec methodology","This report describes a new version of the OntoSpec methodology for ontology building. Defined by the LaRIA Knowledge Engineering Team (University of Picardie Jules Verne, Amiens, France), OntoSpec aims at helping builders to model ontological knowledge (upstream of formal representation). The methodology relies on a set of rigorously-defined modelling primitives and principles. Its application leads to the elaboration of a semi-informal ontology, which is independent of knowledge representation languages. We recently enriched the OntoSpec methodology by endowing it with a new resource, the DOLCE top-level ontology defined at the LOA (IST-CNR, Trento, Italy). The goal of this integration is to provide modellers with additional help in structuring application ontologies, while maintaining independence vis-\`{a}-vis formal representation languages. In this report, we first provide an overview of the OntoSpec methodology's general principles and then describe the DOLCE re-engineering process. A complete version of DOLCE-OS (i.e. a specification of DOLCE in the semi-informal OntoSpec language) is presented in an appendix.",2005-10-18T08:32:38Z,http://arxiv.org/pdf/cs/0510050v1,2024-04-28,
cs/0510062v1,"Using Interval Particle Filtering for Marker less 3D Human Motion Capture","In this paper we present a new approach for marker less human motion capture from conventional camera feeds. The aim of our study is to recover 3D positions of key points of the body that can serve for gait analysis. Our approach is based on foreground segmentation, an articulated body model and particle filters. In order to be generic and simple no restrictive dynamic modelling was used. A new modified particle filtering algorithm was introduced. It is used efficiently to search the model configuration space. This new algorithm which we call Interval Particle Filtering reorganizes the configurations search space in an optimal deterministic way and proved to be efficient in tracking natural human movement. Results for human motion capture from a single camera are presented and compared to results obtained from a marker based system. The system proved to be able to track motion successfully even in partial occlusions.",2005-10-21T13:45:15Z,http://arxiv.org/pdf/cs/0510062v1,2024-04-28,
cs/0510063v1,"Markerless Human Motion Capture for Gait Analysis","The aim of our study is to detect balance disorders and a tendency towards the falls in the elderly, knowing gait parameters. In this paper we present a new tool for gait analysis based on markerless human motion capture, from camera feeds. The system introduced here, recovers the 3D positions of several key points of the human body while walking. Foreground segmentation, an articulated body model and particle filtering are basic elements of our approach. No dynamic model is used thus this system can be described as generic and simple to implement. A modified particle filtering algorithm, which we call Interval Particle Filtering, is used to reorganise and search through the model's configurations search space in a deterministic optimal way. This algorithm was able to perform human movement tracking with success. Results from the treatment of a single cam feeds are shown and compared to results obtained using a marker based human motion capture system.",2005-10-21T13:45:49Z,http://arxiv.org/pdf/cs/0510063v1,2024-04-28,
cs/0510079v2,"Evidence with Uncertain Likelihoods","An agent often has a number of hypotheses, and must choose among them based on observations, or outcomes of experiments. Each of these observations can be viewed as providing evidence for or against various hypotheses. All the attempts to formalize this intuition up to now have assumed that associated with each hypothesis h there is a likelihood function \mu_h, which is a probability measure that intuitively describes how likely each observation is, conditional on h being the correct hypothesis. We consider an extension of this framework where there is uncertainty as to which of a number of likelihood functions is appropriate, and discuss how one formal approach to defining evidence, which views evidence as a function from priors to posteriors, can be generalized to accommodate this uncertainty.",2005-10-25T21:15:31Z,http://arxiv.org/pdf/cs/0510079v2,2024-04-28,
cs/0510083v1,"Neuronal Spectral Analysis of EEG and Expert Knowledge Integration for Automatic Classification of Sleep Stages","Being able to analyze and interpret signal coming from electroencephalogram (EEG) recording can be of high interest for many applications including medical diagnosis and Brain-Computer Interfaces. Indeed, human experts are today able to extract from this signal many hints related to physiological as well as cognitive states of the recorded subject and it would be very interesting to perform such task automatically but today no completely automatic system exists. In previous studies, we have compared human expertise and automatic processing tools, including artificial neural networks (ANN), to better understand the competences of each and determine which are the difficult aspects to integrate in a fully automatic system. In this paper, we bring more elements to that study in reporting the main results of a practical experiment which was carried out in an hospital for sleep pathology study. An EEG recording was studied and labeled by a human expert and an ANN. We describe here the characteristics of the experiment, both human and neuronal procedure of analysis, compare their performances and point out the main limitations which arise from this study.",2005-10-26T14:47:07Z,http://arxiv.org/pdf/cs/0510083v1,2024-04-28,
cs/0510091v1,"An efficient memetic, permutation-based evolutionary algorithm for real-world train timetabling","Train timetabling is a difficult and very tightly constrained combinatorial problem that deals with the construction of train schedules. We focus on the particular problem of local reconstruction of the schedule following a small perturbation, seeking minimisation of the total accumulated delay by adapting times of departure and arrival for each train and allocation of resources (tracks, routing nodes, etc.). We describe a permutation-based evolutionary algorithm that relies on a semi-greedy heuristic to gradually reconstruct the schedule by inserting trains one after the other following the permutation. This algorithm can be hybridised with ILOG commercial MIP programming tool CPLEX in a coarse-grained manner: the evolutionary part is used to quickly obtain a good but suboptimal solution and this intermediate solution is refined using CPLEX. Experimental results are presented on a large real-world case involving more than one million variables and 2 million constraints. Results are surprisingly good as the evolutionary algorithm, alone or hybridised, produces excellent solutions much faster than CPLEX alone.",2005-10-31T06:06:57Z,http://arxiv.org/pdf/cs/0510091v1,2024-04-28,
cs/0511004v1,"Evolutionary Computing","Evolutionary computing (EC) is an exciting development in Computer Science. It amounts to building, applying and studying algorithms based on the Darwinian principles of natural selection. In this paper we briefly introduce the main concepts behind evolutionary computing. We present the main components all evolutionary algorithms (EA), sketch the differences between different types of EAs and survey application areas ranging from optimization, modeling and simulation to entertainment.",2005-11-01T19:46:18Z,http://arxiv.org/pdf/cs/0511004v1,2024-04-28,
cs/0511015v2,"Towards a Hierarchical Model of Consciousness, Intelligence, Mind and Body","This article is taken out.",2005-11-03T16:28:05Z,http://arxiv.org/pdf/cs/0511015v2,2024-04-28,
cs/0511091v1,"Evolution of Voronoi based Fuzzy Recurrent Controllers","A fuzzy controller is usually designed by formulating the knowledge of a human expert into a set of linguistic variables and fuzzy rules. Among the most successful methods to automate the fuzzy controllers development process are evolutionary algorithms. In this work, we propose the Recurrent Fuzzy Voronoi (RFV) model, a representation for recurrent fuzzy systems. It is an extension of the FV model proposed by Kavka and Schoenauer that extends the application domain to include temporal problems. The FV model is a representation for fuzzy controllers based on Voronoi diagrams that can represent fuzzy systems with synergistic rules, fulfilling the $\epsilon$-completeness property and providing a simple way to introduce a priory knowledge. In the proposed representation, the temporal relations are embedded by including internal units that provide feedback by connecting outputs to inputs. These internal units act as memory elements. In the RFV model, the semantic of the internal units can be specified together with the a priori rules. The geometric interpretation of the rules allows the use of geometric variational operators during the evolution. The representation and the algorithms are validated in two problems in the area of system identification and evolutionary robotics.",2005-11-28T07:14:18Z,http://arxiv.org/pdf/cs/0511091v1,2024-04-28,
cs/0512045v2,"Branch-and-Prune Search Strategies for Numerical Constraint Solving","When solving numerical constraints such as nonlinear equations and inequalities, solvers often exploit pruning techniques, which remove redundant value combinations from the domains of variables, at pruning steps. To find the complete solution set, most of these solvers alternate the pruning steps with branching steps, which split each problem into subproblems. This forms the so-called branch-and-prune framework, well known among the approaches for solving numerical constraints. The basic branch-and-prune search strategy that uses domain bisections in place of the branching steps is called the bisection search. In general, the bisection search works well in case (i) the solutions are isolated, but it can be improved further in case (ii) there are continuums of solutions (this often occurs when inequalities are involved). In this paper, we propose a new branch-and-prune search strategy along with several variants, which not only allow yielding better branching decisions in the latter case, but also work as well as the bisection search does in the former case. These new search algorithms enable us to employ various pruning techniques in the construction of inner and outer approximations of the solution set. Our experiments show that these algorithms speed up the solving process often by one order of magnitude or more when solving problems with continuums of solutions, while keeping the same performance as the bisection search when the solutions are isolated.",2005-12-11T19:47:42Z,http://arxiv.org/pdf/cs/0512045v2,2024-04-28,
cs/0512047v2,"Processing Uncertainty and Indeterminacy in Information Systems success mapping","IS success is a complex concept, and its evaluation is complicated, unstructured and not readily quantifiable. Numerous scientific publications address the issue of success in the IS field as well as in other fields. But, little efforts have been done for processing indeterminacy and uncertainty in success research. This paper shows a formal method for mapping success using Neutrosophic Success Map. This is an emerging tool for processing indeterminacy and uncertainty in success research. EIS success have been analyzed using this tool.",2005-12-13T01:21:58Z,http://arxiv.org/pdf/cs/0512047v2,2024-04-28,
cs/0512099v1,"Mathematical Models in Schema Theory","In this paper, a mathematical schema theory is developed. This theory has three roots: brain theory schemas, grid automata, and block-shemas. In Section 2 of this paper, elements of the theory of grid automata necessary for the mathematical schema theory are presented. In Section 3, elements of brain theory necessary for the mathematical schema theory are presented. In Section 4, other types of schemas are considered. In Section 5, the mathematical schema theory is developed. The achieved level of schema representation allows one to model by mathematical tools virtually any type of schemas considered before, including schemas in neurophisiology, psychology, computer science, Internet technology, databases, logic, and mathematics.",2005-12-27T21:29:16Z,http://arxiv.org/pdf/cs/0512099v1,2024-04-28,
cs/0601001v2,"Truecluster: robust scalable clustering with model selection","Data-based classification is fundamental to most branches of science. While recent years have brought enormous progress in various areas of statistical computing and clustering, some general challenges in clustering remain: model selection, robustness, and scalability to large datasets. We consider the important problem of deciding on the optimal number of clusters, given an arbitrary definition of space and clusteriness. We show how to construct a cluster information criterion that allows objective model selection. Differing from other approaches, our truecluster method does not require specific assumptions about underlying distributions, dissimilarity definitions or cluster models. Truecluster puts arbitrary clustering algorithms into a generic unified (sampling-based) statistical framework. It is scalable to big datasets and provides robust cluster assignments and case-wise diagnostics. Truecluster will make clustering more objective, allows for automation, and will save time and costs. Free R software is available.",2006-01-02T13:17:09Z,http://arxiv.org/pdf/cs/0601001v2,2024-04-28,
cs/0601031v1,"Divide-and-Evolve: a New Memetic Scheme for Domain-Independent Temporal Planning","An original approach, termed Divide-and-Evolve is proposed to hybridize Evolutionary Algorithms (EAs) with Operational Research (OR) methods in the domain of Temporal Planning Problems (TPPs). Whereas standard Memetic Algorithms use local search methods to improve the evolutionary solutions, and thus fail when the local method stops working on the complete problem, the Divide-and-Evolve approach splits the problem at hand into several, hopefully easier, sub-problems, and can thus solve globally problems that are intractable when directly fed into deterministic OR algorithms. But the most prominent advantage of the Divide-and-Evolve approach is that it immediately opens up an avenue for multi-objective optimization, even though the OR method that is used is single-objective. Proof of concept approach on the standard (single-objective) Zeno transportation benchmark is given, and a small original multi-objective benchmark is proposed in the same Zeno framework to assess the multi-objective capabilities of the proposed methodology, a breakthrough in Temporal Planning.",2006-01-09T16:57:08Z,http://arxiv.org/pdf/cs/0601031v1,2024-04-28,
cs/0601052v1,"Artificial and Biological Intelligence","This article considers evidence from physical and biological sciences to show machines are deficient compared to biological systems at incorporating intelligence. Machines fall short on two counts: firstly, unlike brains, machines do not self-organize in a recursive manner; secondly, machines are based on classical logic, whereas Nature's intelligence may depend on quantum mechanics.",2006-01-13T19:01:42Z,http://arxiv.org/pdf/cs/0601052v1,2024-04-28,
cs/0601109v3,"Certainty Closure: Reliable Constraint Reasoning with Incomplete or Erroneous Data","Constraint Programming (CP) has proved an effective paradigm to model and solve difficult combinatorial satisfaction and optimisation problems from disparate domains. Many such problems arising from the commercial world are permeated by data uncertainty. Existing CP approaches that accommodate uncertainty are less suited to uncertainty arising due to incomplete and erroneous data, because they do not build reliable models and solutions guaranteed to address the user's genuine problem as she perceives it. Other fields such as reliable computation offer combinations of models and associated methods to handle these types of uncertain data, but lack an expressive framework characterising the resolution methodology independently of the model.   We present a unifying framework that extends the CP formalism in both model and solutions, to tackle ill-defined combinatorial problems with incomplete or erroneous data. The certainty closure framework brings together modelling and solving methodologies from different fields into the CP paradigm to provide reliable and efficient approches for uncertain constraint problems. We demonstrate the applicability of the framework on a case study in network diagnosis. We define resolution forms that give generic templates, and their associated operational semantics, to derive practical solution methods for reliable solutions.",2006-01-25T20:11:11Z,http://arxiv.org/pdf/cs/0601109v3,2024-04-28,
cs/0602022v1,"Avoiding the Bloat with Stochastic Grammar-based Genetic Programming","The application of Genetic Programming to the discovery of empirical laws is often impaired by the huge size of the search space, and consequently by the computer resources needed. In many cases, the extreme demand for memory and CPU is due to the massive growth of non-coding segments, the introns. The paper presents a new program evolution framework which combines distribution-based evolution in the PBIL spirit, with grammar-based genetic programming; the information is stored as a probability distribution on the gra mmar rules, rather than in a population. Experiments on a real-world like problem show that this approach gives a practical solution to the problem of intron growth.",2006-02-07T07:48:27Z,http://arxiv.org/pdf/cs/0602022v1,2024-04-28,
cs/0602031v1,"Classifying Signals with Local Classifiers","This paper deals with the problem of classifying signals. The new method for building so called local classifiers and local features is presented. The method is a combination of the lifting scheme and the support vector machines. Its main aim is to produce effective and yet comprehensible classifiers that would help in understanding processes hidden behind classified signals. To illustrate the method we present the results obtained on an artificial and a real dataset.",2006-02-08T11:38:44Z,http://arxiv.org/pdf/cs/0602031v1,2024-04-28,
cs/0603025v2,"Open Answer Set Programming with Guarded Programs","Open answer set programming (OASP) is an extension of answer set programming where one may ground a program with an arbitrary superset of the program's constants. We define a fixed point logic (FPL) extension of Clark's completion such that open answer sets correspond to models of FPL formulas and identify a syntactic subclass of programs, called (loosely) guarded programs. Whereas reasoning with general programs in OASP is undecidable, the FPL translation of (loosely) guarded programs falls in the decidable (loosely) guarded fixed point logic (mu(L)GF). Moreover, we reduce normal closed ASP to loosely guarded OASP, enabling for the first time, a characterization of an answer set semantics by muLGF formulas. We further extend the open answer set semantics for programs with generalized literals. Such generalized programs (gPs) have interesting properties, e.g., the ability to express infinity axioms. We restrict the syntax of gPs such that both rules and generalized literals are guarded. Via a translation to guarded fixed point logic, we deduce 2-exptime-completeness of satisfiability checking in such guarded gPs (GgPs). Bound GgPs are restricted GgPs with exptime-complete satisfiability checking, but still sufficiently expressive to optimally simulate computation tree logic (CTL). We translate Datalog lite programs to GgPs, establishing equivalence of GgPs under an open answer set semantics, alternation-free muGF, and Datalog lite.",2006-03-07T17:54:59Z,http://arxiv.org/pdf/cs/0603025v2,2024-04-28,
cs/0603034v1,"Metatheory of actions: beyond consistency","Consistency check has been the only criterion for theory evaluation in logic-based approaches to reasoning about actions. This work goes beyond that and contributes to the metatheory of actions by investigating what other properties a good domain description in reasoning about actions should have. We state some metatheoretical postulates concerning this sore spot. When all postulates are satisfied together we have a modular action theory. Besides being easier to understand and more elaboration tolerant in McCarthy's sense, modular theories have interesting properties. We point out the problems that arise when the postulates about modularity are violated and propose algorithmic checks that can help the designer of an action theory to overcome them.",2006-03-09T10:07:46Z,http://arxiv.org/pdf/cs/0603034v1,2024-04-28,
cs/0603038v2,"Estimation of linear, non-gaussian causal models in the presence of confounding latent variables","The estimation of linear causal models (also known as structural equation models) from data is a well-known problem which has received much attention in the past. Most previous work has, however, made an explicit or implicit assumption of gaussianity, limiting the identifiability of the models. We have recently shown (Shimizu et al, 2005; Hoyer et al, 2006) that for non-gaussian distributions the full causal model can be estimated in the no hidden variables case. In this contribution, we discuss the estimation of the model when confounding latent variables are present. Although in this case uniqueness is no longer guaranteed, there is at most a finite set of models which can fit the data. We develop an algorithm for estimating this set, and describe numerical simulations which confirm the theoretical arguments and demonstrate the practical viability of the approach. Full Matlab code is provided for all simulations.",2006-03-09T14:46:18Z,http://arxiv.org/pdf/cs/0603038v2,2024-04-28,
cs/0603081v1,"Application of Support Vector Regression to Interpolation of Sparse Shock Physics Data Sets","Shock physics experiments are often complicated and expensive. As a result, researchers are unable to conduct as many experiments as they would like - leading to sparse data sets. In this paper, Support Vector Machines for regression are applied to velocimetry data sets for shock damaged and melted tin metal. Some success at interpolating between data sets is achieved. Implications for future work are discussed.",2006-03-20T23:43:45Z,http://arxiv.org/pdf/cs/0603081v1,2024-04-28,
cs/0603120v1,"Approximation Algorithms for K-Modes Clustering","In this paper, we study clustering with respect to the k-modes objective function, a natural formulation of clustering for categorical data. One of the main contributions of this paper is to establish the connection between k-modes and k-median, i.e., the optimum of k-median is at most twice the optimum of k-modes for the same categorical data clustering problem. Based on this observation, we derive a deterministic algorithm that achieves an approximation factor of 2. Furthermore, we prove that the distance measure in k-modes defines a metric. Hence, we are able to extend existing approximation algorithms for metric k-median to k-modes. Empirical results verify the superiority of our method.",2006-03-30T02:02:37Z,http://arxiv.org/pdf/cs/0603120v1,2024-04-28,
cs/0604009v1,"Can an Organism Adapt Itself to Unforeseen Circumstances?","A model of an organism as an autonomous intelligent system has been proposed. This model was used to analyze learning of an organism in various environmental conditions. Processes of learning were divided into two types: strong and weak processes taking place in the absence and the presence of aprioristic information about an object respectively. Weak learning is synonymous to adaptation when aprioristic programs already available in a system (an organism) are started. It was shown that strong learning is impossible for both an organism and any autonomous intelligent system. It was shown also that the knowledge base of an organism cannot be updated. Therefore, all behavior programs of an organism are congenital. A model of a conditioned reflex as a series of consecutive measurements of environmental parameters has been advanced. Repeated measurements are necessary in this case to reduce the error during decision making.",2006-04-05T10:29:28Z,http://arxiv.org/pdf/cs/0604009v1,2024-04-28,
cs/0604042v1,"Adaptative combination rule and proportional conflict redistribution rule for information fusion","This paper presents two new promising rules of combination for the fusion of uncertain and potentially highly conflicting sources of evidences in the framework of the theory of belief functions in order to palliate the well-know limitations of Dempster's rule and to work beyond the limits of applicability of the Dempster-Shafer theory. We present both a new class of adaptive combination rules (ACR) and a new efficient Proportional Conflict Redistribution (PCR) rule allowing to deal with highly conflicting sources for static and dynamic fusion applications.",2006-04-11T14:35:15Z,http://arxiv.org/pdf/cs/0604042v1,2024-04-28,
cs/0604070v2,"Retraction and Generalized Extension of Computing with Words","Fuzzy automata, whose input alphabet is a set of numbers or symbols, are a formal model of computing with values. Motivated by Zadeh's paradigm of computing with words rather than numbers, Ying proposed a kind of fuzzy automata, whose input alphabet consists of all fuzzy subsets of a set of symbols, as a formal model of computing with all words. In this paper, we introduce a somewhat general formal model of computing with (some special) words. The new features of the model are that the input alphabet only comprises some (not necessarily all) fuzzy subsets of a set of symbols and the fuzzy transition function can be specified arbitrarily. By employing the methodology of fuzzy control, we establish a retraction principle from computing with words to computing with values for handling crisp inputs and a generalized extension principle from computing with words to computing with all words for handling fuzzy inputs. These principles show that computing with values and computing with all words can be respectively implemented by computing with words. Some algebraic properties of retractions and generalized extensions are addressed as well.",2006-04-19T06:28:55Z,http://arxiv.org/pdf/cs/0604070v2,2024-04-28,
cs/0604086v1,"A Knowledge-Based Approach for Selecting Information Sources","Through the Internet and the World-Wide Web, a vast number of information sources has become available, which offer information on various subjects by different providers, often in heterogeneous formats. This calls for tools and methods for building an advanced information-processing infrastructure. One issue in this area is the selection of suitable information sources in query answering. In this paper, we present a knowledge-based approach to this problem, in the setting where one among a set of information sources (prototypically, data repositories) should be selected for evaluating a user query. We use extended logic programs (ELPs) to represent rich descriptions of the information sources, an underlying domain theory, and user queries in a formal query language (here, XML-QL, but other languages can be handled as well). Moreover, we use ELPs for declarative query analysis and generation of a query description. Central to our approach are declarative source-selection programs, for which we define syntax and semantics. Due to the structured nature of the considered data items, the semantics of such programs must carefully respect implicit context information in source-selection rules, and furthermore combine it with possible user preferences. A prototype implementation of our approach has been realized exploiting the DLV KR system and its plp front-end for prioritized ELPs. We describe a representative example involving specific movie databases, and report about experimental results.",2006-04-21T16:53:28Z,http://arxiv.org/pdf/cs/0604086v1,2024-04-28,
cs/0605012v2,"Perspective alignment in spatial language","It is well known that perspective alignment plays a major role in the planning and interpretation of spatial language. In order to understand the role of perspective alignment and the cognitive processes involved, we have made precise complete cognitive models of situated embodied agents that self-organise a communication system for dialoging about the position and movement of real world objects in their immediate surroundings. We show in a series of robotic experiments which cognitive mechanisms are necessary and sufficient to achieve successful spatial language and why and how perspective alignment can take place, either implicitly or based on explicit marking.",2006-05-04T17:16:02Z,http://arxiv.org/pdf/cs/0605012v2,2024-04-28,
cs/0605017v1,"Reasoning and Planning with Sensing Actions, Incomplete Information, and Static Causal Laws using Answer Set Programming","We extend the 0-approximation of sensing actions and incomplete information in [Son and Baral 2000] to action theories with static causal laws and prove its soundness with respect to the possible world semantics. We also show that the conditional planning problem with respect to this approximation is NP-complete. We then present an answer set programming based conditional planner, called ASCP, that is capable of generating both conformant plans and conditional plans in the presence of sensing actions, incomplete information about the initial state, and static causal laws. We prove the correctness of our implementation and argue that our planner is sound and complete with respect to the proposed approximation. Finally, we present experimental results comparing ASCP to other planners.",2006-05-04T22:35:12Z,http://arxiv.org/pdf/cs/0605017v1,2024-04-28,
cs/0605055v1,"Approximate Discrete Probability Distribution Representation using a Multi-Resolution Binary Tree","Computing and storing probabilities is a hard problem as soon as one has to deal with complex distributions over multiple random variables. The problem of efficient representation of probability distributions is central in term of computational efficiency in the field of probabilistic reasoning. The main problem arises when dealing with joint probability distributions over a set of random variables: they are always represented using huge probability arrays. In this paper, a new method based on binary-tree representation is introduced in order to store efficiently very large joint distributions. Our approach approximates any multidimensional joint distributions using an adaptive discretization of the space. We make the assumption that the lower is the probability mass of a particular region of feature space, the larger is the discretization step. This assumption leads to a very optimized representation in term of time and memory. The other advantages of our approach are the ability to refine dynamically the distribution every time it is needed leading to a more accurate representation of the probability distribution and to an anytime representation of the distribution.",2006-05-12T13:32:50Z,http://arxiv.org/pdf/cs/0605055v1,2024-04-28,
cs/0605108v2,"Diagnosability of Fuzzy Discrete Event Systems","In order to more effectively cope with the real-world problems of vagueness, {\it fuzzy discrete event systems} (FDESs) were proposed recently, and the supervisory control theory of FDESs was developed. In view of the importance of failure diagnosis, in this paper, we present an approach of the failure diagnosis in the framework of FDESs. More specifically: (1) We formalize the definition of diagnosability for FDESs, in which the observable set and failure set of events are {\it fuzzy}, that is, each event has certain degree to be observable and unobservable, and, also, each event may possess different possibility of failure occurring. (2) Through the construction of observability-based diagnosers of FDESs, we investigate its some basic properties. In particular, we present a necessary and sufficient condition for diagnosability of FDESs. (3) Some examples serving to illuminate the applications of the diagnosability of FDESs are described. To conclude, some related issues are raised for further consideration.",2006-05-24T15:49:06Z,http://arxiv.org/pdf/cs/0605108v2,2024-04-28,
cs/0605123v1,"Classification of Ordinal Data","Classification of ordinal data is one of the most important tasks of relation learning. In this thesis a novel framework for ordered classes is proposed. The technique reduces the problem of classifying ordered classes to the standard two-class problem. The introduced method is then mapped into support vector machines and neural networks. Compared with a well-known approach using pairwise objects as training samples, the new algorithm has a reduced complexity and training time. A second novel model, the unimodal model, is also introduced and a parametric version is mapped into neural networks. Several case studies are presented to assert the validity of the proposed models.",2006-05-26T09:44:44Z,http://arxiv.org/pdf/cs/0605123v1,2024-04-28,
cs/9810011v1,"Flysig: Dataflow Oriented Delay-Insensitive Processor for Rapid Prototyping of Signal Processing","As the one-chip integration of HW-modules designed by different companies becomes more and more popular reliability of a HW-design and evaluation of the timing behavior during the prototype stage are absolutely necessary. One way to guarantee reliability is the use of robust design styles, e.g., delay-insensitivity. For early timing evaluation two aspects must be considered: a) The timing needs to be proportional to technology variations and b) the implemented architecture should be identical for prototype and target. The first can be met also by delay-insensitive implementation. The latter one is the key point. A unified architecture is needed for prototyping as well as implementation. Our new approach to rapid prototyping of signal processing tasks is based on a configurable, delay-insensitive implemented processor called Flysig. In essence, the Flysig processor can be understood as a complex FPGA where the CLBs are substituted by bit-serial operators. In this paper the general concept is detailed and first experimental results are given for demonstration of the main advantages: delay-insensitive design style, direct correspondence between prototyping and target architecture, high performance and reasonable shortening of the design cycle.",1998-10-12T10:11:05Z,http://arxiv.org/pdf/cs/9810011v1,2024-04-28,
cs/0111029v1,"Versatile Data Acquisition and Controls for Epics Using Vme-Based Fpgas","Field-Programmable Gate Arrays (FPGAs) have provided Thomas Jefferson National Accelerator Facility (Jefferson Lab) with versatile VME-based data acquisition and control interfaces with minimal development times. FPGA designs have been used to interface to VME and provide control logic for numerous systems. The building blocks of these logic designs can be tailored to the individual needs of each system and provide system operators with read-backs and controls via a VME interface to an EPICS based computer. This versatility allows the system developer to choose components and define operating parameters and options that are not readily available commercially. Jefferson Lab has begun developing standard FPGA libraries that result in quick turn around times and inexpensive designs.",2001-11-09T15:08:12Z,http://arxiv.org/pdf/cs/0111029v1,2024-04-28,
cs/0111030v1,"A Dual Digital Signal Processor VME Board For Instrumentation And Control Applications","A Dual Digital Signal Processing VME Board was developed for the Continuous Electron Beam Accelerator Facility (CEBAF) Beam Current Monitor (BCM) system at Jefferson Lab. It is a versatile general-purpose digital signal processing board using an open architecture, which allows for adaptation to various applications. The base design uses two independent Texas Instrument (TI) TMS320C6711, which are 900 MFLOPS floating-point digital signal processors (DSP). Applications that require a fixed point DSP can be implemented by replacing the baseline DSP with the pin-for-pin compatible TMS320C6211. The design can be manufactured with a reduced chip set without redesigning the printed circuit board. For example it can be implemented as a single-channel DSP with no analog I/O.",2001-11-09T16:04:17Z,http://arxiv.org/pdf/cs/0111030v1,2024-04-28,
cs/0111032v1,"SNS Timing System","This poster describes the timing system being designed for Spallation Neutron Source being built at Oak Ridge National lab.",2001-11-09T19:08:57Z,http://arxiv.org/pdf/cs/0111032v1,2024-04-28,
cs/0207012v1,"Synthesis of Low-Power Digital Circuits Derived from Binary Decision Diagrams","This paper introduces a novel method for synthesizing digital circuits derived from Binary Decision Diagrams (BDDs) that can yield to reduction in power dissipation. The power reduction is achieved by decreasing the switching activity in a circuit while paying close attention to information measures as an optimization criterion. We first present the technique of efficient BDD-based computation of information measures which are used to guide the power optimization procedures. Using this technique, we have developed an algorithm of BDD reordering which leads to reducing the power consumption of the circuits derived from BDDs. Results produced by the synthesis on the ISCAS benchmark circuits are very encouraging.",2002-07-04T04:30:30Z,http://arxiv.org/pdf/cs/0207012v1,2024-04-28,
cs/0207014v1,"On the Information Engine of Circuit Design","This paper addresses a new approach to find a spectrum of information measures for the process of digital circuit synthesis. We consider the problem from the information engine point of view. The circuit synthesis as a whole and different steps of the design process (an example of decision diagram is given) are presented via such measurements as entropy, logical work and information vitality. We also introduce new information measures to provide better estimates of synthesis criteria. We show that the basic properties of information engine, such as the conservation law of information flow and the equilibrium law of information can be formulated.",2002-07-04T04:55:42Z,http://arxiv.org/pdf/cs/0207014v1,2024-04-28,
cs/0405015v1,"A High-Level Reconfigurable Computing Platform Software Frameworks","Reconfigurable computing refers to the use of processors, such as Field Programmable Gate Arrays (FPGAs), that can be modified at the hardware level to take on different processing tasks. A reconfigurable computing platform describes the hardware and software base on top of which modular extensions can be created, depending on the desired application. Such reconfigurable computing platforms can take on varied designs and implementations, according to the constraints imposed and features desired by the scope of applications. This paper introduces a PC-based reconfigurable computing platform software frameworks that is flexible and extensible enough to abstract the different hardware types and functionality that different PCs may have. The requirements of the software platform, architectural issues addressed, rationale behind the decisions made, and frameworks design implemented are discussed.",2004-05-05T01:56:22Z,http://arxiv.org/pdf/cs/0405015v1,2024-04-28,
cs/0407019v2,"Stochastic fuzzy controller","A standard approach to building a fuzzy controller based on stochastic logic uses binary random signals with an average (expected value of a random variable) in the range [0, 1]. A different approach is presented, founded on a representation of the membership functions with the probability density functions.",2004-07-08T06:52:37Z,http://arxiv.org/pdf/cs/0407019v2,2024-04-28,
cs/0407032v1,"Exposing Software Defined Radio Functionality To Native Operating System Applications via Virtual Devices","Many reconfigurable platforms require that applications be written specifically to take advantage of the reconfigurable hardware. In a PC-based environment, this presents an undesirable constraint in that the many already available applications cannot leverage on such hardware. Greatest benefit can only be derived from reconfigurable devices if even native OS applications can transparently utilize reconfigurable devices as they would normal full-fledged hardware devices. This paper presents how Proteus Virtual Devices are used to expose reconfigurable hardware in a transparent manner for use by typical native OS applications.",2004-07-13T08:24:37Z,http://arxiv.org/pdf/cs/0407032v1,2024-04-28,
cs/0409025v1,"Topics in asynchronous systems","In the paper we define and characterize the asynchronous systems from the point of view of their autonomy, determinism, order, non-anticipation, time invariance, symmetry, stability and other important properties. The study is inspired by the models of the asynchronous circuits.",2004-09-13T11:08:09Z,http://arxiv.org/pdf/cs/0409025v1,2024-04-28,
cs/0412040v1,"Data-stationary Architecture to Execute Quantum Algorithms Classically","This paper presents a data stationary architecture in which each word has an attached address field. Address fields massively update in parallel to record data interchanges. Words do not move until memory is read for post processing. A sea of such cells can test large-scale quantum algorithms, although other programming is possible.",2004-12-09T22:10:48Z,http://arxiv.org/pdf/cs/0412040v1,2024-04-28,
cs/0503066v1,"A Practical Approach for Circuit Routing on Dynamic Reconfigurable Devices","Management of communication by on-line routing in new FPGAs with a large amount of logic resources and partial reconfigurability is a new challenging problem. A Network-on-Chip   (NoC) typically uses packet routing mechanism, which has often unsafe data transfers, and network interface overhead. In this paper, circuit routing for such dynamic NoCs is investigated, and a practical 1-dimensional network with an efficient routing algorithm is proposed and implemented. Also, this concept has been extended to the 2-dimensional case. The implementation results show the low area overhead and high performance of this network.",2005-03-24T12:36:07Z,http://arxiv.org/pdf/cs/0503066v1,2024-04-28,
cs/0508038v1,"Quantum Algorithm Processor For Finding Exact Divisors","Wiring diagrams are given for a quantum algorithm processor in CMOS to compute, in parallel, all divisors of an n-bit integer. Lines required in a wiring diagram are proportional to n. Execution time is proportional to the square of n.",2005-08-04T17:35:38Z,http://arxiv.org/pdf/cs/0508038v1,2024-04-28,
cs/0510039v1,"DyNoC: A Dynamic Infrastructure for Communication in Dynamically Reconfigurable Devices","A new paradigm to support the communication among modules dynamically placed on a reconfigurable device at run-time is presented. Based on the network on chip (NoC) infrastructure, we developed a dynamic communication infrastructure as well as routing methodologies capable to handle routing in a NoC with obstacles created by dynamically placed components. We prove the unrestricted reachability of components and pins, the deadlock-freeness and we finally show the feasibility of our approach by means on real life example applications.",2005-10-14T22:03:45Z,http://arxiv.org/pdf/cs/0510039v1,2024-04-28,
cs/0602096v1,"Difficulties in the Implementation of Quantum Computers","This paper reviews various engineering hurdles facing the field of quantum computing. Specifically, problems related to decoherence, state preparation, error correction, and implementability of gates are considered.",2006-02-27T18:59:39Z,http://arxiv.org/pdf/cs/0602096v1,2024-04-28,
cs/0603088v1,"Novel BCD Adders and Their Reversible Logic Implementation for IEEE 754r Format","IEEE 754r is the ongoing revision to the IEEE 754 floating point standard and a major enhancement to the standard is the addition of decimal format. This paper proposes two novel BCD adders called carry skip and carry look-ahead BCD adders respectively. Furthermore, in the recent years, reversible logic has emerged as a promising technology having its applications in low power CMOS, quantum computing, nanotechnology, and optical computing. It is not possible to realize quantum computing without reversible logic. Thus, this paper also paper provides the reversible logic implementation of the conventional BCD adder as the well as the proposed Carry Skip BCD adder using a recently proposed TSG gate. Furthermore, a new reversible gate called TS-3 is also being proposed and it has been shown that the proposed reversible logic implementation of the BCD Adders is much better compared to recently proposed one, in terms of number of reversible gates used and garbage outputs produced. The reversible BCD circuits designed and proposed here form the basis of the decimal ALU of a primitive quantum CPU.",2006-03-22T18:11:40Z,http://arxiv.org/pdf/cs/0603088v1,2024-04-28,
cs/0603091v1,"A New Reversible TSG Gate and Its Application For Designing Efficient Adder Circuits","In the recent years, reversible logic has emerged as a promising technology having its applications in low power CMOS, quantum computing, nanotechnology, and optical computing. The classical set of gates such as AND, OR, and EXOR are not reversible. This paper proposes a new 4 * 4 reversible gate called TSG gate. The proposed gate is used to design efficient adder units. The most significant aspect of the proposed gate is that it can work singly as a reversible full adder i.e reversible full adder can now be implemented with a single gate only. The proposed gate is then used to design reversible ripple carry and carry skip adders. It is demonstrated that the adder architectures designed using the proposed gate are much better and optimized, compared to their existing counterparts in literature; in terms of number of reversible gates and garbage outputs. Thus, this paper provides the initial threshold to building of more complex system which can execute more complicated operations using reversible logic.",2006-03-23T06:44:34Z,http://arxiv.org/pdf/cs/0603091v1,2024-04-28,
cs/0603092v1,"An Extension to DNA Based Fredkin Gate Circuits: Design of Reversible Sequential Circuits using Fredkin Gates","In recent years, reversible logic has emerged as a promising computing paradigm having its applications in low power computing, quantum computing, nanotechnology, optical computing and DNA computing. The classical set of gates such as AND, OR, and EXOR are not reversible. Recently, it has been shown how to encode information in DNA and use DNA amplification to implement Fredkin gates. Furthermore, in the past Fredkin gates have been constructed using DNA, whose outputs are used as inputs for other Fredkin gates. Thus, it can be concluded that arbitrary circuits of Fredkin gates can be constructed using DNA. This paper provides the initial threshold to building of more complex system having reversible sequential circuits and which can execute more complicated operations. The novelty of the paper is the reversible designs of sequential circuits using Fredkin gate. Since, Fredkin gate has already been realized using DNA, it is expected that this work will initiate the building of complex systems using DNA. The reversible circuits designed here are highly optimized in terms of number of gates and garbage outputs. The modularization approach that is synthesizing small circuits and thereafter using them to construct bigger circuits is used for designing the optimal reversible sequential circuits.",2006-03-23T08:33:08Z,http://arxiv.org/pdf/cs/0603092v1,2024-04-28,
cs/0605004v1,"Novel Reversible Multiplier Architecture Using Reversible TSG Gate","In the recent years, reversible logic has emerged as a promising technology having its applications in low power CMOS, quantum computing, nanotechnology, and optical computing. The classical set of gates such as AND, OR, and EXOR are not reversible. Recently a 4 * 4 reversible gate called TSG is proposed. The most significant aspect of the proposed gate is that it can work singly as a reversible full adder, that is reversible full adder can now be implemented with a single gate only. This paper proposes a NXN reversible multiplier using TSG gate. It is based on two concepts. The partial products can be generated in parallel with a delay of d using Fredkin gates and thereafter the addition can be reduced to log2N steps by using reversible parallel adder designed from TSG gates. Similar multiplier architecture in conventional arithmetic (using conventional logic) has been reported in existing literature, but the proposed one in this paper is totally based on reversible logic and reversible cells as its building block. A 4x4 architecture of the proposed reversible multiplier is also designed. It is demonstrated that the proposed multiplier architecture using the TSG gate is much better and optimized, compared to its existing counterparts in literature; in terms of number of reversible gates and garbage outputs. Thus, this paper provides the initial threshold to building of more complex system which can execute more complicated operations using reversible logic.",2006-05-01T09:34:32Z,http://arxiv.org/pdf/cs/0605004v1,2024-04-28,
cs/0605125v1,"Combinational Logic Circuit Design with the Buchberger Algorithm","We detail a procedure for the computation of the polynomial form of an electronic combinational circuit from the design equations in a truth table. The method uses the Buchberger algorithm rather than current traditional methods based on search algorithms. We restrict the analysis to a single output, but the procedure can be generalized to multiple outputs. The procedure is illustrated with the design of a simple arithmetic and logic unit with two 3-bit operands and two control bits.",2006-05-26T18:43:13Z,http://arxiv.org/pdf/cs/0605125v1,2024-04-28,
cs/0605142v1,"Intégration de la synthèse mémoire dans l'outil de synthèse d'architecture GAUT Low Power","The systems supporting signal and image applications process large amount of data. That involves an intensive use of the memory which becomes the bottleneck of systems. Memory limits performances and represents a significant proportion of total consumption. In the development high level synthesis tool called GAUT Low Power, we are interested in the synthesis of the memory unit. In this work, we integrate the data storage and data transfert to constraint the high level synthesis of the datapath's execution unit.",2006-05-30T15:04:18Z,http://arxiv.org/pdf/cs/0605142v1,2024-04-28,
cs/0605143v1,"High-level synthesis under I/O Timing and Memory constraints","The design of complex Systems-on-Chips implies to take into account communication and memory access constraints for the integration of dedicated hardware accelerator. In this paper, we present a methodology and a tool that allow the High-Level Synthesis of DSP algorithm, under both I/O timing and memory constraints. Based on formal models and a generic architecture, this tool helps the designer to find a reasonable trade-off between both the required I/O timing behavior and the internal memory access parallelism of the circuit. The interest of our approach is demonstrated on the case study of a FFT algorithm.",2006-05-30T15:07:51Z,http://arxiv.org/pdf/cs/0605143v1,2024-04-28,
cs/0605144v1,"A Memory Aware High Level Synthesis Too","We introduce a new approach to take into account the memory architecture and the memory mapping in High- Level Synthesis for data intensive applications. We formalize the memory mapping as a set of constraints for the synthesis, and defined a Memory Constraint Graph and an accessibility criterion to be used in the scheduling step. We use a memory mapping file to include those memory constraints in our HLS tool GAUT. It is possible, with the help of GAUT, to explore a wide range of solutions, and to reach a good tradeoff between time, power-consumption, and area.",2006-05-30T15:09:18Z,http://arxiv.org/pdf/cs/0605144v1,2024-04-28,
cs/0605145v1,"Memory Aware High-Level Synthesis for Embedded Systems","We introduce a new approach to take into account the memory architecture and the memory mapping in the High- Level Synthesis of Real-Time embedded systems. We formalize the memory mapping as a set of constraints used in the scheduling step. We use a memory mapping file to include those memory constraints in our HLS tool GAUT. Our scheduling algorithm exhibits a relatively low complexity that permits to tackle complex designs in a reasonable time. Finally, we show how to explore, with the help of GAUT, a wide range of solutions, and to reach a good tradeoff between time, power-consumption, and area.",2006-05-30T15:09:32Z,http://arxiv.org/pdf/cs/0605145v1,2024-04-28,
cs/0605146v1,"Synthèse Comportementale Sous Contraintes de Communication et de Placement Mémoire pour les composants du TDSI","The design of complex Digital Signal Processing systems implies to minimize architectural cost and to maximize timing performances while taking into account communication and memory accesses constraints for the integration of dedicated hardware accelerator. Unfortunately, the traditional Matlab/ Simulink design flows gather not very flexible hardware blocs. In this paper, we present a methodology and a tool that permit the High-Level Synthesis of DSP applications, under both I/O timing and memory constraints. Based on formal models and a generic architecture, our tool GAUT helps the designer in finding a reasonable trade-off between the circuit's performance and its architectural complexity. The efficiency of our approach is demonstrated on the case study of a FFT algorithm.",2006-05-30T15:10:57Z,http://arxiv.org/pdf/cs/0605146v1,2024-04-28,
cs/0608075v1,"Design of multimedia processor based on metric computation","Media-processing applications, such as signal processing, 2D and 3D graphics rendering, and image compression, are the dominant workloads in many embedded systems today. The real-time constraints of those media applications have taxing demands on today's processor performances with low cost, low power and reduced design delay. To satisfy those challenges, a fast and efficient strategy consists in upgrading a low cost general purpose processor core. This approach is based on the personalization of a general RISC processor core according the target multimedia application requirements. Thus, if the extra cost is justified, the general purpose processor GPP core can be enforced with instruction level coprocessors, coarse grain dedicated hardware, ad hoc memories or new GPP cores. In this way the final design solution is tailored to the application requirements. The proposed approach is based on three main steps: the first one is the analysis of the targeted application using efficient metrics. The second step is the selection of the appropriate architecture template according to the first step results and recommendations. The third step is the architecture generation. This approach is experimented using various image and video algorithms showing its feasibility.",2006-08-18T13:12:55Z,http://arxiv.org/pdf/cs/0608075v1,2024-04-28,
cs/0609023v1,"Novel Reversible TSG Gate and Its Application for Designing Components of Primitive Reversible/Quantum ALU","In recent years, reversible logic has emerged as a promising computing paradigm having application in low power CMOS, quantum computing, nanotechnology, and optical computing. The classical set of gates such as AND, OR, and EXOR are not reversible. This paper utilizes a new 4 * 4 reversible gate called TSG gate to build the components of a primitive reversible/quantum ALU. The most significant aspect of the TSG gate is that it can work singly as a reversible full adder, that is reversible full adder can now be implemented with a single gate only. A Novel reversible 4:2 compressor is also designed from the TSG gate which is later used to design a novel 8x8 reversible Wallace tree multiplier. It is proved that the adder, 4:2 compressor and multiplier architectures designed using the TSG gate are better than their counterparts available in literature, in terms of number of reversible gates and garbage outputs. This is perhaps, the first attempt to design a reversible 4:2 compressor and a reversible Wallace tree multiplier as far as existing literature and our knowledge is concerned. Thus, this paper provides an initial threshold to build more complex systems which can execute complicated operations using reversible logic.",2006-09-06T16:09:04Z,http://arxiv.org/pdf/cs/0609023v1,2024-04-28,
cs/0609028v1,"VLSI Implementation of RSA Encryption System Using Ancient Indian Vedic Mathematics","This paper proposes the hardware implementation of RSA encryption/decryption algorithm using the algorithms of Ancient Indian Vedic Mathematics that have been modified to improve performance. The recently proposed hierarchical overlay multiplier architecture is used in the RSA circuitry for multiplication operation. The most significant aspect of the paper is the development of a division architecture based on Straight Division algorithm of Ancient Indian Vedic Mathematics and embedding it in RSA encryption/decryption circuitry for improved efficiency. The coding is done in Verilog HDL and the FPGA synthesis is done using Xilinx Spartan library. The results show that RSA circuitry implemented using Vedic division and multiplication is efficient in terms of area/speed compared to its implementation using conventional multiplication and division architectures",2006-09-07T14:18:41Z,http://arxiv.org/pdf/cs/0609028v1,2024-04-28,
cs/0609029v1,"Reversible Programmable Logic Array (RPLA) using Fredkin & Feynman Gates for Industrial Electronics and Applications","In recent years, reversible logic has emerged as a promising computing paradigm having application in low power CMOS, quantum computing, nanotechnology, and optical computing. The classical set of gates such as AND, OR, and EXOR are not reversible. In this paper, the authors have proposed reversible programmable logic array (RPLA) architecture using reversible Fredkin and Feynman gates. The proposed RPLA has n inputs and m outputs and can realize m functions of n variables. In order to demonstrate the design of RPLA, a 3 input RPLA is designed which can perform any 28 functions using the combination of 8 min terms (23). Furthermore, the application of the designed 3 input RPLA is shown by implementing the full adder and full subtractor functions through it.",2006-09-07T14:25:21Z,http://arxiv.org/pdf/cs/0609029v1,2024-04-28,
cs/0609036v1,"Reduced Area Low Power High Throughput BCD Adders for IEEE 754r Format","IEEE 754r is the ongoing revision to the IEEE 754 floating point standard and a major enhancement to the standard is the addition of decimal format. Firstly, this paper proposes novel two transistor AND and OR gates. The proposed AND gate has no power supply, thus it can be referred as the Powerless AND gate. Similarly, the proposed two transistor OR gate has no ground and can be referred as Groundless OR. Secondly for IEEE 754r format, two novel BCD adders called carry skip and carry look-ahead BCD adders are also proposed in this paper. In order to design the carry look-ahead BCD adder, a novel 4 bit carry look-ahead adder called NCLA is proposed which forms the basic building block of the proposed carry look-ahead BCD adder. Finally, the proposed two transistors AND and OR gates are used to provide the optimized small area low power high throughput circuitries of the proposed BCD adders.",2006-09-08T05:36:20Z,http://arxiv.org/pdf/cs/0609036v1,2024-04-28,
cs/0610090v1,"Combined Integer and Floating Point Multiplication Architecture(CIFM) for FPGAs and Its Reversible Logic Implementation","In this paper, the authors propose the idea of a combined integer and floating point multiplier(CIFM) for FPGAs. The authors propose the replacement of existing 18x18 dedicated multipliers in FPGAs with dedicated 24x24 multipliers designed with small 4x4 bit multipliers. It is also proposed that for every dedicated 24x24 bit multiplier block designed with 4x4 bit multipliers, four redundant 4x4 multiplier should be provided to enforce the feature of self repairability (to recover from the faults). In the proposed CIFM reconfigurability at run time is also provided resulting in low power. The major source of motivation for providing the dedicated 24x24 bit multiplier stems from the fact that single precision floating point multiplier requires 24x24 bit integer multiplier for mantissa multiplication. A reconfigurable, self-repairable 24x24 bit multiplier (implemented with 4x4 bit multiply modules) will ideally suit this purpose, making FPGAs more suitable for integer as well floating point operations. A dedicated 4x4 bit multiplier is also proposed in this paper. Moreover, in the recent years, reversible logic has emerged as a promising technology having its applications in low power CMOS, quantum computing, nanotechnology, and optical computing. It is not possible to realize quantum computing without reversible logic. Thus, this paper also paper provides the reversible logic implementation of the proposed CIFM. The reversible CIFM designed and proposed here will form the basis of the completely reversible FPGAs.",2006-10-14T10:39:42Z,http://arxiv.org/pdf/cs/0610090v1,2024-04-28,
0706.1692v1,"A Methodology for Efficient Space-Time Adapter Design Space Exploration: A Case Study of an Ultra Wide Band Interleaver","This paper presents a solution to efficiently explore the design space of communication adapters. In most digital signal processing (DSP) applications, the overall architecture of the system is significantly affected by communication architecture, so the designers need specifically optimized adapters. By explicitly modeling these communications within an effective graph-theoretic model and analysis framework, we automatically generate an optimized architecture, named Space-Time AdapteR (STAR). Our design flow inputs a C description of Input/Output data scheduling, and user requirements (throughput, latency, parallelism...), and formalizes communication constraints through a Resource Constraints Graph (RCG). The RCG properties enable an efficient architecture space exploration in order to synthesize a STAR component. The proposed approach has been tested to design an industrial data mixing block example: an Ultra-Wideband interleaver.",2007-06-12T13:45:50Z,http://arxiv.org/pdf/0706.1692v1,2024-04-28,
0706.2732v1,"A Design Methodology for Space-Time Adapter","This paper presents a solution to efficiently explore the design space of communication adapters. In most digital signal processing (DSP) applications, the overall architecture of the system is significantly affected by communication architecture, so the designers need specifically optimized adapters. By explicitly modeling these communications within an effective graph-theoretic model and analysis framework, we automatically generate an optimized architecture, named Space-Time AdapteR (STAR). Our design flow inputs a C description of Input/Output data scheduling, and user requirements (throughput, latency, parallelism...), and formalizes communication constraints through a Resource Constraints Graph (RCG). The RCG properties enable an efficient architecture space exploration in order to synthesize a STAR component. The proposed approach has been tested to design an industrial data mixing block example: an Ultra-Wideband interleaver.",2007-06-19T14:18:57Z,http://arxiv.org/pdf/0706.2732v1,2024-04-28,
0706.2824v1,"Méthodologie de modélisation et d'implémentation d'adaptateurs spatio-temporels","The re-use of pre-designed blocks is a well-known concept of the software development. This technique has been applied to System-on-Chip (SoC) design whose complexity and heterogeneity are growing. The re-use is made thanks to high level components, called virtual components (IP), available in more or less flexible forms. These components are dedicated blocks: digital signal processing (DCT, FFT), telecommunications (Viterbi, TurboCodes),... These blocks rest on a model of fixed architecture with very few degrees of personalization. This rigidity is particularly true for the communication interface whose orders of acquisition and production of data, the temporal behavior and protocols of exchanges are fixed. The successful integration of such an IP requires that the designer (1) synchronizes the components (2) converts the protocols between ""incompatible"" blocks (3) temporizes the data to guarantee the temporal constraints and the order of the data. This phase remains however very manual and source of errors. Our approach proposes a formal modeling, based on an original Ressource Compatibility Graph. The synthesis flow is based on a set of transformations of the initial graph to lead to an interface architecture allowing the space-time adaptation of the data exchanges between several components.",2007-06-19T15:56:43Z,http://arxiv.org/pdf/0706.2824v1,2024-04-28,
0707.1151v6,"Logic, Design & Organization of PTVD-SHAM; A Parallel Time Varying & Data Super-helical Access Memory","This paper encompasses a super helical memory system's design, 'Boolean logic & image-logic' as a theoretical concept of an invention-model to 'store time-data' in terms of anticipating the best memory location ever for data/time. A waterfall effect is deemed to assist the process of potential-difference output-switch into diverse logic states in quantum dot computational methods via utilizing coiled carbon nanotubes (CCNTs) and carbon nanotube field effect transistors (CNFETs). A 'quantum confinement' is thus derived for a flow of particles in a categorized quantum well substrate with a normalized capacitance rectifying high B-field flux into electromagnetic induction. Multi-access of coherent sequences of 'qubit addressing' is gained in any magnitude as pre-defined for the orientation of array displacement. Briefly, Gaussian curvature of k<0 is debated in aim of specifying the 2D electron gas characteristics in scenarios where data is stored in short intervals versus long ones e.g. when k'>(k<0) for greater CCNT diameters, space-time continuum is folded by chance for the particle. This benefits from Maxwell-Lorentz theory in Minkowski's space-time viewpoint alike to crystal oscillators for precise data timing purposes and radar systems e.g., time varying self-clocking devices in diverse geographic locations. This application could also be optional for data depository versus extraction, in the best supercomputer system's locations, autonomously. For best performance in minimizing current limiting mechanisms including electromigration, a multilevel metallization and implant process forming elevated sources/drains for the circuit's staircase pyramidal construction, is discussed accordingly.",2007-07-09T19:26:06Z,http://arxiv.org/pdf/0707.1151v6,2024-04-28,
0710.3443v1,"DPA on quasi delay insensitive asynchronous circuits: formalization and improvement","The purpose of this paper is to formally specify a flow devoted to the design of Differential Power Analysis (DPA) resistant QDI asynchronous circuits. The paper first proposes a formal modeling of the electrical signature of QDI asynchronous circuits. The DPA is then applied to the formal model in order to identify the source of leakage of this type of circuits. Finally, a complete design flow is specified to minimize the information leakage. The relevancy and efficiency of the approach is demonstrated using the design of an AES crypto-processor.",2007-10-18T06:57:52Z,http://arxiv.org/pdf/0710.3443v1,2024-04-28,
0710.3535v2,"JANUS: an FPGA-based System for High Performance Scientific Computing","This paper describes JANUS, a modular massively parallel and reconfigurable FPGA-based computing system. Each JANUS module has a computational core and a host. The computational core is a 4x4 array of FPGA-based processing elements with nearest-neighbor data links. Processors are also directly connected to an I/O node attached to the JANUS host, a conventional PC. JANUS is tailored for, but not limited to, the requirements of a class of hard scientific applications characterized by regular code structure, unconventional data manipulation instructions and not too large data-base size. We discuss the architecture of this configurable machine, and focus on its use on Monte Carlo simulations of statistical mechanics. On this class of application JANUS achieves impressive performances: in some cases one JANUS processing element outperfoms high-end PCs by a factor ~ 1000. We also discuss the role of JANUS on other classes of scientific applications.",2007-10-18T15:26:32Z,http://arxiv.org/pdf/0710.3535v2,2024-04-28,
0710.3789v1,"Frequency Analysis of Decoupling Capacitors for Three Voltage Supplies in SoC","Reduction in power consumption has become a major criterion of design in modern ICs. One such scheme to reduce power consumption by an IC is the use of multiple power supplies for critical and non-critical paths. To maintain the impedance of a power distribution system below a specified level, multiple decoupling capacitors are placed at different levels of power grid hierarchy. This paper describes about three-voltage supply power distribution systems. The noise at one power supply can propagate to the other power supply, causing power and signal integrity problems in the overall system. Effects such as anti-resonance and remedies for these effects are studied. Impedance of the three-voltage supply power distribution system is calculated in terms of RLC-model of decoupling capacitors. Further the obtained impedance depends on the frequency; hence brief frequency analysis of impedance is done.",2007-10-19T20:59:53Z,http://arxiv.org/pdf/0710.3789v1,2024-04-28,
0710.4630v1,"CAFFEINE: Template-Free Symbolic Model Generation of Analog Circuits via Canonical Form Functions and Genetic Programming","This paper presents a method to automatically generate compact symbolic performance models of analog circuits with no prior specification of an equation template. The approach takes SPICE simulation data as input, which enables modeling of any nonlinear circuits and circuit characteristics. Genetic programming is applied as a means of traversing the space of possible symbolic expressions. A grammar is specially designed to constrain the search to a canonical form for functions. Novel evolutionary search operators are designed to exploit the structure of the grammar. The approach generates a set of symbolic models which collectively provide a tradeoff between error and model complexity. Experimental results show that the symbolic models generated are compact and easy to understand, making this an effective method for aiding understanding in analog design. The models also demonstrate better prediction quality than posynomials.",2007-10-25T08:07:11Z,http://arxiv.org/pdf/0710.4630v1,2024-04-28,
0710.4632v1,"Hardware Support for Arbitrarily Complex Loop Structures in Embedded Applications","In this paper, the program control unit of an embedded RISC processor is enhanced with a novel zero-overhead loop controller (ZOLC) supporting arbitrary loop structures with multiple-entry/exit nodes. The ZOLC has been incorporated to an open RISC processor core to evaluate the performance of the proposed unit for alternative configurations of the selected processor. It is proven that speed improvements of 8.4% to 48.2% are feasible for the used benchmarks.",2007-10-25T08:07:52Z,http://arxiv.org/pdf/0710.4632v1,2024-04-28,
0710.4634v1,"A Probabilistic Collocation Method Based Statistical Gate Delay Model Considering Process Variations and Multiple Input Switching","Since the advent of new nanotechnologies, the variability of gate delay due to process variations has become a major concern. This paper proposes a new gate delay model that includes impact from both process variations and multiple input switching. The proposed model uses orthogonal polynomial based probabilistic collocation method to construct a delay analytical equation from circuit timing performance. From the experimental results, our approach has less that 0.2% error on the mean delay of gates and less than 3% error on the standard deviation.",2007-10-25T08:08:50Z,http://arxiv.org/pdf/0710.4634v1,2024-04-28,
0710.4636v1,"Why Systems-on-Chip Needs More UML like a Hole in the Head","Let's be clear from the outset: SoC can most certainly make use of UML; SoC just doesn't need more UML, or even all of it. The advent of model mappings, coupled with marks that indicate which mapping rule to apply, enable a major simplification of the use of UML in SoC.",2007-10-25T08:09:23Z,http://arxiv.org/pdf/0710.4636v1,2024-04-28,
0710.4638v1,"Buffer Insertion for Bridges and Optimal Buffer Sizing for Communication Sub-System of Systems-on-Chip","We have presented an optimal buffer sizing and buffer insertion methodology which uses stochastic models of the architecture and Continuous Time Markov Decision Processes CTMDPs. Such a methodology is useful in managing the scarce buffer resources available on chip as compared to network based data communication which can have large buffer space. The modeling of this problem in terms of a CT-MDP framework lead to a nonlinear formulation due to usage of bridges in the bus architecture. We present a methodology to split the problem into several smaller though linear systems and we then solve these subsystems.",2007-10-25T08:10:40Z,http://arxiv.org/pdf/0710.4638v1,2024-04-28,
0710.4639v1,"Modeling the Non-Linear Behavior of Library Cells for an Accurate Static Noise Analysis","In signal integrity analysis, the joint effect of propagated noise through library cells, and of the noise injected on a quiet net by neighboring switching nets through coupling capacitances, must be considered in order to accurately estimate the overall noise impact on design functionality and performances. In this work the impact of the cell non-linearity on the noise glitch waveform is analyzed in detail, and a new macromodel that allows to accurately and efficiently modeling the non-linear effects of the victim driver in noise analysis is presented. Experimental results demonstrate the effectiveness of our method, and confirm that existing noise analysis approaches based on linear superposition of the propagated and crosstalk-injected noise can be highly inaccurate, thus impairing the sign-off functional verification phase.",2007-10-25T08:11:06Z,http://arxiv.org/pdf/0710.4639v1,2024-04-28,
0710.4644v1,"Cycle Accurate Binary Translation for Simulation Acceleration in Rapid Prototyping of SoCs","In this paper, the application of a cycle accurate binary translator for rapid prototyping of SoCs will be presented. This translator generates code to run on a rapid prototyping system consisting of a VLIW processor and FPGAs. The generated code is annotated with information that triggers cycle generation for the hardware in parallel to the execution of the translated program. The VLIW processor executes the translated program whereas the FPGAs contain the hardware for the parallel cycle generation and the bus interface that adapts the bus of the VLIW processor to the SoC bus of the emulated processor core.",2007-10-25T08:18:14Z,http://arxiv.org/pdf/0710.4644v1,2024-04-28,
0710.4645v1,"At-Speed Logic BIST for IP Cores","This paper describes a flexible logic BIST scheme that features high fault coverage achieved by fault-simulation guided test point insertion, real at-speed test capability for multi-clock designs without clock frequency manipulation, and easy physical implementation due to the use of a low-speed SE signal. Application results of this scheme to two widely used IP cores are also reported.",2007-10-25T08:19:34Z,http://arxiv.org/pdf/0710.4645v1,2024-04-28,
0710.4646v1,"Fast Dynamic Memory Integration in Co-Simulation Frameworks for Multiprocessor System on-Chip","In this paper is proposed a technique to integrate and simulate a dynamic memory in a multiprocessor framework based on C/C++/SystemC. Using host machine's memory management capabilities, dynamic data processing is supported without compromising speed and accuracy of the simulation. A first prototype in a shared memory context is presented.",2007-10-25T08:20:27Z,http://arxiv.org/pdf/0710.4646v1,2024-04-28,
0710.4649v1,"Stochastic Power Grid Analysis Considering Process Variations","In this paper, we investigate the impact of interconnect and device process variations on voltage fluctuations in power grids. We consider random variations in the power grid's electrical parameters as spatial stochastic processes and propose a new and efficient method to compute the stochastic voltage response of the power grid. Our approach provides an explicit analytical representation of the stochastic voltage response using orthogonal polynomials in a Hilbert space. The approach has been implemented in a prototype software called OPERA (Orthogonal Polynomial Expansions for Response Analysis). Use of OPERA on industrial power grids demonstrated speed-ups of up to two orders of magnitude. The results also show a significant variation of about $\pm$ 35% in the nominal voltage drops at various nodes of the power grids and demonstrate the need for variation-aware power grid analysis.",2007-10-25T08:24:02Z,http://arxiv.org/pdf/0710.4649v1,2024-04-28,
0710.4652v1,"Locality-Aware Process Scheduling for Embedded MPSoCs","Utilizing on-chip caches in embedded multiprocessor-system-on-a-chip (MPSoC) based systems is critical from both performance and power perspectives. While most of the prior work that targets at optimizing cache behavior are performed at hardware and compilation levels, operating system (OS) can also play major role as it sees the global access pattern information across applications. This paper proposes a cache-conscious OS process scheduling strategy based on data reuse. The proposed scheduler implements two complementary approaches. First, the processes that do not share any data between them are scheduled at different cores if it is possible to do so. Second, the processes that could not be executed at the same time (due to dependences) but share data among each other are mapped to the same processor core so that they share the cache contents. Our experimental results using this new data locality aware OS scheduling strategy are promising, and show significant improvements in task completion times.",2007-10-25T08:31:15Z,http://arxiv.org/pdf/0710.4652v1,2024-04-28,
0710.4653v1,"Simultaneous Reduction of Dynamic and Static Power in Scan Structures","Power dissipation during test is a major challenge in testing integrated circuits. Dynamic power has been the dominant part of power dissipation in CMOS circuits, however, in future technologies the static portion of power dissipation will outreach the dynamic portion. This paper proposes an efficient technique to reduce both dynamic and static power dissipation in scan structures. Scan cell outputs which are not on the critical path(s) are multiplexed to fixed values during scan mode. These constant values and primary inputs are selected such that the transitions occurred on non-multiplexed scan cells are suppressed and the leakage current during scan mode is decreased. A method for finding these vectors is also proposed. Effectiveness of this technique is proved by experiments performed on ISCAS89 benchmark circuits.",2007-10-25T08:32:08Z,http://arxiv.org/pdf/0710.4653v1,2024-04-28,
0710.4654v1,"Modeling Interconnect Variability Using Efficient Parametric Model Order Reduction","Assessing IC manufacturing process fluctuations and their impacts on IC interconnect performance has become unavoidable for modern DSM designs. However, the construction of parametric interconnect models is often hampered by the rapid increase in computational cost and model complexity. In this paper we present an efficient yet accurate parametric model order reduction algorithm for addressing the variability of IC interconnect performance. The efficiency of the approach lies in a novel combination of low-rank matrix approximation and multi-parameter moment matching. The complexity of the proposed parametric model order reduction is as low as that of a standard Krylov subspace method when applied to a nominal system. Under the projection-based framework, our algorithm also preserves the passivity of the resulting parametric models.",2007-10-25T08:33:14Z,http://arxiv.org/pdf/0710.4654v1,2024-04-28,
0710.4655v1,"A Fast Diagnosis Scheme for Distributed Small Embedded SRAMs","This paper proposes a diagnosis scheme aimed at reducing diagnosis time of distributed small embedded SRAMs (e-SRAMs). This scheme improves the one proposed in [A parallel built-in self-diagnostic method for embedded memory buffers, A parallel built-in self-diagnostic method for embedded memory arrays]. The improvements are mainly two-fold. On one hand, the diagnosis of time-consuming Data Retention Faults (DRFs), which is neglected by the diagnosis architecture in [A parallel built-in self-diagnostic method for embedded memory buffers, A parallel built-in self-diagnostic method for embedded memory arrays], is now considered and performed via a DFT technique referred to as the ""No Write Recovery Test Mode (NWRTM)"". On the other hand, a pair comprising a Serial to Parallel Converter (SPC) and a Parallel to Serial Converter (PSC) is utilized to replace the bi-directional serial interface, to avoid the problems of serial fault masking and defect rate dependent diagnosis. Results from our evaluations show that the proposed diagnosis scheme achieves an increased diagnosis coverage and reduces diagnosis time compared to those obtained in [A parallel built-in self-diagnostic method for embedded memory buffers, A parallel built-in self-diagnostic method for embedded memory arrays], with neglectable extra area cost.",2007-10-25T08:34:09Z,http://arxiv.org/pdf/0710.4655v1,2024-04-28,
0710.4656v1,"A Memory Hierarchical Layer Assigning and Prefetching Technique to Overcome the Memory Performance/Energy Bottleneck","The memory subsystem has always been a bottleneck in performance as well as significant power contributor in memory intensive applications. Many researchers have presented multi-layered memory hierarchies as a means to design energy and performance efficient systems. However, most of the previous work do not explore trade-offs systematically. We fill this gap by proposing a formalized technique that takes into consideration data reuse, limited lifetime of the arrays of an application and application specific prefetching opportunities, and performs a thorough trade-off exploration for different memory layer sizes. This technique has been implemented on a prototype tool, which was tested successfully using nine real-life applications of industrial relevance. Following this approach we have able to reduce execution time up to 60%, and energy consumption up to 70%.",2007-10-25T08:34:32Z,http://arxiv.org/pdf/0710.4656v1,2024-04-28,
0710.4657v1,"New Schemes for Self-Testing RAM","This paper gives an overview of a new technique, named pseudo-ring testing (PRT). PRT can be applied for testing wide type of random access memories (RAM): bit- or word-oriented and single- or dual-port RAM's. An essential particularity of the proposed methodology is the emulation of a linear automaton over Galois field by memory own components.",2007-10-25T08:34:59Z,http://arxiv.org/pdf/0710.4657v1,2024-04-28,
0710.4659v1,"Synchronization Processor Synthesis for Latency Insensitive Systems","In this paper we present our contribution in terms of synchronization processor for a SoC design methodology based on the theory of the latency insensitive systems (LIS) of Carloni et al. Our contribution consists in IP encapsulation into a new wrapper model which speed and area are optimized and synthetizability guarantied. The main benefit of our approach is to preserve the local IP performances when encapsulating them and reduce SoC silicon area.",2007-10-25T08:35:37Z,http://arxiv.org/pdf/0710.4659v1,2024-04-28,
0710.4660v1,"Thermal-Aware Task Allocation and Scheduling for Embedded Systems","Temperature affects not only the reliability but also the performance, power, and cost of the embedded system. This paper proposes a thermal-aware task allocation and scheduling algorithm for embedded systems. The algorithm is used as a sub-routine for hardware/software co-synthesis to reduce the peak temperature and achieve a thermally even distribution while meeting real time constraints. The paper investigates both power-aware and thermal-aware approaches to task allocation and scheduling. The experimental results show that the thermal-aware approach outperforms the power-aware schemes in terms of maximal and average temperature reductions. To the best of our knowledge, this is the first task allocation and scheduling algorithm that takes temperature into consideration.",2007-10-25T08:36:55Z,http://arxiv.org/pdf/0710.4660v1,2024-04-28,
0710.4661v1,"Bright-Field AAPSM Conflict Detection and Correction","As feature sizes shrink, it will be necessary to use AAPSM (Alternating-Aperture Phase Shift Masking) to image critical features, especially on the polysilicon layer. This imposes additional constraints on the layouts beyond traditional design rules. Of particular note is the requirement that all critical features be flanked by opposite-phase shifters, while the shifters obey minimum width and spacing requirements. A layout is called phase-assignable if it satisfies this requirement. If a layout is not phase-assignable, the phase conflicts have to removed to enable the use of AAPSM for the layout. Previous work has sought to detect a suitable set of phase Conflicts to be removed, as well as correct them. The contribution of this paper are the following: (1) a new approach to detect a minimal set of phase conflicts (also referred to as AAPSM conflicts), which when corrected will produce a phase-assignable layout; (2) a novel layout modification scheme for correcting these AAPSM conflicts. The proposed approach for conflict detection shows significant improvements in the quality of results and runtime for real industrial circuits, when compared to previous methods. To the best of our knowledge, this is the first time layout modification results are presented for bright-field AAPSM. Our experiments show that the percentage area increase for making a layout phase-assignable ranges from 0.7-11.8%.",2007-10-25T08:37:31Z,http://arxiv.org/pdf/0710.4661v1,2024-04-28,
0710.4663v1,"Statistical Modeling of Pipeline Delay and Design of Pipeline under Process Variation to Enhance Yield in sub-100nm Technologies","Operating frequency of a pipelined circuit is determined by the delay of the slowest pipeline stage. However, under statistical delay variation in sub-100nm technology regime, the slowest stage is not readily identifiable and the estimation of the pipeline yield with respect to a target delay is a challenging problem. We have proposed analytical models to estimate yield for a pipelined design based on delay distributions of individual pipe stages. Using the proposed models, we have shown that change in logic depth and imbalance between the stage delays can improve the yield of a pipeline. A statistical methodology has been developed to optimally design a pipeline circuit for enhancing yield. Optimization results show that, proper imbalance among the stage delays in a pipeline improves design yield by 9% for the same area and performance (and area reduction by about 8.4% under a yield constraint) over a balanced design.",2007-10-25T08:41:06Z,http://arxiv.org/pdf/0710.4663v1,2024-04-28,
0710.4665v1,"New Perspectives and Opportunities From the Wild West of Microelectronic Biochips","Application of Microelectronic to bioanalysis is an emerging field which holds great promise. From the standpoint of electronic and system design, biochips imply a radical change of perspective, since new, completely different constraints emerge while other usual constraints can be relaxed. While electronic parts of the system can rely on the usual established design-flow, fluidic and packaging design, calls for a new approach which relies significantly on experiments. We hereby make some general considerations based on our experience in the development of biochips for cell analysis.",2007-10-25T08:43:31Z,http://arxiv.org/pdf/0710.4665v1,2024-04-28,
0710.4669v1,"SOC Testing Methodology and Practice","On a commercial digital still camera (DSC) controller chip we practice a novel SOC test integration platform, solving real problems in test scheduling, test IO reduction, timing of functional test, scan IO sharing, embedded memory built-in self-test (BIST), etc. The chip has been fabricated and tested successfully by our approach. Test results justify that short test integration cost, short test time, and small area overhead can be achieved. To support SOC testing, a memory BIST compiler and an SOC testing integration system have been developed.",2007-10-25T08:45:18Z,http://arxiv.org/pdf/0710.4669v1,2024-04-28,
0710.4670v1,"Evolutionary Optimization in Code-Based Test Compression","We provide a general formulation for the code-based test compression problem with fixed-length input blocks and propose a solution approach based on Evolutionary Algorithms. In contrast to existing code-based methods, we allow unspecified values in matching vectors, which allows encoding of arbitrary test sets using a relatively small number of code-words. Experimental results for both stuck-at and path delay fault test sets for ISCAS circuits demonstrate an improvement compared to existing techniques.",2007-10-25T08:45:52Z,http://arxiv.org/pdf/0710.4670v1,2024-04-28,
0710.4671v1,"An Application-Specific Design Methodology for STbus Crossbar Generation","As the communication requirements of current and future Multiprocessor Systems on Chips (MPSoCs) continue to increase, scalable communication architectures are needed to support the heavy communication demands of the system. This is reflected in the recent trend that many of the standard bus products such as STbus, have now introduced the capability of designing a crossbar with multiple buses operating in parallel. The crossbar configuration should be designed to closely match the application traffic characteristics and performance requirements. In this work we address this issue of application-specific design of optimal crossbar (using STbus crossbar architecture), satisfying the performance requirements of the application and optimal binding of cores onto the crossbar resources. We present a simulation based design approach that is based on analysis of actual traffic trace of the application, considering local variations in traffic rates, temporal overlap among traffic streams and criticality of traffic streams. Our methodology is applied to several MPSoC designs and the resulting crossbar platforms are validated for performance by cycle-accurate SystemC simulation of the designs. The experimental case studies show large reduction in packet latencies (up to 7x) and large crossbar component savings (up to 3.5x) compared to traditional design approaches.",2007-10-25T08:46:59Z,http://arxiv.org/pdf/0710.4671v1,2024-04-28,
0710.4672v1,"Yield Enhancement of Digital Microfluidics-Based Biochips Using Space Redundancy and Local Reconfiguration","As microfluidics-based biochips become more complex, manufacturing yield will have significant influence on production volume and product cost. We propose an interstitial redundancy approach to enhance the yield of biochips that are based on droplet-based microfluidics. In this design method, spare cells are placed in the interstitial sites within the microfluidic array, and they replace neighboring faulty cells via local reconfiguration. The proposed design method is evaluated using a set of concurrent real-life bioassays.",2007-10-25T08:47:40Z,http://arxiv.org/pdf/0710.4672v1,2024-04-28,
0710.4673v1,"Design of Fault-Tolerant and Dynamically-Reconfigurable Microfluidic Biochips","Microfluidics-based biochips are soon expected to revolutionize clinical diagnosis, DNA sequencing, and other laboratory procedures involving molecular biology. Most microfluidic biochips are based on the principle of continuous fluid flow and they rely on permanently-etched microchannels, micropumps, and microvalves. We focus here on the automated design of ""digital"" droplet-based microfluidic biochips. In contrast to continuous-flow systems, digital microfluidics offers dynamic reconfigurability; groups of cells in a microfluidics array can be reconfigured to change their functionality during the concurrent execution of a set of bioassays. We present a simulated annealing-based technique for module placement in such biochips. The placement procedure not only addresses chip area, but it also considers fault tolerance, which allows a microfluidic module to be relocated elsewhere in the system when a single cell is detected to be faulty. Simulation results are presented for a case study involving the polymerase chain reaction.",2007-10-25T08:48:12Z,http://arxiv.org/pdf/0710.4673v1,2024-04-28,
0710.4678v1,"CMOS-Based Biosensor Arrays","CMOS-based sensor array chips provide new and attractive features as compared to today's standard tools for medical, diagnostic, and biotechnical applications. Examples for molecule- and cell-based approaches and related circuit design issues are discussed.",2007-10-25T09:02:53Z,http://arxiv.org/pdf/0710.4678v1,2024-04-28,
0710.4679v1,"DVS for On-Chip Bus Designs Based on Timing Error Correction","On-chip buses are typically designed to meet performance constraints at worst-case conditions, including process corner, temperature, IR-drop, and neighboring net switching pattern. This can result in significant performance slack at more typical operating conditions. In this paper, we propose a dynamic voltage scaling (DVS) technique for buses, based on a double sampling latch which can detect and correct for delay errors without the need for retransmission. The proposed approach recovers the available slack at non-worst-case operating points through more aggressive voltage scaling and tracks changing conditions by monitoring the error recovery rate. Voltage margins needed in traditional designs to accommodate worst-case performance conditions are therefore eliminated, resulting in a significant improvement in energy efficiency. The approach was implemented for a 6mm memory read bus operating at 1.5GHz (0.13 $\mu$m technology node) and was simulated for a number of benchmark programs. Even at the worst-case process and environment conditions, energy gains of up to 17% are achieved, with error recovery rates under 2.3%. At more typical process and environment conditions, energy gains range from 35% to 45%, with a performance degradation under 2%. An analysis of optimum interconnect architectures for maximizing energy gains with this approach shows that the proposed approach performs well with technology scaling.",2007-10-25T09:03:13Z,http://arxiv.org/pdf/0710.4679v1,2024-04-28,
0710.4681v1,"A Quality-of-Service Mechanism for Interconnection Networks in System-on-Chips","As Moore's Law continues to fuel the ability to build ever increasingly complex system-on-chips (SoCs), achieving performance goals is rising as a critical challenge to completing designs. In particular, the system interconnect must efficiently service a diverse set of data flows with widely ranging quality-of-service (QoS) requirements. However, the known solutions for off-chip interconnects such as large-scale networks are not necessarily applicable to the on-chip environment. Latency and memory constraints for on-chip interconnects are quite different from larger-scale interconnects. This paper introduces a novel on-chip interconnect arbitration scheme. We show how this scheme can be distributed across a chip for high-speed implementation. We compare the performance of the arbitration scheme with other known interconnect arbitration schemes. Existing schemes typically focus heavily on either low latency of service for some initiators, or alternatively on guaranteed bandwidth delivery for other initiators. Our scheme allows service latency on some initiators to be traded off smoothly against jitter bounds on other initiators, while still delivering bandwidth guarantees. This scheme is a subset of the QoS controls that are available in the SonicsMX? (SMX) product.",2007-10-25T09:05:03Z,http://arxiv.org/pdf/0710.4681v1,2024-04-28,
0710.4684v1,"Reliability-Centric High-Level Synthesis","Importance of addressing soft errors in both safety critical applications and commercial consumer products is increasing, mainly due to ever shrinking geometries, higher-density circuits, and employment of power-saving techniques such as voltage scaling and component shut-down. As a result, it is becoming necessary to treat reliability as a first-class citizen in system design. In particular, reliability decisions taken early in system design can have significant benefits in terms of design quality. Motivated by this observation, this paper presents a reliability-centric high-level synthesis approach that addresses the soft error problem. The proposed approach tries to maximize reliability of the design while observing the bounds on area and performance, and makes use of our reliability characterization of hardware components such as adders and multipliers. We implemented the proposed approach, performed experiments with several designs, and compared the results with those obtained by a prior proposal.",2007-10-25T09:07:44Z,http://arxiv.org/pdf/0710.4684v1,2024-04-28,
0710.4685v1,"Reliable System Specification for Self-Checking Data-Paths","The design of reliable circuits has received a lot of attention in the past, leading to the definition of several design techniques introducing fault detection and fault tolerance properties in systems for critical applications/environments. Such design methodologies tackled the problem at different abstraction levels, from switch-level to logic, RT level, and more recently to system level. Aim of this paper is to introduce a novel system-level technique based on the redefinition of the operators functionality in the system specification. This technique provides reliability properties to the system data path, transparently with respect to the designer. Feasibility, fault coverage, performance degradation and overheads are investigated on a FIR circuit.",2007-10-25T09:08:39Z,http://arxiv.org/pdf/0710.4685v1,2024-04-28,
0710.4686v1,"Test Planning for Mixed-Signal SOCs with Wrapped Analog Cores","Many SOCs today contain both digital and analog embedded cores. Even though the test cost for such mixed-signal SOCs is significantly higher than that for digital SOCs, most prior research in this area has focused exclusively on digital cores. We propose a low-cost test development methodology for mixed-signal SOCs that allows the analog and digital cores to be tested in a unified manner, thereby minimizing the overall test cost. The analog cores in the SOC are wrapped such that they can be accessed using a digital test access mechanism (TAM). We evaluate the impact of the use of analog test wrappers on area overhead and test time. To reduce area overhead, we present an analog test wrapper optimization technique, which is then combined with TAM optimization in a cost-oriented heuristic approach for test scheduling. We also demonstrate the feasibility of using analog wrappers by presenting transistor-level simulations for an analog wrapper and a representative core. We present experimental results on test scheduling for an ITC'02 benchmark SOC that has been augmented with five analog cores.",2007-10-25T09:08:40Z,http://arxiv.org/pdf/0710.4686v1,2024-04-28,
0710.4687v1,"On-Chip Test Infrastructure Design for Optimal Multi-Site Testing of System Chips","Multi-site testing is a popular and effective way to increase test throughput and reduce test costs. We present a test throughput model, in which we focus on wafer testing, and consider parameters like test time, index time, abort-on-fail, and contact yield. Conventional multi-site testing requires sufficient ATE resources, such as ATE channels, to allow to test multiple SOCs in parallel. In this paper, we design and optimize on-chip DfT, in order to maximize the test throughput for a given SOC and ATE. The on-chip DfT consists of an E-RPCT wrapper, and, for modular SOCs, module wrappers and TAMs. We present experimental results for a Philips SOC and several ITC'02 SOC Test Benchmarks.",2007-10-25T09:09:14Z,http://arxiv.org/pdf/0710.4687v1,2024-04-28,
0710.4688v1,"On the Optimal Design of Triple Modular Redundancy Logic for SRAM-based FPGAs","Triple Modular Redundancy (TMR) is a suitable fault tolerant technique for SRAM-based FPGA. However, one of the main challenges in achieving 100% robustness in designs protected by TMR running on programmable platforms is to prevent upsets in the routing from provoking undesirable connections between signals from distinct redundant logic parts, which can generate an error in the output. This paper investigates the optimal design of the TMR logic (e.g., by cleverly inserting voters) to ensure robustness. Four different versions of a TMR digital filter were analyzed by fault injection. Faults were randomly inserted straight into the bitstream of the FPGA. The experimental results presented in this paper demonstrate that the number and placement of voters in the TMR design can directly affect the fault tolerance, ranging from 4.03% to 0.98% the number of upsets in the routing able to cause an error in the TMR circuit.",2007-10-25T09:09:34Z,http://arxiv.org/pdf/0710.4688v1,2024-04-28,
0710.4691v1,"An O(bn^2) Time Algorithm for Optimal Buffer Insertion with b Buffer Types","Buffer insertion is a popular technique to reduce the interconnect delay. The classic buffer insertion algorithm of van Ginneken has time complexity O(n^2), where n is the number of buffer positions. Lillis, Cheng and Lin extended van Ginneken's algorithm to allow b buffer types in time O (b^2 n^2). For modern design libraries that contain hundreds of buffers, it is a serious challenge to balance the speed and performance of the buffer insertion algorithm. In this paper, we present a new algorithm that computes the optimal buffer insertion in O (bn^2) time. The reduction is achieved by the observation that the (Q, C) pairs of the candidates that generate the new candidates must form a convex hull. On industrial test cases, the new algorithm is faster than the previous best buffer insertion algorithms by orders of magnitude.",2007-10-25T09:11:11Z,http://arxiv.org/pdf/0710.4691v1,2024-04-28,
0710.4692v1,"Cantilever-Based Biosensors in CMOS Technology","Single-chip CMOS-based biosensors that feature microcantilevers as transducer elements are presented. The cantilevers are functionalized for the capturing of specific analytes, e.g., proteins or DNA. The binding of the analyte changes the mechanical properties of the cantilevers such as surface stress and resonant frequency, which can be detected by an integrated Wheatstone bridge. The monolithic integrated readout allows for a high signal-to-noise ratio, lowers the sensitivity to external interference and enables autonomous device operation.",2007-10-25T09:11:49Z,http://arxiv.org/pdf/0710.4692v1,2024-04-28,
0710.4693v1,"Memory Testing Under Different Stress Conditions: An Industrial Evaluation","This paper presents the effectiveness of various stress conditions (mainly voltage and frequency) on detecting the resistive shorts and open defects in deep sub-micron embedded memories in an industrial environment. Simulation studies on very-low voltage, high voltage and at-speed testing show the need of the stress conditions for high quality products; i.e., low defect-per-million (DPM) level, which is driving the semiconductor market today. The above test conditions have been validated to screen out bad devices on real silicon (a test-chip) built on CMOS 0.18 um technology. IFA (inductive fault analysis) based simulation technique leads to an efficient fault coverage and DPM estimator, which helps the customers upfront to make decisions on test algorithm implementations under different stress conditions in order to reduce the number of test escapes.",2007-10-25T09:14:05Z,http://arxiv.org/pdf/0710.4693v1,2024-04-28,
0710.4697v1,"Statistical Timing Based Optimization using Gate Sizing","The increased dominance of intra-die process variations has motivated the field of Statistical Static Timing Analysis (SSTA) and has raised the need for SSTA-based circuit optimization. In this paper, we propose a new sensitivity based, statistical gate sizing method. Since brute-force computation of the change in circuit delay distribution to gate size change is computationally expensive, we propose an efficient and exact pruning algorithm. The pruning algorithm is based on a novel theory of perturbation bounds which are shown to decrease as they propagate through the circuit. This allows pruning of gate sensitivities without complete propagation of their perturbations. We apply our proposed optimization algorithm to ISCAS benchmark circuits and demonstrate the accuracy and efficiency of the proposed method. Our results show an improvement of up to 10.5% in the 99-percentile circuit delay for the same circuit area, using the proposed statistical optimizer and a run time improvement of up to 56x compared to the brute-force approach.",2007-10-25T09:16:49Z,http://arxiv.org/pdf/0710.4697v1,2024-04-28,
0710.4703v1,"A Way Memoization Technique for Reducing Power Consumption of Caches in Application Specific Integrated Processors","This paper presents a technique for eliminating redundant cache-tag and cache-way accesses to reduce power consumption. The basic idea is to keep a small number of Most Recently Used (MRU) addresses in a Memory Address Buffer (MAB) and to omit redundant tag and way accesses when there is a MAB-hit. Since the approach keeps only tag and set-index values in the MAB, the energy and area overheads are relatively small even for a MAB with a large number of entries. Furthermore, the approach does not sacrifice the performance. In other words, neither the cycle time nor the number of executed cycles increases. The proposed technique has been applied to Fujitsu VLIW processor (FR-V) and its power saving has been estimated using NanoSim. Experiments for 32kB 2-way set associative caches show the power consumption of I-cache and D-cache can be reduced by 40% and 50%, respectively.",2007-10-25T09:27:22Z,http://arxiv.org/pdf/0710.4703v1,2024-04-28,
0710.4704v1,"Resource Sharing and Pipelining in Coarse-Grained Reconfigurable Architecture for Domain-Specific Optimization","Coarse-grained reconfigurable architectures aim to achieve both goals of high performance and flexibility. However, existing reconfigurable array architectures require many resources without considering the specific application domain. Functional resources that take long latency and/or large area can be pipelined and/or shared among the processing elements. Therefore the hardware cost and the delay can be effectively reduced without any performance degradation for some application domains. We suggest such reconfigurable array architecture template and design space exploration flow for domain-specific optimization. Experimental results show that our approach is much more efficient both in performance and area compared to existing reconfigurable architectures.",2007-10-25T09:28:05Z,http://arxiv.org/pdf/0710.4704v1,2024-04-28,
0710.4705v1,"A Study of the Speedups and Competitiveness of FPGA Soft Processor Cores using Dynamic Hardware/Software Partitioning","Field programmable gate arrays (FPGAs) provide designers with the ability to quickly create hardware circuits. Increases in FPGA configurable logic capacity and decreasing FPGA costs have enabled designers to more readily incorporate FPGAs in their designs. FPGA vendors have begun providing configurable soft processor cores that can be synthesized onto their FPGA products. While FPGAs with soft processor cores provide designers with increased flexibility, such processors typically have degraded performance and energy consumption compared to hard-core processors. Previously, we proposed warp processing, a technique capable of optimizing a software application by dynamically and transparently re-implementing critical software kernels as custom circuits in on-chip configurable logic. In this paper, we study the potential of a MicroBlaze soft-core based warp processing system to eliminate the performance and energy overhead of a soft-core processor compared to a hard-core processor. We demonstrate that the soft-core based warp processor achieves average speedups of 5.8 and energy reductions of 57% compared to the soft core alone. Our data shows that a soft-core based warp processor yields performance and energy consumption competitive with existing hard-core processors, thus expanding the usefulness of soft processor cores on FPGAs to a broader range of applications.",2007-10-25T09:29:03Z,http://arxiv.org/pdf/0710.4705v1,2024-04-28,
0710.4706v1,"An Infrastructure to Functionally Test Designs Generated by Compilers Targeting FPGAs","This paper presents an infrastructure to test the functionality of the specific architectures output by a high-level compiler targeting dynamically reconfigurable hardware. It results in a suitable scheme to verify the architectures generated by the compiler, each time new optimization techniques are included or changes in the compiler are performed. We believe this kind of infrastructure is important to verify, by functional simulation, further research techniques, as far as compilation to Field-Programmable Gate Array (FPGA) platforms is concerned.",2007-10-25T09:29:33Z,http://arxiv.org/pdf/0710.4706v1,2024-04-28,
0710.4707v1,"Energy- and Performance-Driven NoC Communication Architecture Synthesis Using a Decomposition Approach","In this paper, we present a methodology for customized communication architecture synthesis that matches the communication requirements of the target application. This is an important problem, particularly for network-based implementations of complex applications. Our approach is based on using frequently encountered generic communication primitives as an alphabet capable of characterizing any given communication pattern. The proposed algorithm searches through the entire design space for a solution that minimizes the system total energy consumption, while satisfying the other design constraints. Compared to the standard mesh architecture, the customized architecture generated by the newly proposed approach shows about 36% throughput increase and 51% reduction in the energy required to encrypt 128 bits of data with a standard encryption algorithm.",2007-10-25T09:29:58Z,http://arxiv.org/pdf/0710.4707v1,2024-04-28,
0710.4709v1,"Analog and Digital Circuit Design in 65 nm CMOS: End of the Road?","This special session adresses the problems that designers face when implementing analog and digital circuits in nanometer technologies. An introductory embedded tutorial will give an overview of the design problems at hand : the leakage power and process variability and their implications for digital circuits and memories, and the reducing supply voltages, the design productivity and signal integrity problems for embedded analog blocks. Next, a panel of experts from both industrial semiconductor houses and design companies, EDA vendors and research institutes will present and discuss with the audience their opinions on whether the design road ends at marker ""65nm"" or not.",2007-10-25T09:30:46Z,http://arxiv.org/pdf/0710.4709v1,2024-04-28,
0710.4711v1,"FPGA Architecture for Multi-Style Asynchronous Logic","This paper presents a novel FPGA architecture for implementing various styles of asynchronous logic. The main objective is to break the dependency between the FPGA architecture dedicated to asynchronous logic and the logic style. The innovative aspects of the architecture are described. Moreover the structure is well suited to be rebuilt and adapted to fit with further asynchronous logic evolutions thanks to the architecture genericity. A full-adder was implemented in different styles of logic to show the architecture flexibility.",2007-10-25T09:32:43Z,http://arxiv.org/pdf/0710.4711v1,2024-04-28,
0710.4712v1,"An Accurate SER Estimation Method Based on Propagation Probability","In this paper, we present an accurate but very fast soft error rate (SER) estimation technique for digital circuits based on error propagation probability (EPP) computation. Experiments results and comparison of the results with the random simulation technique show that our proposed method is on average within 6% of the random simulation method and four to five orders of magnitude faster.",2007-10-25T09:33:14Z,http://arxiv.org/pdf/0710.4712v1,2024-04-28,
0710.4713v1,"Improving the Process-Variation Tolerance of Digital Circuits Using Gate Sizing and Statistical Techniques","A new approach for enhancing the process-variation tolerance of digital circuits is described. We extend recent advances in statistical timing analysis into an optimization framework. Our objective is to reduce the performance variance of a technology-mapped circuit where delays across elements are represented by random variables which capture the manufacturing variations. We introduce the notion of statistical critical paths, which account for both means and variances of performance variation. An optimization engine is used to size gates with a goal of reducing the timing variance along the statistical critical paths. We apply a pair of nested statistical analysis methods deploying a slower more accurate approach for tracking statistical critical paths and a fast engine for evaluation of gate size assignments. We derive a new approximation for the max operation on random variables which is deployed for the faster inner engine. Circuit optimization is carried out using a gain-based algorithm that terminates when constraints are satisfied or no further improvements can be made. We show optimization results that demonstrate an average of 72% reduction in performance variation at the expense of average 20% increase in design area.",2007-10-25T09:33:36Z,http://arxiv.org/pdf/0710.4713v1,2024-04-28,
0710.4714v1,"Assertion-Based Design Exploration of DVS in Network Processor Architectures","With the scaling of technology and higher requirements on performance and functionality, power dissipation is becoming one of the major design considerations in the development of network processors. In this paper, we use an assertion-based methodology for system-level power/performance analysis to study two dynamic voltage scaling (DVS) techniques, traffic-based DVS and execution-based DVS, in a network processor model. Using the automatically generated distribution analyzers, we analyze the power and performance distributions and study their trade-offs for the two DVS policies with different parameter settings such as threshold values and window sizes. We discuss the optimal configurations of the two DVS policies under different design requirements. By a set of experiments, we show that the assertion-based trace analysis methodology is an efficient tool that can help a designer easily compare and study optimal architectural configurations in a large design space.",2007-10-25T09:33:38Z,http://arxiv.org/pdf/0710.4714v1,2024-04-28,
0710.4715v1,"Circuit-Level Modeling for Concurrent Testing of Operational Defects due to Gate Oxide Breakdown","As device sizes shrink and current densities increase, the probability of device failures due to gate oxide breakdown (OBD) also increases. To provide designs that are tolerant to such failures, we must investigate and understand the manifestations of this physical phenomenon at the circuit and system level. In this paper, we develop a model for operational OBD defects, and we explore how to test for faults due to OBD. For a NAND gate, we derive the necessary input conditions that excite and detect errors due to OBD defects at the gate level. We show that traditional pattern generators fail to exercise all of these defects. Finally, we show that these test patterns can be propagated and justified for a combinational circuit in a manner similar to traditional ATPG.",2007-10-25T09:34:11Z,http://arxiv.org/pdf/0710.4715v1,2024-04-28,
0710.4716v1,"Optimized Generation of Data-Path from C Codes for FPGAs","FPGAs, as computing devices, offer significant speedup over microprocessors. Furthermore, their configurability offers an advantage over traditional ASICs. However, they do not yet enjoy high-level language programmability, as microprocessors do. This has become the main obstacle for their wider acceptance by application designers. ROCCC is a compiler designed to generate circuits from C source code to execute on FPGAs, more specifically on CSoCs. It generates RTL level HDLs from frequently executing kernels in an application. In this paper, we describe ROCCC's system overview and focus on its data path generation. We compare the performance of ROCCC-generated VHDL code with that of Xilinx IPs. The synthesis result shows that ROCCC-generated circuit takes around 2x ~ 3x area and runs at comparable clock rate.",2007-10-25T09:34:19Z,http://arxiv.org/pdf/0710.4716v1,2024-04-28,
0710.4717v1,"Multi-Placement Structures for Fast and Optimized Placement in Analog Circuit Synthesis","This paper presents the novel idea of multi-placement structures, for a fast and optimized placement instantiation in analog circuit synthesis. These structures need to be generated only once for a specific circuit topology. When used in synthesis, these pre-generated structures instantiate various layout floorplans for various sizes and parameters of a circuit. Unlike procedural layout generators, they enable fast placement of circuits while keeping the quality of the placements at a high level during a synthesis process. The fast placement is a result of high speed instantiation resulting from the efficiency of the multi-placement structure. The good quality of placements derive from the extensive and intelligent search process that is used to build the multi-placement structure. The target benchmarks of these structures are analog circuits in the vicinity of 25 modules. An algorithm for the generation of such multi-placement structures is presented. Experimental results show placement execution times with an average of a few milliseconds making them usable during layout-aware synthesis for optimized placements.",2007-10-25T09:35:03Z,http://arxiv.org/pdf/0710.4717v1,2024-04-28,
0710.4719v1,"Specification Test Compaction for Analog Circuits and MEMS","Testing a non-digital integrated system against all of its specifications can be quite expensive due to the elaborate test application and measurement setup required. We propose to eliminate redundant tests by employing e-SVM based statistical learning. Application of the proposed methodology to an operational amplifier and a MEMS accelerometer reveal that redundant tests can be statistically identified from a complete set of specification-based tests with negligible error. Specifically, after eliminating five of eleven specification-based tests for an operational amplifier, the defect escape and yield loss is small at 0.6% and 0.9%, respectively. For the accelerometer, defect escape of 0.2% and yield loss of 0.1% occurs when the hot and colt tests are eliminated. For the accelerometer, this level of Compaction would reduce test cost by more than half.",2007-10-25T09:36:21Z,http://arxiv.org/pdf/0710.4719v1,2024-04-28,
0710.4720v1,"Soft-Error Tolerance Analysis and Optimization of Nanometer Circuits","Nanometer circuits are becoming increasingly susceptible to soft-errors due to alpha-particle and atmospheric neutron strikes as device scaling reduces node capacitances and supply/threshold voltage scaling reduces noise margins. It is becoming crucial to add soft-error tolerance estimation and optimization to the design flow to handle the increasing susceptibility. The first part of this paper presents a tool for accurate soft-error tolerance analysis of nanometer circuits (ASERTA) that can be used to estimate the soft-error tolerance of nanometer circuits consisting of millions of gates. The tolerance estimates generated by the tool match SPICE generated estimates closely while taking orders of magnitude less computation time. The second part of the paper presents a tool for soft-error tolerance optimization of nanometer circuits (SERTOPT) using the tolerance estimates generated by ASERTA. The tool finds optimal sizes, channel lengths, supply voltages and threshold voltages to be assigned to gates in a combinational circuit such that the soft-error tolerance is increased while meeting the timing constraint. Experiments on ISCAS'85 benchmark circuits showed that soft-error rate of the optimized circuit decreased by as much as 47% with marginal increase in circuit delay.",2007-10-25T09:36:27Z,http://arxiv.org/pdf/0710.4720v1,2024-04-28,
0710.4721v1,"IEEE 1149.4 Compatible ABMs for Basic RF Measurements","An analogue testing standard IEEE 1149.4 is mainly targeted for low-frequency testing. The problem studied in this paper is extending the standard also for radio frequency testing. IEEE 1149.4 compatible measurement structures (ABMs) developed in this study extract the information one is measuring from the radio frequency signal and represent the result as a DC voltage level. The ABMs presented in this paper are targeted for power and frequency measurements operating in frequencies from 1 GHz to 2 GHz. The power measurement error caused by temperature, supply voltage and process variations is roughly 2 dB and the frequency measurement error is 0.1 GHz, respectively.",2007-10-25T09:36:53Z,http://arxiv.org/pdf/0710.4721v1,2024-04-28,
0710.4722v1,"Designer-Driven Topology Optimization for Pipelined Analog to Digital Converters","This paper suggests a practical ""hybrid"" synthesis methodology which integrates designer-derived analytical models for system-level description with simulation-based models at the circuit level. We show how to optimize stage-resolution to minimize the power in a pipelined ADC. Exploration (via detailed synthesis) of several ADC configurations is used to show that a 4-3-2... resolution distribution uses the least power for a 13-bit 40 MSPS converter in a 0.25 $\mu$m CMOS process.",2007-10-25T09:36:56Z,http://arxiv.org/pdf/0710.4722v1,2024-04-28,
0710.4724v1,"Systematic Figure of Merit Computation for the Design of Pipeline ADC","The emerging concept of SoC-AMS leads to research new top-down methodologies to aid systems designers in sizing analog and mixed devices. This work applies this idea to the high-level optimization of pipeline ADC. Considering a given technology, it consists in comparing different configurations according to their imperfections and their architectures without FFT computation or time-consuming simulations. The final selection is based on a figure of merit.",2007-10-25T09:37:45Z,http://arxiv.org/pdf/0710.4724v1,2024-04-28,
0710.4727v1,"Top-Down Design of a Low-Power Multi-Channel 2.5-Gbit/s/Channel Gated Oscillator Clock-Recovery Circuit","We present a complete top-down design of a low-power multi-channel clock recovery circuit based on gated current-controlled oscillators. The flow includes several tools and methods used to specify block constraints, to design and verify the topology down to the transistor level, as well as to achieve a power consumption as low as 5mW/Gbit/s. Statistical simulation is used to estimate the achievable bit error rate in presence of phase and frequency errors and to prove the feasibility of the concept. VHDL modeling provides extensive verification of the topology. Thermal noise modeling based on well-known concepts delivers design parameters for the device sizing and biasing. We present two practical examples of possible design improvements analyzed and implemented with this methodology.",2007-10-25T09:38:14Z,http://arxiv.org/pdf/0710.4727v1,2024-04-28,
0710.4728v1,"Energy-Aware Routing for E-Textile Applications","As the scale of electronic devices shrinks, ""electronic textiles"" (e-textiles) will make possible a wide variety of novel applications which are currently unfeasible. Due to the wearability concerns, low-power techniques are critical for e-textile applications. In this paper, we address the issue of the energy-aware routing for e-textile platforms and propose an efficient algorithm to solve it. The platform we consider consists of dedicated components for e-textiles, including computational modules, dedicated transmission lines and thin-film batteries on fiber substrates. Furthermore, we derive an analytical upper bound for the achievable number of jobs completed over all possible routing strategies. From a practical standpoint, for the Advanced Encryption Standard (AES) cipher, the routing technique we propose achieves about fifty percent of this analytical upper bound. Moreover, compared to the non-energy-aware counterpart, our routing technique increases the number of encryption jobs completed by one order of magnitude.",2007-10-25T09:38:43Z,http://arxiv.org/pdf/0710.4728v1,2024-04-28,
0710.4729v1,"Modeling and Analysis of Loading Effect in Leakage of Nano-Scaled Bulk-CMOS Logic Circuits","In nanometer scaled CMOS devices significant increase in the subthreshold, the gate and the reverse biased junction band-to-band-tunneling (BTBT) leakage, results in the large increase of total leakage power in a logic circuit. Leakage components interact with each other in device level (through device geometry, doping profile) and also in the circuit level (through node voltages). Due to the circuit level interaction of the different leakage components, the leakage of a logic gate strongly depends on the circuit topology i.e. number and nature of the other logic gates connected to its input and output. In this paper, for the first time, we have analyzed loading effect on leakage and proposed a method to accurately estimate the total leakage in a logic circuit, from its logic level description considering the impact of loading and transistor stacking.",2007-10-25T09:39:25Z,http://arxiv.org/pdf/0710.4729v1,2024-04-28,
0710.4731v1,"Leakage-Aware Interconnect for On-Chip Network","On-chip networks have been proposed as the interconnect fabric for future systems-on-chip and multi-processors on chip. Power is one of the main constraints of these systems and interconnect consumes a significant portion of the power budget. In this paper, we propose four leakage-aware interconnect schemes. Our schemes achieve 10.13%~63.57% active leakage savings and 12.35%~95.96% standby leakage savings across schemes while the delay penalty ranges from 0% to 4.69%.",2007-10-25T09:40:02Z,http://arxiv.org/pdf/0710.4731v1,2024-04-28,
0710.4733v1,"Smart Temperature Sensor for Thermal Testing of Cell-Based ICs","In this paper we present a simple and efficient built-in temperature sensor for thermal monitoring of standard-cell based VLSI circuits. The proposed smart temperature sensor uses a ring-oscillator composed of complex gates instead of inverters to optimize their linearity. Simulation results from a 0.18$\mu$m CMOS technology show that the non-linearity error of the sensor can be reduced when an adequate set of standard logic gates is selected.",2007-10-25T09:41:13Z,http://arxiv.org/pdf/0710.4733v1,2024-04-28,
0710.4735v1,"Worst-Case and Average-Case Analysis of n-Detection Test Sets","Test sets that detect each target fault n times (n-detection test sets) are typically generated for restricted values of n due to the increase in test set size with n. We perform both a worst-case analysis and an average-case analysis to check the effect of restricting n on the unmodeled fault coverage of an (arbitrary) n-detection test set. Our analysis is independent of any particular test set or test generation approach. It is based on a specific set of target faults and a specific set of untargeted faults. It shows that, depending on the circuit, very large values of n may be needed to guarantee the detection of all the untargeted faults. We discuss the implications of these results.",2007-10-25T09:42:30Z,http://arxiv.org/pdf/0710.4735v1,2024-04-28,
0710.4736v1,"A New Embedded Measurement Structure for eDRAM Capacitor","The embedded DRAM (eDRAM) is more and more used in System On Chip (SOC). The integration of the DRAM capacitor process into a logic process is challenging to get satisfactory yields. The specific process of DRAM capacitor and the low capacitance value (~30F) of this device induce problems of process monitoring and failure analysis. We propose a new test structure to measure the capacitance value of each DRAM cell capacitor in a DRAM array. This concept has been validated by simulation on a 0.18$\mu$m eDRAM technology.",2007-10-25T09:42:35Z,http://arxiv.org/pdf/0710.4736v1,2024-04-28,
0710.4738v1,"Exploring NoC Mapping Strategies: An Energy and Timing Aware Technique","Complex applications implemented as Systems on Chip (SoCs) demand extensive use of system level modeling and validation. Their implementation gathers a large number of complex IP cores and advanced interconnection schemes, such as hierarchical bus architectures or networks on chip (NoCs). Modeling applications involves capturing its computation and communication characteristics. Previously proposed communication weighted models (CWM) consider only the application communication aspects. This work proposes a communication dependence and computation model (CDCM) that can simultaneously consider both aspects of an application. It presents a solution to the problem of mapping applications on regular NoCs while considering execution time and energy consumption. The use of CDCM is shown to provide estimated average reductions of 40% in execution time, and 20% in energy consumption, for current technologies.",2007-10-25T09:43:59Z,http://arxiv.org/pdf/0710.4738v1,2024-04-28,
0710.4742v1,"Hardware Accelerated Power Estimation","In this paper, we present power emulation, a novel design paradigm that utilizes hardware acceleration for the purpose of fast power estimation. Power emulation is based on the observation that the functions necessary for power estimation (power model evaluation, aggregation, etc.) can be implemented as hardware circuits. Therefore, we can enhance any given design with ""power estimation hardware"", map it to a prototyping platform, and exercise it with any given test stimuli to obtain power consumption estimates. Our empirical studies with industrial designs reveal that power emulation can achieve significant speedups (10X to 500X) over state-of-the-art commercial register-transfer level (RTL) power estimation tools.",2007-10-25T09:45:28Z,http://arxiv.org/pdf/0710.4742v1,2024-04-28,
0710.4747v1,"An Efficient Transparent Test Scheme for Embedded Word-Oriented Memories","Memory cores are usually the densest portion with the smallest feature size in system-on-chip (SOC) designs. The reliability of memory cores thus has heavy impact on the reliability of SOCs. Transparent test is one of useful technique for improving the reliability of memories during life time. This paper presents a systematic algorithm used for transforming a bit-oriented march test into a transparent word-oriented march test. The transformed transparent march test has shorter test complexity compared with that proposed in the previous works [Theory of transparent BIST for RAMs, A transparent online memory test for simultaneous detection of functional faults and soft errors in memories]. For example, if a memory with 32-bit words is tested with March C-, time complexity of the transparent word-oriented test transformed by the proposed scheme is only about 56% or 19% time complexity of the transparent word-oriented test converted by the scheme reported in [Theory of transparent BIST for RAMs] or [A transparent online memory test for simultaneous detection of functional faults and soft errors in memories], respectively.",2007-10-25T09:48:22Z,http://arxiv.org/pdf/0710.4747v1,2024-04-28,
0710.4748v1,"Systematic Transaction Level Modeling of Embedded Systems with SystemC","This paper gives an overview of a transaction level modeling (TLM) design flow for straightforward embedded system design with SystemC. The goal is to systematically develop both application-specific HW and SW components of an embedded system using the TLM approach, thus allowing for fast communication architecture exploration, rapid prototyping and early embedded SW development. To this end, we specify the lightweight transaction-based communication protocol SHIP and present a methodology for automatic mapping of the communication part of a system to a given architecture, including HW/SW interfaces.",2007-10-25T09:49:10Z,http://arxiv.org/pdf/0710.4748v1,2024-04-28,
0710.4751v1,"Influence of Memory Hierarchies on Predictability for Time Constrained Embedded Software","Safety-critical embedded systems having to meet real-time constraints are expected to be highly predictable in order to guarantee at design time that certain timing deadlines will always be met. This requirement usually prevents designers from utilizing caches due to their highly dynamic, thus hardly predictable behavior. The integration of scratchpad memories represents an alternative approach which allows the system to benefit from a performance gain comparable to that of caches while at the same time maintaining predictability. In this work, we compare the impact of scratchpad memories and caches on worst case execution time (WCET) analysis results. We show that caches, despite requiring complex techniques, can have a negative impact on the predicted WCET, while the estimated WCET for scratchpad memories scales with the achieved Performance gain at no extra analysis cost.",2007-10-25T09:51:11Z,http://arxiv.org/pdf/0710.4751v1,2024-04-28,
0710.4754v1,"Design of a Virtual Component Neutral Network-on-Chip Transaction Layer","Research studies have demonstrated the feasibility and advantages of Network-on-Chip (NoC) over traditional bus-based architectures but have not focused on compatibility communication standards. This paper describes a number of issues faced when designing a VC-neutral NoC, i.e. compatible with standards such as AHB 2.0, AXI, VCI, OCP, and various other proprietary protocols, and how a layered approach to communication helps solve these issues.",2007-10-25T09:52:56Z,http://arxiv.org/pdf/0710.4754v1,2024-04-28,
0710.4757v1,"Techniques for Fast Transient Fault Grading Based on Autonomous Emulation","Very deep submicron and nanometer technologies have increased notably integrated circuit (IC) sensitiveness to radiation. Soft errors are currently appearing into ICs working at earth surface. Hardened circuits are currently required in many applications where Fault Tolerance (FT) was not a requirement in the very near past. The use of platform FPGAs for the emulation of single-event upset effects (SEU) is gaining attention in order to speed up the FT evaluation. In this work, a new emulation system for FT evaluation with respect to SEU effects is proposed, providing shorter evaluation times by performing all the evaluation process in the FPGA and avoiding emulator-host communication bottlenecks.",2007-10-25T09:53:37Z,http://arxiv.org/pdf/0710.4757v1,2024-04-28,
0710.4759v1,"A Fast Concurrent Power-Thermal Model for Sub-100nm Digital ICs","As technology scales down, the static power is expected to become a significant fraction of the total power. The exponential dependence of static power with the operating temperature makes the thermal profile estimation of high-performance ICs a key issue to compute the total power dissipated in next-generations. In this paper we present accurate and compact analytical models to estimate the static power dissipation and the temperature of operation of CMOS gates. The models are the fundamentals of a performance estimation tool in which numerical procedures are avoided for any computation to set a faster estimation and optimization. The models developed are compared to measurements and SPICE simulations for a 0.12mm technology showing excellent results.",2007-10-25T09:54:18Z,http://arxiv.org/pdf/0710.4759v1,2024-04-28,
0710.4760v1,"Low Power Oriented CMOS Circuit Optimization Protocol","Low power oriented circuit optimization consists in selecting the best alternative between gate sizing, buffer insertion and logic structure transformation, for satisfying a delay constraint at minimum area cost. In this paper we used a closed form model of delay in CMOS structures to define metrics for a deterministic selection of the optimization alternative. The target is delay constraint satisfaction with minimum area cost. We validate the design space exploration method, defining maximum and minimum delay bounds on logical paths. Then we adapt this method to a ""constant sensitivity method"" allowing to size a circuit at minimum area under a delay constraint. An optimisation protocol is finally defined to manage the trade-off performance constraint - circuit structure. These methods are implemented in an optimization tool (POPS) and validated by comparing on a 0.25$\mu$m process, the optimization efficiency obtained on various benchmarks (ISCAS?85) to that resulting from an industrial tool.",2007-10-25T09:54:46Z,http://arxiv.org/pdf/0710.4760v1,2024-04-28,
0710.4761v1,"Low-Cost Multi-Gigahertz Test Systems Using CMOS FPGAs and PECL","This paper describes two research projects that develop new low-cost techniques for testing devices with multiple high-speed (2 to 5 Gbps) signals. Each project uses commercially available components to keep costs low, yet achieves performance characteristics comparable to (and in some ways exceeding) more expensive ATE. A common CMOS FPGA-based logic core provides flexibility, adaptability, and communication with controlling computers while customized positive emitter-coupled logic (PECL) achieves multi-gigahertz data rates with about $\pm$25ps timing accuracy.",2007-10-25T09:55:04Z,http://arxiv.org/pdf/0710.4761v1,2024-04-28,
0710.4762v1,"Area-Efficient Selective Multi-Threshold CMOS Design Methodology for Standby Leakage Power Reduction","This paper presents a design flow for an improved selective multi-threshold(Selective-MT) circuit. The Selective-MT circuit is improved so that plural MT-cells can share one switch transistor. We propose the design methodology from RTL(Register Transfer Level) to final layout with optimizing switch transistor structure.",2007-10-25T09:55:21Z,http://arxiv.org/pdf/0710.4762v1,2024-04-28,
0710.4763v1,"Logic Design for On-Chip Test Clock Generation - Implementation Details and Impact on Delay Test Quality","This paper addresses delay test for SOC devices with high frequency clock domains. A logic design for on-chip high-speed clock generation, implemented to avoid expensive test equipment, is described in detail. Techniques for on-chip clock generation, meant to reduce test vector count and to increase test quality, are discussed. ATPG results for the proposed techniques are given.",2007-10-25T09:55:27Z,http://arxiv.org/pdf/0710.4763v1,2024-04-28,
0710.4764v1,"Hotspot Prevention Through Runtime Reconfiguration in Network-On-Chip","Many existing thermal management techniques focus on reducing the overall power consumption of the chip, and do not address location-specific temperature problems referred to as hotspots. We propose the use of dynamic runtime reconfiguration to shift the hotspot-inducing computation periodically and make the thermal profile more uniform. Our analysis shows that dynamic reconfiguration is an effective technique in reducing hotspots for NoCs.",2007-10-25T09:55:48Z,http://arxiv.org/pdf/0710.4764v1,2024-04-28,
0710.4794v1,"Power-Performance Trade-Offs in Nanometer-Scale Multi-Level Caches Considering Total Leakage","In this paper, we investigate the impact of T_{ox} and Vth on power performance trade-offs for on-chip caches. We start by examining the optimization of the various components of a single level cache and then extend this to two level cache systems. In addition to leakage, our studies also account for the dynamic power expanded as a result of cache misses. Our results show that one can often reduce overall power by increasing the size of the L2 cache if we only allow one pair of Vth/T_{ox} in L2. However, if we allow the memory cells and the peripherals to have their own Vth's and T_{ox}'s, we show that a two-level cache system with smaller L2's will yield less total leakage. We further show that two Vth's and two T_{ox}'s are sufficient to get close to an optimal solution, and that Vth is generally a better design knob than T_{ox} for leakage optimization, thus it is better to restrict the number of T_{ox}'s rather than Vth's if cost is a concern.",2007-10-25T11:51:44Z,http://arxiv.org/pdf/0710.4794v1,2024-04-28,
0710.4795v1,"Test Time Reduction Reusing Multiple Processors in a Network-on-Chip Based Architecture","The increasing complexity and the short life cycles of embedded systems are pushing the current system-on-chip designs towards a rapid increasing on the number of programmable processing units, while decreasing the gate count for custom logic. Considering this trend, this work proposes a test planning method capable of reusing available processors as test sources and sinks, and the on-chip network as the test access mechanism. Experimental results are based on ITC'02 benchmarks and on two open core processors compliant with MIPS and SPARC instruction set. The results show that the cooperative use of both the on-chip network and the embedded processors can increase the test parallelism and reduce the test time without additional cost in area and pins.",2007-10-25T11:52:22Z,http://arxiv.org/pdf/0710.4795v1,2024-04-28,
0710.4796v1,"A Hybrid Prefetch Scheduling Heuristic to Minimize at Run-Time the Reconfiguration Overhead of Dynamically Reconfigurable Hardware","Due to the emergence of highly dynamic multimedia applications there is a need for flexible platforms and run-time scheduling support for embedded systems. Dynamic Reconfigurable Hardware (DRHW) is a promising candidate to provide this flexibility but, currently, not sufficient run-time scheduling support to deal with the run-time reconfigurations exists. Moreover, executing at run-time a complex scheduling heuristic to provide this support may generate an excessive run-time penalty. Hence, we have developed a hybrid design/run-time prefetch heuristic that schedules the reconfigurations at run-time, but carries out the scheduling computations at design-time by carefully identifying a set of near-optimal schedules that can be selected at run-time. This approach provides run-time flexibility with a negligible penalty.",2007-10-25T11:53:03Z,http://arxiv.org/pdf/0710.4796v1,2024-04-28,
0710.4801v1,"Behavioural Transformation to Improve Circuit Performance in High-Level Synthesis","Early scheduling algorithms usually adjusted the clock cycle duration to the execution time of the slowest operation. This resulted in large slack times wasted in those cycles executing faster operations. To reduce the wasted times multi-cycle and chaining techniques have been employed. While these techniques have produced successful designs, its effectiveness is often limited due to the area increment that may derive from chaining, and the extra latencies that may derive from multicycling. In this paper we present an optimization method that solves the time-constrained scheduling problem by transforming behavioural specifications into new ones whose subsequent synthesis substantially improves circuit performance. Our proposal breaks up some of the specification operations, allowing their execution during several possibly unconsecutive cycles, and also the calculation of several data-dependent operation fragments in the same cycle. To do so, it takes into account the circuit latency and the execution time of every specification operation. The experimental results carried out show that circuits obtained from the optimized specification are on average 60% faster than those synthesized from the original specification, with only slight increments in the circuit area.",2007-10-25T11:55:59Z,http://arxiv.org/pdf/0710.4801v1,2024-04-28,
0710.4805v1,"Modeling of a Reconfigurable OFDM IP Block Family For an RF System Simulator","The idea of design domain specific Mother Model of IP block family as a base of modeling of system integration is presented here. A common reconfigurable Mother Model for ten different standardized digital OFDM transmitters has been developed. By means of a set of parameters, the mother model can be reconfigured to any of the ten selected standards. So far the applicability of the proposed reconfiguration and analog-digital co-modeling methods have been proved by modeling the function of the digital parts of three, 802.11a, ADSL and DRM, transmitters in an RF system simulator. The model is intended to be used as signal source template in RF system simulations. The concept is not restricted to signal sources, it can be applied to any IP block development. The idea of the Mother Model will be applied in other design domains to prove that in certain application areas, OFDM transceivers in this case, the design process can progress simultaneously in different design domains - mixed signal, system and RTL-architectural - without the need of high-level synthesis. Only the Mother Models of three design domains are needed to be formally proved to function as specified.",2007-10-25T11:57:50Z,http://arxiv.org/pdf/0710.4805v1,2024-04-28,
0710.4806v1,"A VLSI Design Flow for Secure Side-Channel Attack Resistant ICs","This paper presents a digital VLSI design flow to create secure, side-channel attack (SCA) resistant integrated circuits. The design flow starts from a normal design in a hardware description language such as VHDL or Verilog and provides a direct path to a SCA resistant layout. Instead of a full custom layout or an iterative design process with extensive simulations, a few key modifications are incorporated in a regular synchronous CMOS standard cell design flow. We discuss the basis for side-channel attack resistance and adjust the library databases and constraints files of the synthesis and place & route procedures accordingly. Experimental results show that a DPA attack on a regular single ended CMOS standard cell implementation of a module of the DES algorithm discloses the secret key after 200 measurements. The same attack on a secure version still does not disclose the secret key after more than 2000 measurements.",2007-10-25T11:57:56Z,http://arxiv.org/pdf/0710.4806v1,2024-04-28,
0710.4808v1,"Fast and Accurate Transaction Level Modeling of an Extended AMBA2.0 Bus Architecture","Transaction Level Modeling (TLM) approach is used to meet the simulation speed as well as cycle accuracy for large scale SoC performance analysis. We implemented a transaction-level model of a proprietary bus called AHB+ which supports an extended AMBA2.0 protocol. The AHB+ transaction-level model shows 353 times faster than pin-accurate RTL model while maintaining 97% of accuracy on average. We also present the development procedure of TLM of a bus architecture.",2007-10-25T11:59:15Z,http://arxiv.org/pdf/0710.4808v1,2024-04-28,
0710.4809v1,"C Based Hardware Design for Wireless Applications","The algorithms used in wireless applications are increasingly more sophisticated and consequently more challenging to implement in hardware. Traditional design flows require developing the micro architecture, coding the RTL, and verifying the generated RTL against the original functional C or MATLAB specification. This paper describes a C-based design flow that is well suited for the hardware implementation of DSP algorithms commonly found in wireless applications. The C design flow relies on guided synthesis to generate the RTL directly from the untimed C algorithm. The specifics of the C-based design flow are described using a simple DSP filtering algorithm consisting of a forward adaptive equalizer, a 64-QAM slicer and an adaptive decision feedback equalizer. The example illustrates some of the capabilities and advantages offered by this flow.",2007-10-25T11:59:48Z,http://arxiv.org/pdf/0710.4809v1,2024-04-28,
0710.4812v1,"Area and Throughput Trade-Offs in the Design of Pipelined Discrete Wavelet Transform Architectures","The JPEG2000 standard defines the discrete wavelet transform (DWT) as a linear space-to-frequency transform of the image domain in an irreversible compression. This irreversible discrete wavelet transform is implemented by FIR filter using 9/7 Daubechies coefficients or a lifting scheme of factorizated coefficients from 9/7 Daubechies coefficients. This work investigates the tradeoffs between area, power and data throughput (or operating frequency) of several implementations of the Discrete Wavelet Transform using the lifting scheme in various pipeline designs. This paper shows the results of five different architectures synthesized and simulated in FPGAs. It concludes that the descriptions with pipelined operators provide the best area-power-operating frequency trade-off over non-pipelined operators descriptions. Those descriptions require around 40% more hardware to increase the maximum operating frequency up to 100% and reduce power consumption to less than 50%. Starting from behavioral HDL descriptions provide the best area-power-operating frequency trade-off, improving hardware cost and maximum operating frequency around 30% in comparison to structural descriptions for the same power requirement.",2007-10-25T12:00:41Z,http://arxiv.org/pdf/0710.4812v1,2024-04-28,
0710.4813v1,"Queue Management in Network Processors","One of the main bottlenecks when designing a network processing system is very often its memory subsystem. This is mainly due to the state-of-the-art network links operating at very high speeds and to the fact that in order to support advanced Quality of Service (QoS), a large number of independent queues is desirable. In this paper we analyze the performance bottlenecks of various data memory managers integrated in typical Network Processing Units (NPUs). We expose the performance limitations of software implementations utilizing the RISC processing cores typically found in most NPU architectures and we identify the requirements for hardware assisted memory management in order to achieve wire-speed operation at gigabit per second rates. Furthermore, we describe the architecture and performance of a hardware memory manager that fulfills those requirements. This memory manager, although it is implemented in a reconfigurable technology, it can provide up to 6.2Gbps of aggregate throughput, while handling 32K independent queues.",2007-10-25T12:00:48Z,http://arxiv.org/pdf/0710.4813v1,2024-04-28,
0710.4814v1,"picoArray Technology: The Tool's Story","This paper briefly describes the picoArray? architecture, and in particular the deterministic internal communication fabric. The methods that have been developed for debugging and verifying systems using devices from the picoArray family are explained. In order to maximize the computational ability of these devices, hardware debugging support has been kept to a minimum and the methods and tools developed to take this into account.",2007-10-25T12:01:15Z,http://arxiv.org/pdf/0710.4814v1,2024-04-28,
0710.4820v1,"ISEGEN: Generation of High-Quality Instruction Set Extensions by Iterative Improvement","Customization of processor architectures through Instruction Set Extensions (ISEs) is an effective way to meet the growing performance demands of embedded applications. A high-quality ISE generation approach needs to obtain results close to those achieved by experienced designers, particularly for complex applications that exhibit regularity: expert designers are able to exploit manually such regularity in the data flow graphs to generate high-quality ISEs. In this paper, we present ISEGEN, an approach that identifies high-quality ISEs by iterative improvement following the basic principles of the well-known Kernighan-Lin (K-L) min-cut heuristic. Experimental results on a number of MediaBench, EEMBC and cryptographic applications show that our approach matches the quality of the optimal solution obtained by exhaustive search. We also show that our ISEGEN technique is on average 20x faster than a genetic formulation that generates equivalent solutions. Furthermore, the ISEs identified by our technique exhibit 35% more speedup than the genetic solution on a large cryptographic application (AES) by effectively exploiting its regular structure.",2007-10-25T12:04:11Z,http://arxiv.org/pdf/0710.4820v1,2024-04-28,
0710.4824v1,"FPGA based Agile Algorithm-On-Demand Co-Processor","With growing computational needs of many real-world applications, frequently changing specifications of standards, and the high design and NRE costs of ASICs, an algorithm-agile FPGA based co-processor has become a viable alternative. In this article, we report about the general design of an algorith-agile co-processor and the proof-of-concept implementation.",2007-10-25T12:04:43Z,http://arxiv.org/pdf/0710.4824v1,2024-04-28,
0710.4825v1,"Meeting the Embedded Design Needs of Automotive Applications","The importance of embedded systems in driving innovation in automotive applications continues to grow. Understanding the specific needs of developers targeting this market is also helping to drive innovation in RISC core design. This paper describes how a RISC instruction set architecture has evolved to better meet those needs, and the key implementation features in two very different RISC cores are used to demonstrate the challenges of designing for real-time automotive systems.",2007-10-25T12:05:33Z,http://arxiv.org/pdf/0710.4825v1,2024-04-28,
0710.4826v1,"The Integration of On-Line Monitoring and Reconfiguration Functions using EDAA - European design and Automation Association1149.4 Into a Safety Critical Automotive Electronic Control Unit","This paper presents an innovative application of EDAA - European design and Automation Association 1149.4 and the Integrated Diagnostic Reconfiguration (IDR) as tools for the implementation of an embedded test solution for an Automotive Electronic Control Unit implemented as a fully integrated mixed signal system. The paper described how the test architecture can be used for fault avoidance with results from a hardware prototype presented. The paper concludes that fault avoidance can be integrated into mixed signal electronic systems to handle key failure modes.",2007-10-25T12:06:43Z,http://arxiv.org/pdf/0710.4826v1,2024-04-28,
0710.4827v1,"Debug Support, Calibration and Emulation for Multiple Processor and Powertrain Control SoCs","The introduction of complex SoCs with multiple processor cores presents new development challenges, such that development support is now a decisive factor when choosing a System-on-Chip (SoC). The presented developments support strategy addresses the challenges using both architecture and technology approaches. The Multi-Core Debug Support (MCDS) architecture provides flexible triggering using cross triggers and a multiple core break and suspend switch. Temporal trace ordering is guaranteed down to cycle level by on-chip time stamping. The Package Sized-ICE (PSI) approach is a novel method of including trace buffers, overlay memories, processing resources and communication interfaces without changing device behavior. PSI requires no external emulation box, as the debug host interfaces directly with the SoC using a standard interface.",2007-10-25T12:07:17Z,http://arxiv.org/pdf/0710.4827v1,2024-04-28,
0710.4832v1,"SystemC Analysis of a New Dynamic Power Management Architecture","This paper presents a new dynamic power management architecture of a System on Chip. The Power State Machine describing the status of the core follows the recommendations of the ACPI standard. The algorithm controls the power states of each block on the basis of battery status, chip temperature and a user defined task priority.",2007-10-25T12:10:19Z,http://arxiv.org/pdf/0710.4832v1,2024-04-28,
0710.4833v1,"Exploiting Real-Time FPGA Based Adaptive Systems Technology for Real-Time Sensor Fusion in Next Generation Automotive Safety Systems","We present a system for the boresighting of sensors using inertial measurement devices as the basis for developing a range of dynamic real-time sensor fusion applications. The proof of concept utilizes a COTS FPGA platform for sensor fusion and real-time correction of a misaligned video sensor. We exploit a custom-designed 32-bit soft processor core and C-based design & synthesis for rapid, platform-neutral development. Kalman filter and sensor fusion techniques established in advanced aviation systems are applied to automotive vehicles with results exceeding typical industry requirements for sensor alignment. Results of the static and the dynamic tests demonstrate that using inexpensive accelerometers mounted on (or during assembly of) a sensor and an Inertial Measurement Unit (IMU) fixed to a vehicle can be used to compute the misalignment of the sensor to the IMU and thus vehicle. In some cases the model predications and test results exceeded the requirements by an order of magnitude with a 3-sigma or 99% confidence.",2007-10-25T12:11:11Z,http://arxiv.org/pdf/0710.4833v1,2024-04-28,
0710.4834v1,"Platform Based Design for Automotive Sensor Conditioning","In this paper a general architecture suitable to interface several kinds of sensors for automotive applications is presented. A platform based design approach is pursued to improve system performance while minimizing time-to-market.. The platform is composed by an analog front-end and a digital section. The latter is based on a microcontroller core (8051 IP by Oregano) plus a set of dedicated hardware dedicated to the complex signal processing required for sensor conditioning. The microcontroller handles also the communication with external devices (as a PC) for data output and fast prototyping. A case study is presented concerning the conditioning of a Gyro yaw rate sensor for automotive applications. Measured performance results outperform current state-of-the-art commercial devices.",2007-10-25T12:12:03Z,http://arxiv.org/pdf/0710.4834v1,2024-04-28,
0710.4838v1,"A 6bit, 1.2GSps Low-Power Flash-ADC in 0.13$μ$m Digital CMOS","A 6bit flash-ADC with 1.2GSps, wide analog bandwidth and low power, realized in a standard digital 0.13 $\mu$m CMOS copper technology is presented. Employing capacitive interpolation gives various advantages when designing for low power: no need for a reference resistor ladder, implicit sample-and-hold operation, no edge effects in the interpolation network (as compared to resistive interpolation), and a very low input capacitance of only 400fF, which leads to an easily drivable analog converter interface. Operating at 1.2GSps the ADC achieves an effective resolution bandwidth (ERBW) of 700MHz, while consuming 160mW of power. At 600MSps we achieve an ERBW of 600MHz with only 90mW power consumption, both from a 1.5V supply. This corresponds to outstanding Figure-of-Merit numbers (FoM) of 2.2 and 1.5pJ/convstep, respectively. The module area is 0.12mm^2.",2007-10-25T12:15:35Z,http://arxiv.org/pdf/0710.4838v1,2024-04-28,
0710.4839v1,"A 97mW 110MS/s 12b Pipeline ADC Implemented in 0.18$μ$m Digital CMOS","A 12 bit Pipeline ADC fabricated in a 0.18 $\mu$m pure digital CMOS technology is presented. Its nominal conversion rate is 110MS/s and the nominal supply voltage is 1.8V. The effective number of bits is 10.4 when a 10MHz input signal with 2V_{P-P} signal swing is applied. The occupied silicon area is 0.86mm^2 and the power consumption equals 97mW. A switched capacitor bias current circuit scale the bias current automatically with the conversion rate, which gives scaleable power consumption and full performance of the ADC from 20 to 140MS/s.",2007-10-25T12:16:26Z,http://arxiv.org/pdf/0710.4839v1,2024-04-28,
0710.4840v1,"Testing Logic Cores using a BIST P1500 Compliant Approach: A Case of Study","In this paper we describe how we applied a BIST-based approach to the test of a logic core to be included in System-on-a-chip (SoC) environments. The approach advantages are the ability to protect the core IP, the simple test interface (thanks also to the adoption of the P1500 standard), the possibility to run the test at-speed, the reduced test time, and the good diagnostic capabilities. The paper reports figures about the achieved fault coverage, the required area overhead, and the performance slowdown, and compares the figures with those for alternative approaches, such as those based on full scan and sequential ATPG.",2007-10-25T12:17:29Z,http://arxiv.org/pdf/0710.4840v1,2024-04-28,
0710.4842v1,"Using Mobilize Power Management IP for Dynamic & Static Power Reduction in SoC at 130 nm","At 130 nm and 90 nm, power consumption (both dynamic and static) has become a barrier in the roadmap for SoC designs targeting battery powered, mobile applications. This paper presents the results of dynamic and static power reduction achieved implementing Tensilica's 32-bit Xtensa microprocessor core, using Virtual Silicon's Power Management IP. Independent voltage islands are created using Virtual Silicon's VIP PowerSaver standard cells by using voltage level shifting cells and voltage isolation cells to implement power islands. The VIP PowerSaver standard cells are characterized at 1.2V, 1.0V and 0.8V, to accommodate voltage scaling. Power islands can also be turned off completely. Designers can significantly lower both the dynamic power and the quiescent or leakage power of their SoC designs, with very little impact on speed or area using Virtual Silicon's VIP Gate Bias standard cells.",2007-10-25T12:18:38Z,http://arxiv.org/pdf/0710.4842v1,2024-04-28,
0710.4843v1,"MultiNoC: A Multiprocessing System Enabled by a Network on Chip","The MultiNoC system implements a programmable on-chip multiprocessing platform built on top of an efficient, low area overhead intra-chip interconnection scheme. The employed interconnection structure is a Network on Chip, or NoC. NoCs are emerging as a viable alternative to increasing demands on interconnection architectures, due to the following characteristics: (i) energy efficiency and reliability; (ii) scalability of bandwidth, when compared to traditional bus architectures; (iii) reusability; (iv) distributed routing decisions. An external host computer feeds MultiNoC with application instructions and data. After this initialization procedure, MultiNoC executes some algorithm. After finishing execution of the algorithm, output data can be read back by the host. Sequential or parallel algorithms conveniently adapted to the MultiNoC structure can be executed. The main motivation to propose this design is to enable the investigation of current trends to increase the number of embedded processors in SoCs, leading to the concept of ""sea of processors"" systems.",2007-10-25T12:19:07Z,http://arxiv.org/pdf/0710.4843v1,2024-04-28,
0710.4844v1,"A Partitioning Methodology for Accelerating Applications in Hybrid Reconfigurable Platforms","In this paper, we propose a methodology for partitioning and mapping computational intensive applications in reconfigurable hardware blocks of different granularity. A generic hybrid reconfigurable architecture is considered so as the methodology can be applicable to a large number of heterogeneous reconfigurable platforms. The methodology mainly consists of two stages, the analysis and the mapping of the application onto fine and coarse-grain hardware resources. A prototype framework consisting of analysis, partitioning and mapping tools has been also developed. For the coarse-grain reconfigurable hardware, we use our previous-developed high-performance coarse-grain data-path. In this work, the methodology is validated using two real-world applications, an OFDM transmitter and a JPEG encoder. In the case of the OFDM transmitter, a maximum clock cycles decrease of 82% relative to the ones in an all fine-grain mapping solution is achieved. The corresponding performance improvement for the JPEG is 43%.",2007-10-25T12:20:27Z,http://arxiv.org/pdf/0710.4844v1,2024-04-28,
0710.4845v1,"Evaluation of SystemC Modelling of Reconfigurable Embedded Systems","This paper evaluates the use of pin and cycle accurate SystemC models for embedded system design exploration and early software development. The target system is MicroBlaze VanillaNet Platform running MicroBlaze uClinux operating system. The paper compares Register Transfer Level (RTL) Hardware Description Language (HDL) simulation speed to the simulation speed of several different SystemC models. It is shown that simulation speed of pin and cycle accurate models can go up to 150 kHz, compared to 100 Hz range of HDL simulation. Furthermore, utilising techniques that temporarily compromise cycle accuracy, effective simulation speed of up to 500 kHz can be obtained.",2007-10-25T12:21:19Z,http://arxiv.org/pdf/0710.4845v1,2024-04-28,
0710.4850v1,"Hardware Support for QoS-based Function Allocation in Reconfigurable Systems","This contribution presents a new approach for allocating suitable function-implementation variants depending on given quality-of-service function-requirements for run-time reconfigurable multi-device systems. Our approach adapts methodologies from the domain of knowledge-based systems which can be used for doing run-time hardware/software resource usage optimizations.",2007-10-25T12:24:59Z,http://arxiv.org/pdf/0710.4850v1,2024-04-28,
0711.0838v2,"On the operating unit size of load/store architectures","We introduce a strict version of the concept of a load/store instruction set architecture in the setting of Maurer machines. We take the view that transformations on the states of a Maurer machine are achieved by applying threads as considered in thread algebra to the Maurer machine. We study how the transformations on the states of the main memory of a strict load/store instruction set architecture that can be achieved by applying threads depend on the operating unit size, the cardinality of the instruction set, and the maximal number of states of the threads.",2007-11-06T11:16:34Z,http://arxiv.org/pdf/0711.0838v2,2024-04-28,
0711.2383v1,"Decoding the Golden Code: a VLSI design","The recently proposed Golden code is an optimal space-time block code for 2 X 2 multiple-input multiple-output (MIMO) systems. The aim of this work is the design of a VLSI decoder for a MIMO system coded with the Golden code. The architecture is based on a rearrangement of the sphere decoding algorithm that achieves maximum-likelihood (ML) decoding performance. Compared to other approaces, the proposed solution exhibits an inherent flexibility in terms of modulation schemes QAM modulation size and this makes our architecture particularly suitable for adaptive modulation schemes.",2007-11-15T11:55:30Z,http://arxiv.org/pdf/0711.2383v1,2024-04-28,
0711.2671v1,"Combined Integer and Variable Precision (CIVP) Floating Point Multiplication Architecture for FPGAs","In this paper, we propose an architecture/methodology for making FPGAs suitable for integer as well as variable precision floating point multiplication. The proposed work will of great importance in applications which requires variable precision floating point multiplication such as multi-media processing applications. In the proposed architecture/methodology, we propose the replacement of existing 18x18 bit and 25x18 bit dedicated multipliers in FPGAs with dedicated 24x24 bit and 24x9 bit multipliers, respectively. We have proved that our approach of providing the dedicated 24x24 bit and 24x9 bit multipliers in FPGAs will make them efficient for performing integer as well as single precision, double precision, and Quadruple precision floating point multiplications.",2007-11-16T20:18:32Z,http://arxiv.org/pdf/0711.2671v1,2024-04-28,
0711.2674v1,"Partial Reversible Gates(PRG) for Reversible BCD Arithmetic","IEEE 754r is the ongoing revision to the IEEE 754 floating point standard and a major enhancement to the standard is the addition of decimal format. Furthermore, in the recent years reversible logic has emerged as a promising computing paradigm having its applications in low power CMOS, quantum computing, nanotechnology, and optical computing. The major goal in reversible logic is to minimize the number of reversible gates and garbage outputs. Thus, this paper proposes the novel concept of partial reversible gates that will satisfy the reversibility criteria for specific cases in BCD arithmetic. The partial reversible gate is proposed to minimize the number of reversible gates and garbage outputs, while designing the reversible BCD arithmetic circuits.",2007-11-16T20:25:20Z,http://arxiv.org/pdf/0711.2674v1,2024-04-28,
0802.3441v1,"Efficient implementation of GALS systems over commercial synchronous FPGAs: a new approach","The new vision presented is aimed to overcome the logic overhead issues that previous works exhibit when applying GALS techniques to programmable logic devices. The proposed new view relies in a 2-phase, bundled data parity based protocol for data transfer and clock generation tasks. The ability of the introduced methodology for smart real-time delay selection allows the implementation of a variety of new methodologies for electromagnetic interference mitigation and device environment changes adaptation.",2008-02-23T13:11:13Z,http://arxiv.org/pdf/0802.3441v1,2024-04-28,
0807.1765v1,"Archer: A Community Distributed Computing Infrastructure for Computer Architecture Research and Education","This paper introduces Archer, a community-based computing resource for computer architecture research and education. The Archer infrastructure integrates virtualization and batch scheduling middleware to deliver high-throughput computing resources aggregated from resources distributed across wide-area networks and owned by different participating entities in a seamless manner. The paper discusses the motivations leading to the design of Archer, describes its core middleware components, and presents an analysis of the functionality and performance of a prototype wide-area deployment running a representative computer architecture simulation workload.",2008-07-11T02:47:55Z,http://arxiv.org/pdf/0807.1765v1,2024-04-28,
0807.3732v1,"An adaptive embedded architecture for real-time Particle Image Velocimetry algorithms","Particle Image Velocimetry (PIV) is a method of im-aging and analysing fields of flows. The PIV tech-niques compute and display all the motion vectors of the field in a resulting image. Speeds more than thou-sand vectors per second can be required, each speed being environment-dependent. Essence of this work is to propose an adaptive FPGA-based system for real-time PIV algorithms. The proposed structure is ge-neric so that this unique structure can be re-used for any PIV applications that uses the cross-correlation technique. The major structure remains unchanged, adaptations only concern the number of processing operations. The required speed (corresponding to the number of vector per second) is obtained thanks to a parallel processing strategy. The image processing designer duplicates the processing modules to distrib-ute the operations. The result is a FPGA-based archi-tecture, which is easily adapted to algorithm specifica-tions without any hardware requirement. The design flow is fast and reliable.",2008-07-23T19:22:06Z,http://arxiv.org/pdf/0807.3732v1,2024-04-28,
0808.2584v2,"On Transformations of Load-Store Maurer Instruction Set Architecture","In this paper, we study how certain conditions can affect the transformations on the states of the memory of a strict load-store Maurer ISA, when half of the data memory serves as the part of the operating unit.",2008-08-19T12:31:07Z,http://arxiv.org/pdf/0808.2584v2,2024-04-28,
0808.2602v2,"Easily testable logical networks based on a 'widened long flip-flop'","The article describes an attempt to solve at once three basic problems arising at testing a complex digital equipment for defects: 1) the problem of an exponential increasing of the complexity of testing the equipment with the complexity of the equipment; 2) the problem of testing of the tester; 3) the problem of a mutual masking of defects. The proposed solution is nothing more than using certain limitations for connections between usual logical gates. Arbitrary multiple stuck-at-faults are supposed as defects.",2008-08-19T14:44:05Z,http://arxiv.org/pdf/0808.2602v2,2024-04-28,
0812.3871v1,"Decting Errors in Reversible Circuits With Invariant Relationships","Reversible logic is experience renewed interest as we are approach the limits of CMOS technologies. While physical implementations of reversible gates have yet to materialize, it is safe to assume that they will rely on faulty individual components. In this work we present a present a method to provide fault tolerance to a reversible circuit based on invariant relationships.",2008-12-19T18:14:54Z,http://arxiv.org/pdf/0812.3871v1,2024-04-28,
0901.4081v1,"Adaptive FPGA NoC-based Architecture for Multispectral Image Correlation","An adaptive FPGA architecture based on the NoC (Network-on-Chip) approach is used for the multispectral image correlation. This architecture must contain several distance algorithms depending on the characteristics of spectral images and the precision of the authentication. The analysis of distance algorithms is required which bases on the algorithmic complexity, result precision, execution time and the adaptability of the implementation. This paper presents the comparison of these distance computation algorithms on one spectral database. The result of a RGB algorithm implementation was discussed.",2009-01-26T19:54:27Z,http://arxiv.org/pdf/0901.4081v1,2024-04-28,
0906.3832v1,"Hardware Trojan by Hot Carrier Injection","This paper discusses how hot carrier injection (HCI) can be exploited to create a trojan that will cause hardware failures. The trojan is produced not via additional logic circuitry but by controlled scenarios that maximize and accelerate the HCI effect in transistors. These scenarios range from manipulating the manufacturing process to varying the internal voltage distribution. This new type of trojan is difficult to test due to its gradual hardware degradation mechanism. This paper describes the HCI effect, detection techniques and discusses the possibility for maliciously induced HCI trojans.",2009-06-20T22:56:19Z,http://arxiv.org/pdf/0906.3832v1,2024-04-28,
0906.3834v1,"Exploiting Semiconductor Properties for Hardware Trojans","This paper discusses the possible introduction of hidden reliability defects during CMOS foundry fabrication processes that may lead to accelerated wearout of the devices. These hidden defects or hardware Trojans can be created by deviation from foundry design rules and processing parameters. The Trojans are produced by exploiting time-based wearing mechanisms (HCI, NBTI, TDDB and EM) and/or condition-based triggers (ESD, Latchup and Softerror). This class of latent damage is difficult to test due to its gradual degradation nature. The paper describes life-time expectancy results for various Trojan induced scenarios. Semiconductor properties, processing and design parameters critical for device reliability and Trojan creation are discussed.",2009-06-20T23:12:45Z,http://arxiv.org/pdf/0906.3834v1,2024-04-28,
0909.0099v1,"Hardware Virtualization Support In INTEL, AMD And IBM Power Processors","At present, the mostly used and developed mechanism is hardware virtualization which provides a common platform to run multiple operating systems and applications in independent partitions. More precisely, it is all about resource virtualization as the term hardware virtualization is emphasized. In this paper, the aim is to find out the advantages and limitations of current virtualization techniques, analyze their cost and performance and also depict which forthcoming hardware virtualization techniques will able to provide efficient solutions for multiprocessor operating systems. This is done by making a methodical literature survey and statistical analysis of the benchmark reports provided by SPEC (Standard Performance Evaluation Corporation) and TPC (Transaction processing Performance Council). Finally, this paper presents the current aspects of hardware virtualization which will help the IT managers of the large organizations to take effective decision while choosing server with virtualization support. Again, the future works described in section 4 of this paper focuses on some real world challenges such as abstraction of multiple servers, language level virtualization, pre-virtualization etc. which may be point of great interest for the researchers.",2009-09-01T06:15:22Z,http://arxiv.org/pdf/0909.0099v1,2024-04-28,
0909.1876v1,"Turbo NOC: a framework for the design of Network On Chip based turbo decoder architectures","This work proposes a general framework for the design and simulation of network on chip based turbo decoder architectures. Several parameters in the design space are investigated, namely the network topology, the parallelism degree, the rate at which messages are sent by processing nodes over the network and the routing strategy. The main results of this analysis are: i) the most suited topologies to achieve high throughput with a limited complexity overhead are generalized de-Bruijn and generalized Kautz topologies; ii) depending on the throughput requirements different parallelism degrees, message injection rates and routing algorithms can be used to minimize the network area overhead.",2009-09-10T07:29:52Z,http://arxiv.org/pdf/0909.1876v1,2024-04-28,
0910.3427v4,"A Scalable VLSI Architecture for Soft-Input Soft-Output Depth-First Sphere Decoding","Multiple-input multiple-output (MIMO) wireless transmission imposes huge challenges on the design of efficient hardware architectures for iterative receivers. A major challenge is soft-input soft-output (SISO) MIMO demapping, often approached by sphere decoding (SD). In this paper, we introduce the - to our best knowledge - first VLSI architecture for SISO SD applying a single tree-search approach. Compared with a soft-output-only base architecture similar to the one proposed by Studer et al. in IEEE J-SAC 2008, the architectural modifications for soft input still allow a one-node-per-cycle execution. For a 4x4 16-QAM system, the area increases by 57% and the operating frequency degrades by 34% only.",2009-10-18T21:51:55Z,http://arxiv.org/pdf/0910.3427v4,2024-04-28,
0910.3736v1,"A Fault-tolerant Structure for Reliable Multi-core Systems Based on Hardware-Software Co-design","To cope with the soft errors and make full use of the multi-core system, this paper gives an efficient fault-tolerant hardware and software co-designed architecture for multi-core systems. And with a not large number of test patterns, it will use less than 33% hardware resources compared with the traditional hardware redundancy (TMR) and it will take less than 50% time compared with the traditional software redundancy (time redundant).Therefore, it will be a good choice for the fault-tolerant architecture for the future high-reliable multi-core systems.",2009-10-20T04:01:51Z,http://arxiv.org/pdf/0910.3736v1,2024-04-28,
1001.3716v1,"A Multicore Processor based Real-Time System for Automobile management application","In this paper we propose an Intelligent Management System which is capable of managing the automobile functions using the rigorous real-time principles and a multicore processor in order to realize higher efficiency and safety for the vehicle. It depicts how various automobile functionalities can be fine grained and treated to fit in real time concepts. It also shows how the modern multicore processors can be of good use in organizing vast amounts of correlated functions to be executed in real-time with excellent time commitments. The modeling of the automobile tasks with real time commitments, organizing appropriate scheduling for various real time tasks and the usage of a multicore processor enables the system to realize higher efficiency and offer better safety levels to the vehicle. The industry available real time operating system is used for scheduling various tasks and jobs on the multicore processor.",2010-01-21T04:34:07Z,http://arxiv.org/pdf/1001.3716v1,2024-04-28,
1001.3781v1,"An Architectural Approach for Decoding and Distributing Functions in FPUs in a Functional Processor System","The main goal of this research is to develop the concepts of a revolutionary processor system called Functional Processor System. The fairly novel work carried out in this proposal concentrates on decoding of function pipelines and distributing it in FPUs as a part of scheduling approach. As the functional programs are super-level programs that entails requirements only at functional level, decoding of functions and distribution of functions in the heterogeneous functional processor units are a challenge. We explored the possibilities of segregation of the functions from the application program and distributing the functions on the relevant FPUs by using address mapping techniques. Here we pursue the perception of feeding the functions into the processor farm rather than the processor fetching the instructions or functions and executing it. This work is carried out at theoretical levels and it requires a long way to go in the realization of this work in hardware perhaps with a large industrial team with a pragmatic time frame.",2010-01-21T11:31:11Z,http://arxiv.org/pdf/1001.3781v1,2024-04-28,
1001.4694v1,"VLSI Architectures for WIMAX Channel Decoders","This chapter describes the main architectures proposed in the literature to implement the channel decoders required by the WiMax standard, namely convolutional codes, turbo codes (both block and convolutional) and LDPC. Then it shows a complete design of a convolutional turbo code encoder/decoder system for WiMax.",2010-01-26T14:11:18Z,http://arxiv.org/pdf/1001.4694v1,2024-04-28,
1002.1881v1,"Evaluation and Design Space Exploration of a Time-Division Multiplexed NoC on FPGA for Image Analysis Applications","The aim of this paper is to present an adaptable Fat Tree NoC architecture for Field Programmable Gate Array (FPGA) designed for image analysis applications. Traditional NoCs (Network on Chip) are not optimal for dataflow applications with large amount of data. On the opposite, point to point communications are designed from the algorithm requirements but they are expensives in terms of resource and wire. We propose a dedicated communication architecture for image analysis algorithms. This communication mechanism is a generic NoC infrastructure dedicated to dataflow image processing applications, mixing circuit-switching and packet-switching communications. The complete architecture integrates two dedicated communication architectures and reusable IP blocks. Communications are based on the NoC concept to support the high bandwidth required for a large number and type of data.",2010-02-09T15:21:20Z,http://arxiv.org/pdf/1002.1881v1,2024-04-28,
1007.4465v1,"FPGA Implementation of a Reconfigurable Viterbi Decoder for WiMAX Receiver","Field Programmable Gate Array technology (FPGA) is a highly configurable option for implementing many sophisticated signal processing tasks in Software Defined Radios (SDRs). Those types of radios are realized using highly configurable hardware platforms. Convolutional codes are used in every robust digital communication system and Viterbi algorithm is employed in wireless communications to decode the convolutional codes. Such decoders are complex and dissipate large amount of power. In this paper, a low power-reconfigurable Viterbi decoder for WiMAX receiver is described using a VHDL code for FPGA implementation. The proposed design is implemented on Xilinx Virtex-II Pro, XC2vpx30 FPGA using the FPGA Advantage Pro package provided by Mentor Graphics and ISE 10.1 by Xilinx.",2010-07-26T14:06:59Z,http://arxiv.org/pdf/1007.4465v1,2024-04-28,
1008.2729v1,"Asynchronous logic circuits and sheaf obstructions","This article exhibits a particular encoding of logic circuits into a sheaf formalism. The central result of this article is that there exists strictly more information available to a circuit designer in this setting than exists in static truth tables, but less than exists in event-level simulation. This information is related to the timing behavior of the logic circuits, and thereby provides a ``bridge'' between static logic analysis and detailed simulation.",2010-08-16T18:14:06Z,http://arxiv.org/pdf/1008.2729v1,2024-04-28,
1008.3288v1,"Reversible Logic Synthesis of Fault Tolerant Carry Skip BCD Adder","Reversible logic is emerging as an important research area having its application in diverse fields such as low power CMOS design, digital signal processing, cryptography, quantum computing and optical information processing. This paper presents a new 4*4 parity preserving reversible logic gate, IG. The proposed parity preserving reversible gate can be used to synthesize any arbitrary Boolean function. It allows any fault that affects no more than a single signal readily detectable at the circuit's primary outputs. It is shown that a fault tolerant reversible full adder circuit can be realized using only two IGs. The proposed fault tolerant full adder (FTFA) is used to design other arithmetic logic circuits for which it is used as the fundamental building block. It has also been demonstrated that the proposed design offers less hardware complexity and is efficient in terms of gate count, garbage outputs and constant inputs than the existing counterparts.",2010-08-19T12:25:44Z,http://arxiv.org/pdf/1008.3288v1,2024-04-28,
1008.3311v1,"Fault tolerant reversible logic synthesis: Carry look-ahead and carry-skip adders","Irreversible logic circuits dissipate heat for every bit of information that is lost. Information is lost when the input vector cannot be recovered from its corresponding output vector. Reversible logic circuit naturally takes care of heating because it implements only the functions that have one-to-one mapping between its input and output vectors. Therefore reversible logic design becomes one of the promising research directions in low power dissipating circuit design in the past few years and has found its application in low power CMOS design, digital signal processing and nanotechnology. This paper presents the efficient approaches for designing reversible fast adders that implement carry look-ahead and carry-skip logic. The proposed 16-bit high speed reversible adder will include IG gates for the realization of its basic building block. The IG gate is universal in the sense that it can be used to synthesize any arbitrary Boolean-functions. The IG gate is parity preserving, that is, the parity of the inputs matches the parity of the outputs. It allows any fault that affects no more than a single signal readily detectable at the circuit's primary outputs. Therefore, the proposed high speed adders will have the inherent opportunity of detecting errors in its output side. It has also been demonstrated that the proposed design offers less hardware complexity and is efficient in terms of gate count, garbage outputs and constant inputs than the existing counterparts.",2010-08-19T14:35:12Z,http://arxiv.org/pdf/1008.3311v1,2024-04-28,
1008.3340v1,"Synthesis of Fault Tolerant Reversible Logic Circuits","Reversible logic is emerging as an important research area having its application in diverse fields such as low power CMOS design, digital signal processing, cryptography, quantum computing and optical information processing. This paper presents a new 4*4 universal reversible logic gate, IG. It is a parity preserving reversible logic gate, that is, the parity of the inputs matches the parity of the outputs. The proposed parity preserving reversible gate can be used to synthesize any arbitrary Boolean function. It allows any fault that affects no more than a single signal readily detectable at the circuit's primary outputs. Finally, it is shown how a fault tolerant reversible full adder circuit can be realized using only two IGs. It has also been demonstrated that the proposed design offers less hardware complexity and is efficient in terms of gate count, garbage outputs and constant inputs than the existing counterparts.",2010-08-19T16:12:11Z,http://arxiv.org/pdf/1008.3340v1,2024-04-28,
1008.3344v1,"Efficient Approaches for Designing Fault Tolerant Reversible Carry Look-Ahead and Carry-Skip Adders","Combinational or Classical logic circuits dissipate heat for every bit of information that is lost. Information is lost when the input vector cannot be recovered from its corresponding output vector. Reversible logic circuit implements only the functions having one-to-one mapping between its input and output vectors and therefore naturally takes care of heating. Reversible logic design becomes one of the promising research directions in low power dissipating circuit design in the past few years and has found its application in low power CMOS design, digital signal processing and nanotechnology. This paper presents the efficient approaches for designing fault tolerant reversible fast adders that implement carry look-ahead and carry-skip logic. The proposed high speed reversible adders include MIG gates for the realization of its basic building block. The MIG gate is universal and parity preserving. It allows any fault that affects no more than a single signal readily detectable at the circuit's primary outputs. It has also been demonstrated that the proposed design offers less hardware complexity and is efficient in terms of gate count, garbage outputs and constant inputs than the existing counterparts.",2010-08-19T16:27:28Z,http://arxiv.org/pdf/1008.3344v1,2024-04-28,
1008.3352v1,"Variable Block Carry Skip Logic using Reversible Gates","Reversible circuits have applications in digital signal processing, computer graphics, quantum computation and cryptography. In this paper, a generalized k*k reversible gate family is proposed and a 3*3 gate of the family is discussed. Inverter, AND, OR, NAND, NOR, and EXOR gates can be realized by this gate. Implementation of a full-adder circuit using two such 3*3 gates is given. This full-adder circuit contains only two reversible gates and produces no extra garbage outputs. The proposed full-adder circuit is efficient in terms of gate count, garbage outputs and quantum cost. A 4-bit carry skip adder is designed using this full-adder circuit and a variable block carry skip adder is discussed. Necessary equations required to evaluate these adder are presented.",2010-08-19T17:07:51Z,http://arxiv.org/pdf/1008.3352v1,2024-04-28,
1008.3357v1,"Building Toffoli Network for Reversible Logic Synthesis Based on Swapping Bit Strings","In this paper, we have implemented and designed a sorting network for reversible logic circuits synthesis in terms of n*n Toffoli gates. The algorithm presented in this paper constructs a Toffoli Network based on swapping bit strings. Reduction rules are then applied by simple template matching and removing useless gates from the network. Random selection of bit strings and reduction of control inputs are used to minimize both the number of gates and gate width. The method produces near optimal results for up to 3-input 3-output circuits.",2010-08-19T17:21:54Z,http://arxiv.org/pdf/1008.3357v1,2024-04-28,
1008.3452v2,"Memristor-based Circuits for Performing Basic Arithmetic Operations","In almost all of the currently working circuits, especially in analog circuits implementing signal processing applications, basic arithmetic operations such as multiplication, addition, subtraction and division are performed on values which are represented by voltages or currents. However, in this paper, we propose a new and simple method for performing analog arithmetic operations which in this scheme, signals are represented and stored through a memristance of the newly found circuit element, i.e. memristor, instead of voltage or current. Some of these operators such as divider and multiplier are much simpler and faster than their equivalent voltage-based circuits and they require less chip area. In addition, a new circuit is designed for programming the memristance of the memristor with predetermined analog value. Presented simulation results demonstrate the effectiveness and the accuracy of the proposed circuits.",2010-08-20T07:53:24Z,http://arxiv.org/pdf/1008.3452v2,2024-04-28,
1008.3533v1,"A Novel Quantum Cost Efficient Reversible Full Adder Gate in Nanotechnology","Reversible logic has become one of the promising research directions in low power dissipating circuit design in the past few years and has found its applications in low power CMOS design, cryptography, optical information processing and nanotechnology. This paper presents a novel and quantum cost efficient reversible full adder gate in nanotechnology. This gate can work singly as a reversible full adder unit and requires only one clock cycle. The proposed gate is a universal gate in the sense that it can be used to synthesize any arbitrary Boolean functions. It has been demonstrated that the hardware complexity offered by the proposed gate is less than the existing counterparts. The proposed reversible full adder gate also adheres to the theoretical minimum established by the researchers.",2010-08-20T16:23:54Z,http://arxiv.org/pdf/1008.3533v1,2024-04-28,
1008.3694v1,"Sorting Network for Reversible Logic Synthesis","In this paper, we have introduced an algorithm to implement a sorting network for reversible logic synthesis based on swapping bit strings. The algorithm first constructs a network in terms of n*n Toffoli gates read from left to right. The number of gates in the circuit produced by our algorithm is then reduced by template matching and removing useless gates from the network. We have also compared the efficiency of the proposed method with the existing ones.",2010-08-22T11:13:34Z,http://arxiv.org/pdf/1008.3694v1,2024-04-28,
1008.4668v1,"BSSSN: Bit String Swapping Sorting Network for Reversible Logic Synthesis","In this paper, we have introduced the notion of UselessGate and ReverseOperation. We have also given an algorithm to implement a sorting network for reversible logic synthesis based on swapping bit strings. The network is constructed in terms of n*n Toffoli Gates read from left to right and it has shown that there will be no more gates than the number of swappings the algorithm requires. The gate complexity of the network is O(n2). The number of gates in the network can be further reduced by template reduction technique and removing UselessGate from the network.",2010-08-27T09:05:49Z,http://arxiv.org/pdf/1008.4668v1,2024-04-28,
1009.1796v1,"Power optimized programmable embedded controller","Now a days, power has become a primary consideration in hardware design, and is critical in computer systems especially for portable devices with high performance and more functionality. Clock-gating is the most common technique used for reducing processor's power. In this work clock gating technique is applied to optimize the power of fully programmable Embedded Controller (PEC) employing RISC architecture. The CPU designed supports i) smart instruction set, ii) I/O port, UART iii) on-chip clocking to provide a range of frequencies , iv) RISC as well as controller concepts. The whole design is captured using VHDL and is implemented on FPGA chip using Xilinx .The architecture and clock gating technique together is found to reduce the power consumption by 33.33% of total power consumed by this chip.",2010-09-09T14:41:27Z,http://arxiv.org/pdf/1009.1796v1,2024-04-28,
1009.2622v1,"On the Design and Analysis of Quaternary Serial and Parallel Adders","Optimization techniques for decreasing the time and area of adder circuits have been extensively studied for years mostly in binary logic system. In this paper, we provide the necessary equations required to design a full adder in quaternary logic system. We develop the equations for single-stage parallel adder which works as a carry look-ahead adder. We also provide the design of a logarithmic stage parallel adder which can compute the carries within log2(n) time delay for n qudits. At last, we compare the designs and finally propose a hybrid adder which combines the advantages of serial and parallel adder.",2010-09-14T10:48:56Z,http://arxiv.org/pdf/1009.2622v1,2024-04-28,
1009.3819v1,"Fault Tolerant Variable Block Carry Skip Logic (VBCSL) using Parity Preserving Reversible Gates","Reversible logic design has become one of the promising research directions in low power dissipating circuit design in the past few years and has found its application in low power CMOS design, digital signal processing and nanotechnology. This paper presents the efficient design approaches of fault tolerant carry skip adders (FTCSAs) and compares those designs with the existing ones. Variable block carry skip logic (VBCSL) using the fault tolerant full adders (FTFAs) has also been developed. The designs are minimized in terms of hardware complexity, gate count, constant inputs and garbage outputs. Besides of it, technology independent evaluation of the proposed designs clearly demonstrates its superiority with the existing counterparts.",2010-09-20T13:53:39Z,http://arxiv.org/pdf/1009.3819v1,2024-04-28,
1009.4590v1,"A Unique 10 Segment Display for Bengali Numerals","Segmented display is widely used for efficient display of alphanumeric characters. English numerals are displayed by 7 segment and 16 segment display. The segment size is uniform in this two display architecture. Display architecture using 8, 10, 11, 18 segments have been proposed for Bengali numerals 0...9 yet no display architecture is designed using segments of uniform size and uniform power consumption. In this paper we have proposed a uniform 10 segment architecture for Bengali numerals. This segment architecture uses segments of uniform size and no bent segment is used.",2010-09-23T12:01:38Z,http://arxiv.org/pdf/1009.4590v1,2024-04-28,
1009.4977v1,"Universal Numeric Segmented Display","Segmentation display plays a vital role to display numerals. But in today's world matrix display is also used in displaying numerals. Because numerals has lots of curve edges which is better supported by matrix display. But as matrix display is costly and complex to implement and also needs more memory, segment display is generally used to display numerals. But as there is yet no proposed compact display architecture to display multiple language numerals at a time, this paper proposes uniform display architecture to display multiple language digits and general mathematical expressions with higher accuracy and simplicity by using a 18-segment display, which is an improvement over the 16 segment display.",2010-09-25T06:17:02Z,http://arxiv.org/pdf/1009.4977v1,2024-04-28,
1009.6132v1,"Multi-standard programmable baseband modulator for next generation wireless communication","Considerable research has taken place in recent times in the area of parameterization of software defined radio (SDR) architecture. Parameterization decreases the size of the software to be downloaded and also limits the hardware reconfiguration time. The present paper is based on the design and development of a programmable baseband modulator that perform the QPSK modulation schemes and as well as its other three commonly used variants to satisfy the requirement of several established 2G and 3G wireless communication standards. The proposed design has been shown to be capable of operating at a maximum data rate of 77 Mbps on Xilinx Virtex 2-Pro University field programmable gate array (FPGA) board. The pulse shaping root raised cosine (RRC) filter has been implemented using distributed arithmetic (DA) technique in the present work in order to reduce the computational complexity, and to achieve appropriate power reduction and enhanced throughput. The designed multiplier-less programmable 32-tap FIR-based RRC filter has been found to withstand a peak inter-symbol interference (ISI) distortion of -41 dBs",2010-09-09T04:43:52Z,http://arxiv.org/pdf/1009.6132v1,2024-04-28,
1011.4157v2,"A full-custom ASIC design of a 8-bit, 25 MHz, Pipeline ADC using 0.35 um CMOS technology","The purpose of this project was to design and implement a pipeline Analog-to-Digital Converter using 0.35um CMOS technology. Initial requirements of a 25-MHz conversion rate and 8-bits of resolution where the only given ones. Although additional secondary goals such as low power consumption and small area were stated. The architecture is based on a 1.5 bit per stage structure utilizing digital correction for each stage [12]. A differential switched capacitor circuit consisting of a cascade gm-C op-amp with 200MHz ft is used for sampling and amplification in each stage [12]. Differential dynamic comparators are used to implement the decision levels required for the 1.5-b per stage structure. Correction of the pipeline is accomplished by using digital correction circuit consist of D-latches and full-adders. Area and Power consumption of whole design was 0.24mm2 and 35mW respectively. The maximum sample rate at which the converter gave an adequate output was 33MHz.",2010-11-18T09:12:50Z,http://arxiv.org/pdf/1011.4157v2,2024-04-28,
1101.3698v1,"Systolic Arrays for Lattice-Reduction-Aided MIMO Detection","Multiple-input, multiple-output (MIMO) technology provides high data rate and enhanced QoS for wireless com- munications. Since the benefits from MIMO result in a heavy computational load in detectors, the design of low-complexity sub-optimum receivers is currently an active area of research. Lattice-reduction-aided detection (LRAD) has been shown to be an effective low-complexity method with near-ML performance. In this paper we advocate the use of systolic array architectures for MIMO receivers, and in particular we exhibit one of them based on LRAD. The ""LLL lattice reduction algorithm"" and the ensuing linear detections or successive spatial-interference cancellations can be located in the same array, which is con- siderably hardware-efficient. Since the conventional form of the LLL algorithm is not immediately suitable for parallel processing, two modified LLL algorithms are considered here for the systolic array. LLL algorithm with full-size reduction (FSR-LLL) is one of the versions more suitable for parallel processing. Another variant is the all-swap lattice-reduction (ASLR) algorithm for complex-valued lattices, which processes all lattice basis vectors simultaneously within one iteration. Our novel systolic array can operate both algorithms with different external logic controls. In order to simplify the systolic array design, we replace the Lov\'asz condition in the definition of LLL-reduced lattice with the looser Siegel condition. Simulation results show that for LR- aided linear detections, the bit-error-rate performance is still maintained with this relaxation. Comparisons between the two algorithms in terms of bit-error-rate performance, and average FPGA processing time in the systolic array are made, which shows that ASLR is a better choice for a systolic architecture, especially for systems with a large number of antennas.",2011-01-17T18:09:52Z,http://arxiv.org/pdf/1101.3698v1,2024-04-28,
1101.5364v1,"RISC and CISC","Comparison of RISC & CISC in details, encompassing the addressing modes, evolution, definitions and characteristics. Pre - RISC design is also elaborated. Both the architectures are explained with the help of example. Analysis is made based on performance.",2011-01-27T19:20:42Z,http://arxiv.org/pdf/1101.5364v1,2024-04-28,
1104.0298v1,"High Speed Multiple Valued Logic Full Adder Using Carbon Nano Tube Field Effect Transistor","High speed Full-Adder (FA) module is a critical element in designing high performance arithmetic circuits. In this paper, we propose a new high speed multiple-valued logic FA module. The proposed FA is constructed by 14 transistors and 3 capacitors, using carbon nano-tube field effect transistor (CNFET) technology. Furthermore, our proposed technique has been examined in different voltages (i.e., 0.65v and 0.9v). The observed results reveal power consumption and power delay product (PDP) improvements compared to existing FA counterparts",2011-04-02T07:14:38Z,http://arxiv.org/pdf/1104.0298v1,2024-04-28,
1104.3310v1,"Computer Arithmetic Preserving Hamming Distance of Operands in Operation Result","The traditional approach to fault tolerant computing involves replicating computation units and applying a majority vote operation on individual result bits. This approach, however, has several limitations; the most severe is the resource requirement. This paper presents a new method for fault tolerant computing where for a given error rate, the hamming distance between correct inputs and faulty inputs as well as the hamming distance between a correct result and a faulty result is preserved throughout processing thereby enabling correction of up to transient faults per computation cycle. The new method is compared and contrasted with current protection methods and its cost / performance is analyzed.",2011-04-17T12:15:42Z,http://arxiv.org/pdf/1104.3310v1,2024-04-28,
1105.1014v1,"Improving Network-on-Chip-based turbo decoder architectures","In this work novel results concerning Network-on-Chip-based turbo decoder architectures are presented. Stemming from previous publications, this work concentrates first on improving the throughput by exploiting adaptive-bandwidth reduction techniques. This technique shows in the best case an improvement of more than 60 Mb/s. Moreover, it is known that double-binary turbo decoders require higher area than binary ones. This characteristic has the negative effect of increasing the data width of the network nodes. Thus, the second contribution of this work is to reduce the network complexity to support doublebinary codes, by exploiting bit-level and pseudo-floating-point representation of the extrinsic information. These two techniques allow for an area reduction of up to more than the 40% with a performance degradation of about 0.2 dB.",2011-05-05T08:41:43Z,http://arxiv.org/pdf/1105.1014v1,2024-04-28,
1105.1967v1,"Algebra-Logical Repair Method for FPGA Logic Blocks","An algebra-logical repair method for FPGA functional logic blocks on the basis of solving the coverage problem is proposed. It is focused on implementation into Infrastructure IP for system-on-a chip and system-in-package. A method is designed for providing the operability of FPGA blocks and digital system as a whole. It enables to obtain exact and optimal solution associated with the minimum number of spares needed to repair the FPGA logic components with multiple faults.",2011-05-10T15:03:05Z,http://arxiv.org/pdf/1105.1967v1,2024-04-28,
1105.1973v1,"Brain-like infrastructure for embedded SoC diagnosis","This article describes high-speed multiprocessor architecture for the concurrent analyzing information represented in analytic, graph- and table forms of associative relations to search, recognize and make a decision in n-dimensional vector discrete space. Vector-logical process models of actual applications,for which the quality of solution is estimated by the proposed integral non-arithmetical metric of the interaction between Boolean vectors, are described.",2011-05-10T15:24:49Z,http://arxiv.org/pdf/1105.1973v1,2024-04-28,
1105.2624v1,"A Flexible LDPC code decoder with a Network on Chip as underlying interconnect architecture","LDPC (Low Density Parity Check) codes are among the most powerful and widely adopted modern error correcting codes. The iterative decoding algorithms required for these codes involve high computational complexity and high processing throughput is achieved by allocating a sufficient number of processing elements (PEs). Supporting multiple heterogeneous LDPC codes on a parallel decoder poses serious problems in the design of the interconnect structure for such PEs. The aim of this work is to explore the feasibility of NoC (Network on Chip) based decoders, where full flexibility in terms of supported LDPC codes is obtained resorting to an NoC to connect PEs. NoC based LDPC decoders have been previously considered unfeasible because of the cost overhead associated to packet management and routing. On the contrary, the designed NoC adopts a low complexity routing, which introduces a very limited cost overhead with respect to architectures dedicated to specific classes of codes. Moreover the paper proposes an efficient configuration technique, which allows for fast on--the--fly switching among different codes. The decoder architecture is scalable and VLSI synthesis results are presented for several cases of study, including the whole set of WiMAX LDPC codes, WiFi codes and DVB-S2 standard.",2011-05-13T07:17:16Z,http://arxiv.org/pdf/1105.2624v1,2024-04-28,
1105.2960v1,"Multi-Amdahl: Optimal Resource Sharing with Multiple Program Execution Segments","This paper presents Multi-Amdahl, a resource allocation analytical tool for heterogeneous systems. Our model includes multiple program execution segments, where each one is accelerated by a specific hardware unit. The acceleration speedup of the specific hardware unit is a function of a limited resource, such as the unit area, power, or energy. Using the Lagrange theorem we discover the optimal resource distribution between all specific units. We then illustrate this general Multi-Amdahl technique using several examples of area and power allocation among several cores and accelerators.",2011-05-15T19:03:25Z,http://arxiv.org/pdf/1105.2960v1,2024-04-28,
1106.3677v1,"Pseudo-Ring Testing Schemes and Algorithms of RAM Built-In and Embedded Self-Testing","Scan and ring schemes of the pseudo-ring memory selftesting are investigated. Both schemes are based on emulation of the linear or nonlinear feedback shift register by memory itself. Peculiarities of the pseudo-ring schemes implementation for multi-port and embedded memories, and for register file are described. It is shown that only small additional logic is required and allows microcontrollers at-speed testing. Also, in this article,are given the a posteriori values of some type of memories faults coverage when pseudo-ring testing schemes are applied.",2011-06-18T18:44:38Z,http://arxiv.org/pdf/1106.3677v1,2024-04-28,
1106.3681v1,"SoC Software Components Diagnosis Technology","A novel approach to evaluation of hardware and software testability, represented in the form of register transfer graph, is proposed. Instances of making of software graph models for their subsequent testing and diagnosis are shown.",2011-06-18T19:07:44Z,http://arxiv.org/pdf/1106.3681v1,2024-04-28,
1107.3924v1,"Reversible arithmetic logic unit","Quantum computer requires quantum arithmetic. The sophisticated design of a reversible arithmetic logic unit (reversible ALU) for quantum arithmetic has been investigated in this letter. We provide explicit construction of reversible ALU effecting basic arithmetic operations. By provided the corresponding control unit, the proposed reversible ALU can combine the classical arithmetic and logic operation in a reversible integrated system. This letter provides actual evidence to prove the possibility of the realization of reversible Programmable Logic Device (RPLD) using reversible ALU.",2011-07-20T09:20:30Z,http://arxiv.org/pdf/1107.3924v1,2024-04-28,
1108.3970v2,"A Design Methodology for Folded, Pipelined Architectures in VLSI Applications using Projective Space Lattices","Semi-parallel, or folded, VLSI architectures are used whenever hardware resources need to be saved at design time. Most recent applications that are based on Projective Geometry (PG) based balanced bipartite graph also fall in this category. In this paper, we provide a high-level, top-down design methodology to design optimal semi-parallel architectures for applications, whose Data Flow Graph (DFG) is based on PG bipartite graph. Such applications have been found e.g. in error-control coding and matrix computations. Unlike many other folding schemes, the topology of connections between physical elements does not change in this methodology. Another advantage is the ease of implementation. To lessen the throughput loss due to folding, we also incorporate a multi-tier pipelining strategy in the design methodology. The design methodology has been verified by implementing a synthesis tool in C++, which has been verified as well. The tool is publicly available. Further, a complete decoder was manually protototyped before the synthesis tool design, to verify all the algorithms evolved in this paper, towards various steps of refinement. Another specific high-performance design of an LDPC decoder based on this methodology was worked out in past, and has been patented as well.",2011-08-19T14:32:32Z,http://arxiv.org/pdf/1108.3970v2,2024-04-28,
1108.5497v3,"Facile Algebraic Representation of a Novel Quaternary Logic","In this work, a novel quaternary algebra has been proposed that can be used to implement an arbitrary quaternary logic function in more than one systematic ways. The proposed logic has evolved from and is closely related to the Boolean algebra for binary domain; yet it does not lack the benefits of a higher-radix system. It offers seamless integration of the binary logic functions and expressions through a set of transforms and allows any binary logic simplification technique to be applied in quaternary domain. Since physical realization of the operators defined in this logic has recently been reported, it has become very important to have a well-defined algebra that will facilitate the algebraic manipulation of the novel quaternary logic and aid in designing various complex logic circuits. Therefore, based on our earlier works, here we describe the complete algebraic representation of this logic for the first time. The efficacy of the logic has been shown by designing and comparing several common logic circuits with existing designs in both binary and quaternary domain.",2011-08-28T08:07:09Z,http://arxiv.org/pdf/1108.5497v3,2024-04-28,
1109.0708v1,"A Novel Methodology for Thermal Analysis & 3-Dimensional Memory Integration","The semiconductor industry is reaching a fascinating confluence in several evolutionary trends that will likely lead to a number of revolutionary changes in the design, implementation, scaling, and the use of computer systems. However, recently Moore's law has come to a stand-still since device scaling beyond 65 nm is not practical. 2D integration has problems like memory latency, power dissipation, and large foot-print. 3D technology comes as a solution to the problems posed by 2D integration. The utilization of 3D is limited by the problem of temperature crisis. It is important to develop an accurate power profile extraction methodology to design 3D structure. In this paper, design of 3D integration of memory is considered and hence the static power dissipation of the memory cell is analysed in transistor level and is used to accurately model the inter-layer thermal effects for 3D memory stack. Subsequently, packaging of the chip is considered and modelled using an architecture level simulator. This modelling is intended to analyse the thermal effects of 3D memory, its reliability and lifetime of the chip, with greater accuracy.",2011-09-04T12:28:46Z,http://arxiv.org/pdf/1109.0708v1,2024-04-28,
1109.0752v1,"An improved distributed routing algorithm for Benes based optical NoC","Integrated optical interconnect is believed to be one of the main technologies to replace electrical wires. Optical Network-on-Chip (ONoC) has attracted more attentions nowadays. Benes topology is a good choice for ONoC for its rearrangeable non-blocking character, multistage feature and easy scalability. Routing algorithm plays an important role in determining the performance of ONoC. But traditional routing algorithms for Benes network are not suitable for ONoC communication, we developed a new distributed routing algorithm for Benes ONoC in this paper. Our algorithm selected the routing path dynamically according to network condition and enables more path choices for the message traveling in the network. We used OPNET to evaluate the performance of our routing algorithm and also compared it with a well-known bit-controlled routing algorithm. ETE delay and throughput were showed under different packet length and network sizes. Simulation results show that our routing algorithm can provide better performance for ONoC.",2011-09-04T20:02:53Z,http://arxiv.org/pdf/1109.0752v1,2024-04-28,
1109.0755v1,"Intelligent Bees for QoS Routing in Networks-on-Chip","Networks-on-Chip (NoCs) for future many-core processor platforms integrate more and more heterogeneous components of different types and many real-time and latency-sensitive applications can run on a single chip concurrently. The reconfigurable FPGA and reconfigurable NoCs have emerged for the purpose of reusability. Those types' traffics within NoCs exhibit diverse, burst, and unpredictable communication patterns. QoS guaranteed mechanisms are necessary to provide guaranteed throughput (GT) or guaranteed bandwidth (GB) performance for NoCs. In this paper, we propose a QoS routing algorithm inspired by bees' foraging behaviors to provide guaranteed bandwidth performance. Virtual circuits and Spatial Division Multiplexing are employed to maintain available paths for different type's traffics.",2011-09-04T20:46:08Z,http://arxiv.org/pdf/1109.0755v1,2024-04-28,
1110.1549v1,"Power comparison of CMOS and adiabatic full adder circuit","Full adders are important components in applications such as digital signal processors (DSP) architectures and microprocessors. Apart from the basic addition adders also used in performing useful operations such as subtraction, multiplication, division, address calculation, etc. In most of these systems the adder lies in the critical path that determines the overall performance of the system. In this paper conventional complementary metal oxide semiconductor (CMOS) and adiabatic adder circuits are analyzed in terms of power and transistor count using 0.18UM technology.",2011-10-07T14:40:51Z,http://arxiv.org/pdf/1110.1549v1,2024-04-28,
1110.3281v4,"Faster Energy Efficient Dadda Based Baugh-Wooley Multipliers","In this work faster Baugh-Wooley multiplication has been achieved by using a combination of two design techniques: partition of the partial products into two parts for independent parallel column compression and acceleration of the final addition using a hybrid adder proposed in this work. Based on the proposed techniques 8, 16, 32 and 64-bit Dadda based Baugh-Wooley multipliers has been developed and compared with the regular Baugh-Wooley multiplier. The performance of the proposed multiplier is analyzed by evaluating the delay, area and power, with 180 nm process technologies on interconnect and layout using industry standard design and layout tools. The result analysis shows that the 64-bit proposed multiplier is as much as 26.9% faster than the regular Baugh-Wooley multiplier and requires only 2.21% more power. Also the power-delay product of the proposed design is significantly lower than that of the regular Baugh-Wooley multiplier.",2011-10-14T17:52:39Z,http://arxiv.org/pdf/1110.3281v4,2024-04-28,
1110.3376v2,"Faster and Low Power Twin Precision Multiplier","In this work faster unsigned multiplication has been achieved by using a combination of High Performance Multiplication [HPM] column reduction technique and implementing a N-bit multiplier using 4 N/2-bit multipliers (recursive multiplication) and acceleration of the final addition using a hybrid adder. Low power has been achieved by using clock gating technique. Based on the proposed technique 16 and 32-bit multipliers are developed. The performance of the proposed multiplier is analyzed by evaluating the delay, area and power, with TCBNPHP 90 nm process technology on interconnect and layout using Cadence NC launch, RTL compiler and ENCOUNTER tools. The results show that the 32-bit proposed multiplier is as much as 22% faster, occupies only 3% more area and consumes 30% lesser power with respect to the recently reported twin precision multiplier.",2011-10-15T03:54:27Z,http://arxiv.org/pdf/1110.3376v2,2024-04-28,
1110.3535v1,"Multi-core processors - An overview","Microprocessors have revolutionized the world we live in and continuous efforts are being made to manufacture not only faster chips but also smarter ones. A number of techniques such as data level parallelism, instruction level parallelism and hyper threading (Intel's HT) already exists which have dramatically improved the performance of microprocessor cores. This paper briefs on evolution of multi-core processors followed by introducing the technology and its advantages in today's world. The paper concludes by detailing on the challenges currently faced by multi-core processors and how the industry is trying to address these issues.",2011-10-16T22:48:56Z,http://arxiv.org/pdf/1110.3535v1,2024-04-28,
1110.3584v1,"Optimal Final Carry Propagate Adder Design for Parallel Multipliers","Based on the ASIC layout level simulation of 7 types of adder structures each of four different sizes, i.e. a total of 28 adders, we propose expressions for the width of each of the three regions of the final Carry Propagate Adder (CPA) to be used in parallel multipliers. We also propose the types of adders to be used in each region that would lead to the optimal performance of the hybrid final adders in parallel multipliers. This work evaluates the complete performance of the analyzed designs in terms of delay, area, power through custom design and layout in 0.18 um CMOS process technology.",2011-10-17T06:16:05Z,http://arxiv.org/pdf/1110.3584v1,2024-04-28,
1110.3655v1,"Accelerating Algorithms using a Dataflow Graph in a Reconfigurable System","In this paper, the acceleration of algorithms using a design of a field programmable gate array (FPGA) as a prototype of a static dataflow architecture is discussed. The static dataflow architecture using operators interconnected by parallel buses was implemented. Accelerating algorithms using a dataflow graph in a reconfigurable system shows the potential for high computation rates. The results of benchmarks implemented using the static dataflow architecture are reported at the end of this paper.",2011-10-17T13:00:10Z,http://arxiv.org/pdf/1110.3655v1,2024-04-28,
1110.6865v1,"FPGA implementation of short critical path CORDIC-based approximation of the eight-point DCT","This paper presents an efficient approach for multiplierless implementation for eight-point DCT approximation, which based on coordinate rotation digital computer (CORDIC) algorithm. The main design objective is to make critical path of corresponding circuits shorter and reduce the combinational delay of proposed scheme.",2011-10-31T17:13:25Z,http://arxiv.org/pdf/1110.6865v1,2024-04-28,
1111.0703v1,"Efficient Network for Non-Binary QC-LDPC Decoder","This paper presents approaches to develop efficient network for non-binary quasi-cyclic LDPC (QC-LDPC) decoders. By exploiting the intrinsic shifting and symmetry properties of the check matrices, significant reduction of memory size and routing complexity can be achieved. Two different efficient network architectures for Class-I and Class-II non-binary QC-LDPC decoders have been proposed, respectively. Comparison results have shown that for the code of the 64-ary (1260, 630) rate-0.5 Class-I code, the proposed scheme can save more than 70.6% hardware required by shuffle network than the state-of-the-art designs. The proposed decoder example for the 32-ary (992, 496) rate-0.5 Class-II code can achieve a 93.8% shuffle network reduction compared with the conventional ones. Meanwhile, based on the similarity of Class-I and Class-II codes, similar shuffle network is further developed to incorporate both classes of codes at a very low cost.",2011-11-03T01:10:43Z,http://arxiv.org/pdf/1111.0703v1,2024-04-28,
1111.0704v1,"Reduced-Latency SC Polar Decoder Architectures","Polar codes have become one of the most favorable capacity achieving error correction codes (ECC) along with their simple encoding method. However, among the very few prior successive cancellation (SC) polar decoder designs, the required long code length makes the decoding latency high. In this paper, conventional decoding algorithm is transformed with look-ahead techniques. This reduces the decoding latency by 50%. With pipelining and parallel processing schemes, a parallel SC polar decoder is proposed. Sub-structure sharing approach is employed to design the merged processing element (PE). Moreover, inspired by the real FFT architecture, this paper presents a novel input generating circuit (ICG) block that can generate additional input signals for merged PEs on-the-fly. Gate-level analysis has demonstrated that the proposed design shows advantages of 50% decoding latency and twice throughput over the conventional one with similar hardware cost.",2011-11-03T01:12:49Z,http://arxiv.org/pdf/1111.0704v1,2024-04-28,
1111.0705v1,"Low-Latency SC Decoder Architectures for Polar Codes","Nowadays polar codes are becoming one of the most favorable capacity achieving error correction codes for their low encoding and decoding complexity. However, due to the large code length required by practical applications, the few existing successive cancellation (SC) decoder implementations still suffer from not only the high hardware cost but also the long decoding latency. This paper presents novel several approaches to design low-latency decoders for polar codes based on look-ahead techniques. Look-ahead techniques can be employed to reschedule the decoding process of polar decoder in numerous approaches. However, among those approaches, only well-arranged ones can achieve good performance in terms of both latency and hardware complexity. By revealing the recurrence property of SC decoding chart, the authors succeed in reducing the decoding latency by 50% with look-ahead techniques. With the help of VLSI-DSP design techniques such as pipelining, folding, unfolding, and parallel processing, methodologies for four different polar decoder architectures have been proposed to meet various application demands. Sub-structure sharing scheme has been adopted to design the merged processing element (PE) for further hardware reduction. In addition, systematic methods for construction refined pipelining decoder (2nd design) and the input generating circuits (ICG) block have been given. Detailed gate-level analysis has demonstrated that the proposed designs show latency advantages over conventional ones with similar hardware cost.",2011-11-03T01:15:58Z,http://arxiv.org/pdf/1111.0705v1,2024-04-28,
1111.3056v1,"Performance of Cache Memory Subsystems for Multicore Architectures","Advancements in multi-core have created interest among many research groups in finding out ways to harness the true power of processor cores. Recent research suggests that on-board component such as cache memory plays a crucial role in deciding the performance of multi-core systems. In this paper, performance of cache memory is evaluated through the parameters such as cache access time, miss rate and miss penalty. The influence of cache parameters over execution time is also discussed. Results obtained from simulated studies of multi-core environments with different instruction set architectures (ISA) like ALPHA and X86 are produced.",2011-11-13T19:28:01Z,http://arxiv.org/pdf/1111.3056v1,2024-04-28,
1111.4279v1,"Elastic Fidelity: Trading-off Computational Accuracy for Energy Reduction","Power dissipation and energy consumption have become one of the most important problems in the design of processors today. This is especially true in power-constrained environments, such as embedded and mobile computing. While lowering the operational voltage can reduce power consumption, there are limits imposed at design time, beyond which hardware components experience faulty operation. Moreover, the decrease in feature size has led to higher susceptibility to process variations, leading to reliability issues and lowering yield. However, not all computations and all data in a workload need to maintain 100% fidelity. In this paper, we explore the idea of employing functional or storage units that let go the conservative guardbands imposed on the design to guarantee reliable execution. Rather, these units exhibit Elastic Fidelity, by judiciously lowering the voltage to trade-off reliable execution for power consumption based on the error guarantees required by the executing code. By estimating the accuracy required by each computational segment of a workload, and steering each computation to different functional and storage units, Elastic Fidelity Computing obtains power and energy savings while reaching the reliability targets required by each computational segment. Our preliminary results indicate that even with conservative estimates, Elastic Fidelity can reduce the power and energy consumption of a processor by 11-13% when executing applications involving human perception that are typically included in modern mobile platforms, such as audio, image, and video decoding.",2011-11-18T04:12:12Z,http://arxiv.org/pdf/1111.4279v1,2024-04-28,
1111.7258v1,"A New Design for Array Multiplier with Trade off in Power and Area","In this paper a low power and low area array multiplier with carry save adder is proposed. The proposed adder eliminates the final addition stage of the multiplier than the conventional parallel array multiplier. The conventional and proposed multiplier both are synthesized with 16-T full adder. Among Transmission Gate, Transmission Function Adder, 14-T, 16-T full adder shows energy efficiency. In the proposed 4x4 multiplier to add carry bits with out using Ripple Carry Adder (RCA) in the final stage, the carries given to the input of the next left column input. Due to this the proposed multiplier shows 56 less transistor count, then cause trade off in power and area. The proposed multiplier has shown 13.91% less power, 34.09% more speed and 59.91% less energy consumption for TSMC 0.18nm technology at a supply voltage 2.0V than the conventional multiplier.",2011-11-30T18:07:44Z,http://arxiv.org/pdf/1111.7258v1,2024-04-28,
1112.0727v1,"Quantum Cost Efficient Reversible BCD Adder for Nanotechnology Based Systems","Reversible logic allows low power dissipating circuit design and founds its application in cryptography, digital signal processing, quantum and optical information processing. This paper presents a novel quantum cost efficient reversible BCD adder for nanotechnology based systems using PFAG gate. It has been demonstrated that the proposed design offers less hardware complexity and requires minimum number of garbage outputs than the existing counterparts. The remarkable property of the proposed designs is that its quantum realization is given in NMR technology.",2011-12-04T06:48:53Z,http://arxiv.org/pdf/1112.0727v1,2024-04-28,
1201.0954v1,"Information Analysis Infrastructure for Diagnosis","A high-speed multiprocessor architecture for brain-like analyzing information represented in analytic, graph- and table forms of associative relations to search, recognize and make a decision in n-dimensional vector discrete space is offered. Vector-logical process models of actual applications, where the quality of solution is estimated by the proposed integral non-arithmetical metric of the interaction between binary vectors, are described. The theoretical proof of the metric for a vector logical space and the quality criteria for estimating solutions is created.",2012-01-04T18:21:06Z,http://arxiv.org/pdf/1201.0954v1,2024-04-28,
1201.1674v1,"Theoretical Modeling and Simulation of Phase-Locked Loop (PLL) for Clock Data Recovery (CDR)","Modern communication and computer systems require rapid (Gbps), efficient and large bandwidth data transfers. Agressive scaling of digital integrated systems allow buses and communication controller circuits to be integrated with the microprocessor on the same chip. The Peripheral Component Interconnect Express (PCIe) protocol handles all communcation between the central processing unit (CPU) and hardware devices. PCIe buses require efficient clock data recovery circuits (CDR) to recover clock signals embedded in data during transmission. This paper describes the theoretical modeling and simulation of a phase-locked loop (PLL) used in a CDR circuit. A simple PLL architecture for a 5 GHz CDR circuit is proposed and elaborated in this work. Simulations were carried out using a Hardware Description Language, Verilog- AMS. The effect of jitter on the proposed design is also simulated and evaluated in this work. It was found that the proposed design is robust against both input and VCO jitter.",2012-01-09T01:00:44Z,http://arxiv.org/pdf/1201.1674v1,2024-04-28,
1201.2107v1,"Design and ASIC implementation of DUC/DDC for communication systems","Communication systems use the concept of transmitting information using the electrical distribution network as a communication channel. To enable the transmission data signal modulated on a carrier signal is superimposed on the electrical wires. Typical power lines are designed to handle 50/60 Hz of AC power signal; however they can carry the signals up to 500 KHz frequency. This work aims to aid transmission/reception of an audio signal in the spectrum from 300 Hz to 4000 Hz using PLCC on a tunable carrier frequency in the spectrum from 200 KHz to 500 KHz. For digital amplitude modulation the sampling rate of the carrier and the audio signal has to be matched. Tunable carrier generation can be achieved with Direct Digital Synthesizers at a desired sampling rate. DSP Sample rate conversion techniques are very useful to make the sampling circuits to work on their own sampling rates which are fine for the data/modulated-carrier signal's bandwidth. This also simplifies the complexity of the sampling circuits. Digital Up Conversion (DUC) and Digital Down Conversion (DDC) are DSP sample rate conversion techniques which refer to increasing and decreasing the sampling rate of a signal respectively.",2012-01-10T16:46:57Z,http://arxiv.org/pdf/1201.2107v1,2024-04-28,
1202.0613v1,"A Resolution for Shared Memory Conflict in Multiprocessor System-on-a-Chip","Now days, manufacturers are focusing on increasing the concurrency in multiprocessor system-on-a-chip (MPSoC) architecture instead of increasing clock speed, for embedded systems. Traditionally lock-based synchronization is provided to support concurrency; as managing locks can be very difficult and error prone. Transactional memories and lock based systems have been extensively used to provide synchronization between multiple processors [1] in general-purpose systems. It has been shown that locks have numerous shortcomings over transactional memory in terms of power consumption, ease of programming and performance. In this paper, we propose a new semaphore scheme for synchronization in shared cache memory in an MPSoC. Moreover, we have evaluated and compared our scheme with locks and transactions in terms of energy consumption and cache miss rate using SimpleScalar functional simulator.",2012-02-03T06:47:39Z,http://arxiv.org/pdf/1202.0613v1,2024-04-28,
1203.0787v1,"A handy systematic method for data hazards detection in an instruction set of a pipelined microprocessor","It is intended in this document to introduce a handy systematic method for enumerating all possible data dependency cases that could occur between any two instructions that might happen to be processed at the same time at different stages of the pipeline. Given instructions of the instruction set, specific information about operands of each instruction and when an instruction reads or writes data, the method could be used to enumerate all possible data hazard cases and to determine whether forwarding or stalling is suitable for resolving each case.",2012-03-04T23:43:07Z,http://arxiv.org/pdf/1203.0787v1,2024-04-28,
1203.4150v1,"Designing a WISHBONE Protocol Network Adapter for an Asynchronous Network-on-Chip","The Scaling of microchip technologies, from micron to submicron and now to deep sub-micron (DSM) range, has enabled large scale systems-on-chip (SoC). In future deep submicron (DSM) designs, the interconnect effect will definitely dominate performance. Network-on-Chip (NoC) has become a promising solution to bus-based communication infrastructure limitations. NoC designs usually targets Application Specific Integrated Circuits (ASICs), however, the fabrication process costs a lot. Implementing a NoC on an FPGA does not only reduce the cost but also decreases programming and verification cycles. In this paper, an Asynchronous NoC has been implemented on a SPARTAN-3E\textregistered device. The NoC supports basic transactions of both widely used on-chip interconnection standards, the Open Core Protocol (OCP) and the WISHBONE Protocol. Although, FPGA devices are synchronous in nature, it has been shown that they can be used to prototype a Global Asynchronous Local Synchronous (GALS) systems, comprising an Asynchronous NoC connecting IP cores operating in different clock domains.",2012-03-19T16:18:54Z,http://arxiv.org/pdf/1203.4150v1,2024-04-28,
1203.5349v1,"LOCKE Detailed Specification Tables","This document shows the detailed specification of LOCKE coherence protocol for each cache controller, using a table-based technique. This representation provides clear, concise visual information yet includes sufficient detail (e.g., transient states) arguably lacking in the traditional, graphical form of state diagrams.",2012-03-22T10:48:33Z,http://arxiv.org/pdf/1203.5349v1,2024-04-28,
1204.1179v1,"C-slow Technique vs Multiprocessor in designing Low Area Customized Instruction set Processor for Embedded Applications","The demand for high performance embedded processors, for consumer electronics, is rapidly increasing for the past few years. Many of these embedded processors depend upon custom built Instruction Ser Architecture (ISA) such as game processor (GPU), multimedia processors, DSP processors etc. Primary requirement for consumer electronic industry is low cost with high performance and low power consumption. A lot of research has been evolved to enhance the performance of embedded processors through parallel computing. But some of them focus superscalar processors i.e. single processors with more resources like Instruction Level Parallelism (ILP) which includes Very Long Instruction Word (VLIW) architecture, custom instruction set extensible processor architecture and others require more number of processing units on a single chip like Thread Level Parallelism (TLP) that includes Simultaneous Multithreading (SMT), Chip Multithreading (CMT) and Chip Multiprocessing (CMP). In this paper, we present a new technique, named C-slow, to enhance performance for embedded processors for consumer electronics by exploiting multithreading technique in single core processors. Without resulting into the complexity of micro controlling with Real Time Operating system (RTOS), C-slowed processor can execute multiple threads in parallel using single datapath of Instruction Set processing element. This technique takes low area & approach complexity of general purpose processor running RTOS.",2012-04-05T10:59:03Z,http://arxiv.org/pdf/1204.1179v1,2024-04-28,
1204.2809v1,"Performance-Optimum Superscalar Architecture for Embedded Applications","Embedded applications are widely used in portable devices such as wireless phones, personal digital assistants, laptops, etc. High throughput and real time requirements are especially important in such data-intensive tasks. Therefore, architectures that provide the required performance are the most desirable. On the other hand, processor performance is severely related to the average memory access delay, number of processor registers and also size of the instruction window and superscalar parameters. Therefore, cache, register file and superscalar parameters are the major architectural concerns in designing a superscalar architecture for embedded processors. Although increasing cache and register file size leads to performance improvements in high performance embedded processors, the increased area, power consumption and memory delay are the overheads of these techniques. This paper explores the effect of cache, register file and superscalar parameters on the processor performance to specify the optimum size of these parameters for embedded applications. Experimental results show that although having bigger size of these parameters is one of the performance improvement approaches in embedded processors, however, by increasing the size of some parameters over a threshold value, performance improvement is saturated and especially in cache size, increments over this threshold value decrease the performance.",2012-04-12T18:40:55Z,http://arxiv.org/pdf/1204.2809v1,2024-04-28,
1204.5407v1,"Reversible Programmable Logic Array (RPLA) using Feynman & MUX Gates for Low Power Industrial Applications","This paper present the research work directed towards the design of reversible programmable logic array using very high speed integrated circuit hardware description language (VHDL). Reversible logic circuits have significant importance in bioinformatics, optical information processing, CMOS design etc. In this paper the authors propose the design of new RPLA using Feynman & MUX gate.VHDL based codes of reversible gates with simulating results are shown .This proposed RPLA may be further used to design any reversible logic function or Boolean function (Adder, subtractor etc.) which dissipate very low or ideally no heat.",2012-04-24T15:28:25Z,http://arxiv.org/pdf/1204.5407v1,2024-04-28,
1205.1737v2,"A simple 1-byte 1-clock RC4 design and its efficient implementation in FPGA coprocessor for secured ethernet communication","In the field of cryptography till date the 1-byte in 1-clock is the best known RC4 hardware design [1], while the 1-byte in 3clocks is the best known implementation [2,3]. The design algorithm in [1] considers two consecutive bytes together and processes them in 2 clocks. The design of 1-byte in 3-clocks is too much modular and clock hungry. In this paper considering the RC4 algorithm, as it is, a simpler RC4 hardware design providing higher throughput is proposed in which 1-byte is processed in 1-clock. In the design two sequential tasks are executed as two independent events during rising and falling edges of the same clock and the swapping is directly executed using a MUX-DEMUX combination. The power consumed in behavioral and structural designs of RC4 are estimated and a power optimization technique is proposed. The NIST statistical test suite is run on RC4 key streams in order to know its randomness property. The encryption and decryption designs are respectively embedded on two FPGA boards with RC4 in a custom coprocessor followed by Ethernet communication.",2012-05-08T16:30:40Z,http://arxiv.org/pdf/1205.1737v2,2024-04-28,
1205.1860v1,"Wishbone bus Architecture - A Survey and Comparison","The performance of an on-chip interconnection architecture used for communication between IP cores depends on the efficiency of its bus architecture. Any bus architecture having advantages of faster bus clock speed, extra data transfer cycle, improved bus width and throughput is highly desirable for a low cost, reduced time-to-market and efficient System-on-Chip (SoC). This paper presents a survey of WISHBONE bus architecture and its comparison with three other on-chip bus architectures viz. Advanced Micro controller Bus Architecture (AMBA) by ARM, CoreConnect by IBM and Avalon by Altera. The WISHBONE Bus Architecture by Silicore Corporation appears to be gaining an upper edge over the other three bus architecture types because of its special performance parameters like the use of flexible arbitration scheme and additional data transfer cycle (Read-Modify-Write cycle). Moreover, its IP Cores are available free for use requiring neither any registration nor any agreement or license.",2012-05-09T03:18:40Z,http://arxiv.org/pdf/1205.1860v1,2024-04-28,
1205.1866v1,"Microcontroller Based Testing of Digital IP-Core","Testing core based System on Chip is a challenge for the test engineers. To test the complete SOC at one time with maximum fault coverage, test engineers prefer to test each IP-core separately. At speed testing using external testers is more expensive because of gigahertz processor. The purpose of this paper is to develop cost efficient and flexible test methodology for testing digital IP-cores . The prominent feature of the approach is to use microcontroller to test IP-core. The novel feature is that there is no need of test pattern generator and output response analyzer as microcontroller performs the function of both. This approach has various advantages such as at speed testing, low cost, less area overhead and greater flexibility since most of the testing process is based on software.",2012-05-09T04:15:39Z,http://arxiv.org/pdf/1205.1866v1,2024-04-28,
1205.1871v1,"Design Space Exploration to Find the Optimum Cache and Register File Size for Embedded Applications","In the future, embedded processors must process more computation-intensive network applications and internet traffic and packet-processing tasks become heavier and sophisticated. Since the processor performance is severely related to the average memory access delay and also the number of processor registers affects the performance, cache and register file are two major parts in designing embedded processor architecture. Although increasing cache and register file size leads to performance improvement in embedded applications and packet-processing tasks in high traffic networks with too much packets, the increased area, power consumption and memory hierarchy delay are the overheads of these techniques. Therefore, implementing these components in the optimum size is of significant interest in the design of embedded processors. This paper explores the effect of cache and register file size on the processor performance to calculate the optimum size of these components for embedded applications. Experimental results show that although having bigger cache and register file is one of the performance improvement approaches in embedded processors, however, by increasing the size of these parameters over a threshold level, performance improvement is saturated and then, decreased.",2012-05-09T05:23:13Z,http://arxiv.org/pdf/1205.1871v1,2024-04-28,
1205.2153v1,"Design and implementation of real time AES-128 on real time operating system for multiple FPGA communication","Security is the most important part in data communication system, where more randomization in secret keys increases the security as well as complexity of the cryptography algorithms. As a result in recent dates these algorithms are compensating with enormous memory spaces and large execution time on hardware platform. Field programmable gate arrays (FPGAs), provide one of the major alternative in hardware platform scenario due to its reconfiguration nature, low price and marketing speed. In FPGA based embedded system we can use embedded processor to execute particular algorithm with the inclusion of a real time operating System (RTOS), where threads may reduce resource utilization and time consumption. A process in the runtime is separated in different smaller tasks which are executed by the scheduler to meet the real time dead line using RTOS. In this paper we demonstrate the design and implementation of a 128-bit Advanced Encryption Standard (AES) both symmetric key encryption and decryption algorithm by developing suitable hardware and software design on Xilinx Spartan- 3E (XC3S500E-FG320) device using an Xilkernel RTOS, the implementation has been tested successfully The system is optimized in terms of execution speed and hardware utilization.",2012-05-10T04:04:45Z,http://arxiv.org/pdf/1205.2153v1,2024-04-28,
1205.2428v1,"Relaxed Half-Stochastic Belief Propagation","Low-density parity-check codes are attractive for high throughput applications because of their low decoding complexity per bit, but also because all the codeword bits can be decoded in parallel. However, achieving this in a circuit implementation is complicated by the number of wires required to exchange messages between processing nodes. Decoding algorithms that exchange binary messages are interesting for fully-parallel implementations because they can reduce the number and the length of the wires, and increase logic density. This paper introduces the Relaxed Half-Stochastic (RHS) decoding algorithm, a binary message belief propagation (BP) algorithm that achieves a coding gain comparable to the best known BP algorithms that use real-valued messages. We derive the RHS algorithm by starting from the well-known Sum-Product algorithm, and then derive a low-complexity version suitable for circuit implementation. We present extensive simulation results on two standardized codes having different rates and constructions, including low bit error rate results. These simulations show that RHS can be an advantageous replacement for the existing state-of-the-art decoding algorithms when targeting fully-parallel implementations.",2012-05-11T05:06:47Z,http://arxiv.org/pdf/1205.2428v1,2024-04-28,
1205.4967v1,"Investigating Warp Size Impact in GPUs","There are a number of design decisions that impact a GPU's performance. Among such decisions deciding the right warp size can deeply influence the rest of the design. Small warps reduce the performance penalty associated with branch divergence at the expense of a reduction in memory coalescing. Large warps enhance memory coalescing significantly but also increase branch divergence. This leaves designers with two choices: use a small warps and invest in finding new solutions to enhance coalescing or use large warps and address branch divergence employing effective control-flow solutions. In this work our goal is to investigate the answer to this question. We analyze warp size impact on memory coalescing and branch divergence. We use our findings to study two machines: a GPU using small warps but equipped with excellent memory coalescing (SW+) and a GPU using large warps but employing an MIMD engine immune from control-flow costs (LW+). Our evaluations show that building coalescing-enhanced small warp GPUs is a better approach compared to pursuing a control-flow enhanced large warp GPU.",2012-05-22T16:26:23Z,http://arxiv.org/pdf/1205.4967v1,2024-04-28,
1206.1567v1,"Architecture for real time continuous sorting on large width data volume for fpga based applications","In engineering applications sorting is an important and widely studied problem where execution speed and resources used for computation are of extreme importance, especially if we think about real time data processing. Most of the traditional sorting techniques compute the process after receiving all of the data and hence the process needs large amount of resources for data storage. So, suitable design strategy needs to be adopted if we wish to sort a large amount of data in real time, which essential means higher speed of process execution and utilization of fewer resources in most of the cases. This paper proposes a single chip scalable architecture based on Field Programmable Gate Array(FPGA), for a modified counting sort algorithm where data acquisition and sorting is being done in real time scenario. Our design promises to work efficiently, where data can be accepted in the run time scenario without any need of prior storage of data and also the execution speed of our algorithm is invariant to the length of the data stream. The proposed design is implemented and verified on Spartan 3E(XC3S500E-FG320) FPGA system. The results prove that our design is better in terms of some of the design parameters compared to the existing research works.",2012-06-07T18:17:32Z,http://arxiv.org/pdf/1206.1567v1,2024-04-28,
1206.2132v1,"RepTFD: Replay Based Transient Fault Detection","The advances in IC process make future chip multiprocessors (CMPs) more and more vulnerable to transient faults. To detect transient faults, previous core-level schemes provide redundancy for each core separately. As a result, they may leave transient faults in the uncore parts, which consume over 50% area of a modern CMP, escaped from detection. This paper proposes RepTFD, the first core-level transient fault detection scheme with 100% coverage. Instead of providing redundancy for each core separately, RepTFD provides redundancy for a group of cores as a whole. To be specific, it replays the execution of the checked group of cores on a redundant group of cores. Through comparing the execution results between the two groups of cores, all malignant transient faults can be caught. Moreover, RepTFD adopts a novel pending period based record-replay approach, which can greatly reduce the number of execution orders that need to be enforced in the replay-run. Hence, RepTFD brings only 4.76% performance overhead in comparison to the normal execution without fault-tolerance according to our experiments on the RTL design of an industrial CMP named Godson-3. In addition, RepTFD only consumes about 0.83% area of Godson-3, while needing only trivial modifications to existing components of Godson-3.",2012-06-11T08:50:37Z,http://arxiv.org/pdf/1206.2132v1,2024-04-28,
1206.4753v2,"DLS: Directoryless Shared Last-level Cache","Directory-based protocols have been the de facto solution for maintaining cache coherence in shared-memory parallel systems comprising multi/many cores, where each store instruction is eagerly made globally visible by invalidating the private cache (PC) backups of other cores. Consequently, the directory not only consumes large chip area, but also incurs considerable energy consumption and performance degradation, due to the large number of Invalidation/Ack messages transferred in the interconnection network and resulting network congestion. In this paper, we reveal the interesting fact that the directory is actually an unnecessary luxury for practical parallel systems. Because of widely deployed software/hardware techniques involving instruction reordering, most (if not all) parallel systems work under the weak consistency model, where a remote store instruction is allowed to be invisible to a core before the next synchronization of the core, instead of being made visible eagerly by invalidating PC backups of other cores. Based on this key observation, we propose a lightweight novel scheme called {\em DLS (DirectoryLess Shared last-level cache)}, which completely removes the directory and Invalidation/Ack messages, and efficiently maintains cache coherence using a novel {\em self-suspicion + speculative execution} mechanism. Experimental results over SPLASH-2 benchmarks show that on a 16-core processor, DLS not only completely removes the chip area cost of the directory, but also improves processor performance by 11.08%, reduces overall network traffic by 28.83%, and reduces energy consumption of the network by 15.65% on average (compared with traditional MESI protocol with full directory). Moreover, DLS does not involve any modification to programming languages and compilers, and hence is seamlessly compatible with legacy codes.",2012-06-21T00:59:43Z,http://arxiv.org/pdf/1206.4753v2,2024-04-28,
1207.1187v1,"Dynamic Priority Queue: An SDRAM Arbiter With Bounded Access Latencies for Tight WCET Calculation","This report introduces a shared resource arbitration scheme ""DPQ - Dynamic Priority Queue"" which provides bandwidth guarantees and low worst case latency to each master in an MPSoC. Being a non-trivial candidate for timing analysis, SDRAM has been chosen as a showcase, but the approach is valid for any shared resource arbitration.   Due to its significant cost, data rate and physical size advantages, SDRAM is a potential candidate for cost sensitive, safety critical and space conserving systems. The variable access latency is a major drawback of SDRAM that induces largely over estimated Worst Case Execution Time (WCET) bounds of applications. In this report we present the DPQ together with an algorithm to predict the shared SDRAM's worst case latencies. We use the approach to calculate WCET bounds of six hardware tasks executing on an Altera Cyclone III FPGA with shared DDR2 memory. The results show that the DPQ is a fair arbitration scheme and produces low WCET bounds.",2012-07-05T08:20:02Z,http://arxiv.org/pdf/1207.1187v1,2024-04-28,
1207.1683v1,"Design and Development of Low Cost Multi-Channel USB Data","This paper describes the design and development of low cost USB Data Acquisition System (DAS) for the measurement of physical parameters. Physical parameters such as temperature, humidity, light intensity etc., which are generally slowly varying signals are sensed by respective sensors or integrated sensors and converted into voltages. The DAS is designed using PIC18F4550 microcontroller, communicating with Personal Computer (PC) through USB (Universal Serial Bus). The designed DAS has been tested with the application program developed in Visual Basic, which allows online monitoring in graphical as well as numerical display.",2012-07-06T16:52:24Z,http://arxiv.org/pdf/1207.1683v1,2024-04-28,
1207.2060v1,"Design of PIC12F675 Microcontroller Based Data Acquisition System for Slowly Varying Signals","The present paper describes the design of a cost effective, better resolution data acquisition system (DAS) which is compatible to most of the PC and laptops. A low cost DAS has been designed using PIC12F675 having 4-channel analog input with 10-bit resolution for the monitoring of slowly varying signals. The DAS so designed is interfaced to the serial port of the PC. Firmware is written in Basic using Oshonsoft PIC IDE and burn to the microcontroller by using PICkit2 programmer. An application program is also developed using Visual Basic 6 which allows to display the waveform of the signal(s) and simultaneously the data also can be saved into the hard disk of the computer for future use and analysis.",2012-07-09T14:25:51Z,http://arxiv.org/pdf/1207.2060v1,2024-04-28,
1207.2739v1,"Low Cost PC Based Real Time Data Logging System Using PCs Parallel Port For Slowly Varying Signals","A low cost PC based real time data logging system can be used in the laboratories for the measurement, monitoring and storage of the data for slowly varying signals in science and engineering stream. This can be designed and interfaced to the PCs Parallel Port, which is common to all desktop computers or Personal Computers (PCs). By the use of this data logging system one can monitor, measure and store data for slowly varying signals, which is hard to visualise the signal waveforms by ordinary CRO (Cathode Ray Oscilloscope) and DSO (Digital Storage Oscilloscope). The data so stored can be used for further study and analysis. It can be used for a wide range of applications to monitor and store data of temperature, humidity, light intensity, ECG signals etc. with proper signal conditioning circuitry.",2012-07-11T18:29:21Z,http://arxiv.org/pdf/1207.2739v1,2024-04-28,
1207.2840v1,"Design and Performance Analysis of hybrid adders for high speed arithmetic circuit","Adder cells using Gate Diffusion Technique (GDI) & PTL-GDI technique are described in this paper. GDI technique allows reducing power consumption, propagation delay and low PDP (power delay product) whereas Pass Transistor Logic (PTL) reduces the count of transistors used to make different logic gates, by eliminating redundant transistors. Performance comparison with various Hybrid Adder is been presented. In this paper, we propose two new designs based on GDI & PTL techniques, which is found to be much more power efficient in comparison with existing design technique. Only 10 transistors are used to implement the SUM & CARRY function for both the designs. The SUM and CARRY cell are implemented in a cascaded way i.e. firstly the XOR cell is implemented and then using XOR as input SUM as well as CARRY cell is implemented. For Proposed GDI adder the SUM as well as CARRY cell is designed using GDI technique. On the other hand in Proposed PTL-GDI adder the SUM cell is constructed using PTL technique and the CARRY cell is designed using GDI technique. The advantages of both the designs are discussed. The significance of these designs is substantiated by the simulation results obtained from Cadence Virtuoso 180nm environment.",2012-07-12T03:57:37Z,http://arxiv.org/pdf/1207.2840v1,2024-04-28,
1208.0995v1,"Design and implementation of a digital clock showing digits in Bangla font using microcontroller AT89C4051","In this paper, a digital clock is designed where the microcontroller is used for timing controller and the font of the Bangla digits are designed, and programmed within the microcontroller. The design is cost effective, simple and easy for maintenance.",2012-08-05T09:22:06Z,http://arxiv.org/pdf/1208.0995v1,2024-04-28,
1208.2374v2,"Dynamic Warp Resizing in High-Performance SIMT","Modern GPUs synchronize threads grouped in a warp at every instruction. These results in improving SIMD efficiency and makes sharing fetch and decode resources possible. The number of threads included in each warp (or warp size) affects divergence, synchronization overhead and the efficiency of memory access coalescing. Small warps reduce the performance penalty associated with branch and memory divergence at the expense of a reduction in memory coalescing. Large warps enhance memory coalescing significantly but also increase branch and memory divergence. Dynamic workload behavior, including branch/memory divergence and coalescing, is an important factor in determining the warp size returning best performance. Optimal warp size can vary from one workload to another or from one program phase to the next. Based on this observation, we propose Dynamic Warp Resizing (DWR). DWR takes innovative microarchitectural steps to adjust warp size during runtime and according to program characteristics. DWR outperforms static warp size decisions, up to 1.7X to 2.28X, while imposing less than 1% area overhead. We investigate various alternative configurations and show that DWR performs better for narrower SIMD and larger caches.",2012-08-11T18:05:59Z,http://arxiv.org/pdf/1208.2374v2,2024-04-28,
1209.3099v1,"A Cache Management Strategy to Replace Wear Leveling Techniques for Embedded Flash Memory","Prices of NAND flash memories are falling drastically due to market growth and fabrication process mastering while research efforts from a technological point of view in terms of endurance and density are very active. NAND flash memories are becoming the most important storage media in mobile computing and tend to be less confined to this area. The major constraint of such a technology is the limited number of possible erase operations per block which tend to quickly provoke memory wear out. To cope with this issue, state-of-the-art solutions implement wear leveling policies to level the wear out of the memory and so increase its lifetime. These policies are integrated into the Flash Translation Layer (FTL) and greatly contribute in decreasing the write performance. In this paper, we propose to reduce the flash memory wear out problem and improve its performance by absorbing the erase operations throughout a dual cache system replacing FTL wear leveling and garbage collection services. We justify this idea by proposing a first performance evaluation of an exclusively cache based system for embedded flash memories. Unlike wear leveling schemes, the proposed cache solution reduces the total number of erase operations reported on the media by absorbing them in the cache for workloads expressing a minimal global sequential rate.",2012-09-14T06:09:10Z,http://arxiv.org/pdf/1209.3099v1,2024-04-28,
1209.3564v1,"Deadlock Recovery Technique in Bus Enhanced NoC Architecture","Increase in the speed of processors has led to crucial role of communication in the performance of systems. As a result, routing is taken into consideration as one of the most important subjects of the Network on Chip architecture. Routing algorithms to deadlock avoidance prevent packets route completely based on network traffic condition by means of restricting the route of packets. This action leads to less performance especially in non-uniform traffic patterns. On the other hand True Fully Adoptive Routing algorithm provides routing of packets completely based on traffic condition. However, deadlock detection and recovery mechanisms are needed to handle deadlocks. Use of global bus beside NoC as a parallel supportive environment, provide platform to offer advantages of both features of bus and NoC. This bus is useful for broadcast and multicast operations, sending delay sensitive signals, system management and other services. In this research, we use this bus as an escaping path for deadlock recovery technique. According to simulation results, this bus is suitable platform for deadlock recovery technique.",2012-09-17T07:14:15Z,http://arxiv.org/pdf/1209.3564v1,2024-04-28,
1210.6338v1,"A Ternary Digital to Analog Converter with High Power Output and 170-dB Dynamic Range","A prototype of a very high dynamic range 32-bits Digital to Analog Converter (DAC) was designed and built for the purpose of direct auditory stimulus generation. It provides signals from less than 100 nV up to 50 Watts peak power output, driving a 32-Ohms earphone or speaker. The use of ternary cells makes possible a 170 dB dynamic range that is basically limited by thermal noise only.",2012-10-22T15:11:08Z,http://arxiv.org/pdf/1210.6338v1,2024-04-28,
1211.5248v1,"Design Of A Reconfigurable DSP Processor With Bit Efficient Residue Number System","Residue Number System (RNS), which originates from the Chinese Remainder Theorem, offers a promising future in VLSI because of its carry-free operations in addition, subtraction and multiplication. This property of RNS is very helpful to reduce the complexity of calculation in many applications. A residue number system represents a large integer using a set of smaller integers, called residues. But the area overhead, cost and speed not only depend on this word length, but also the selection of moduli, which is a very crucial step for residue system. This parameter determines bit efficiency, area, frequency etc. In this paper a new moduli set selection technique is proposed to improve bit efficiency which can be used to construct a residue system for digital signal processing environment. Subsequently, it is theoretically proved and illustrated using examples, that the proposed solution gives better results than the schemes reported in the literature. The novelty of the architecture is shown by comparison the different schemes reported in the literature. Using the novel moduli set, a guideline for a Reconfigurable Processor is presented here that can process some predefined functions. As RNS minimizes the carry propagation, the scheme can be implemented in Real Time Signal Processing & other fields where high speed computations are required.",2012-11-22T10:46:25Z,http://arxiv.org/pdf/1211.5248v1,2024-04-28,
1301.0051v1,"MIMS: Towards a Message Interface based Memory System","Memory system is often the main bottleneck in chipmultiprocessor (CMP) systems in terms of latency, bandwidth and efficiency, and recently additionally facing capacity and power problems in an era of big data. A lot of research works have been done to address part of these problems, such as photonics technology for bandwidth, 3D stacking for capacity, and NVM for power as well as many micro-architecture level innovations. Many of them need a modification of current memory architecture, since the decades-old synchronous memory architecture (SDRAM) has become an obstacle to adopt those advances. However, to the best of our knowledge, none of them is able to provide a universal memory interface that is scalable enough to cover all these problems.   In this paper, we argue that a message-based interface should be adopted to replace the traditional bus-based interface in memory system. A novel message interface based memory system (MIMS) is proposed. The key innovation of MIMS is that processor and memory system communicate through a universal and flexible message interface. Each message packet could contain multiple memory requests or commands along with various semantic information. The memory system is more intelligent and active by equipping with a local buffer scheduler, which is responsible to process packet, schedule memory requests, and execute specific commands with the help of semantic information. The experimental results by simulator show that, with accurate granularity message, the MIMS would improve performance by 53.21%, while reducing energy delay product (EDP) by 55.90%, the effective bandwidth utilization is improving by 62.42%. Furthermore, combining multiple requests in a packet would reduce link overhead and provide opportunity for address compression.",2013-01-01T03:19:24Z,http://arxiv.org/pdf/1301.0051v1,2024-04-28,
1301.1465v2,"A joint communication and application simulator for NoC-based SoCs","NoCs have become a widespread paradigm in the system-on-chip design world, not only for multi-purpose SoCs, but also for application-specific ICs. The common approach in the NoC design world is to separate the design of the interconnection from the design of the processing elements: this is well suited for a large number of developments, but the need for joint application and NoC design is not uncommon, especially in the application specific case. The correlation between processing and communication tasks can be strong, and separate or trace-based simulations fall often short of the desired precision. In this work, the OMNET++ based JANoCS simulator is presented: concurrent simulation of processing and communication allow cycle-accurate evaluation of the system. Two cases of study are presented, showing both the need for joint simulations and the effectiveness of JANoCS.",2013-01-08T09:59:28Z,http://arxiv.org/pdf/1301.1465v2,2024-04-28,
1301.3281v1,"Reconfiguration Strategies for Online Hardware Multitasking in Embedded Systems","An intensive use of reconfigurable hardware is expected in future embedded systems. This means that the system has to decide which tasks are more suitable for hardware execution. In order to make an efficient use of the FPGA it is convenient to choose one that allows hardware multitasking, which is implemented by using partial dynamic reconfiguration. One of the challenges for hardware multitasking in embedded systems is the online management of the only reconfiguration port of present FPGA devices. This paper presents different online reconfiguration scheduling strategies which assign the reconfiguration interface resource using different criteria: workload distribution or task deadline. The online scheduling strategies presented take efficient and fast decisions based on the information available at each moment. Experiments have been made in order to analyze the performance and convenience of these reconfiguration strategies.",2013-01-15T09:51:33Z,http://arxiv.org/pdf/1301.3281v1,2024-04-28,
1304.0835v1,"Improved Analytical Delay Models for RC-Coupled Interconnects","As the process technologies scale into deep submicron region, crosstalk delay is becoming increasingly severe, especially for global on-chip buses. To cope with this problem, accurate delay models of coupled interconnects are needed. In particular, delay models based on analytical approaches are desirable, because they not only are largely transparent to technology, but also explicitly establish the connections between delays of coupled interconnects and transition patterns, thereby enabling crosstalk alleviating techniques such as crosstalk avoidance codes (CACs). Unfortunately, existing analytical delay models, such as the widely cited model in [1], have limited accuracy and do not account for loading capacitance. In this paper, we propose analytical delay models for coupled interconnects that address these disadvantages. By accounting for more wires and eschewing the Elmore delay, our delay models achieve better accuracy than the model in [1].",2013-04-03T03:52:52Z,http://arxiv.org/pdf/1304.0835v1,2024-04-28,
1304.5081v1,"Open Tiled Manycore System-on-Chip","Manycore System-on-Chip include an increasing amount of processing elements and have become an important research topic for improvements of both hardware and software. While research can be conducted using system simulators, prototyping requires a variety of components and is very time consuming. With the Open Tiled Manycore System-on-Chip (OpTiMSoC) we aim at building such an environment for use in our and other research projects as prototyping platform.   This paper describes the project goals and aspects of OpTiMSoC and summarizes the current status and ideas.",2013-04-18T11:00:50Z,http://arxiv.org/pdf/1304.5081v1,2024-04-28,
1305.3038v1,"Phase-Priority based Directory Coherence for Multicore Processor","As the number of cores in a single chip increases, a typical implementation of coherence protocol adds significant hardware and complexity overhead. Besides, the performance of CMP system depends on the data access latency, which is highly affected by coherence protocol and on-chip interconnect. In this paper, we propose PPB (Phase-Priority Based) cache coherence protocol, an optimization of modern directory coherence protocol. We take advantage of the observation that transient states occur in directory coherence protocol, resulting in some unnecessary transient states and stalling. PPB cache coherence protocol decouples a coherence transaction and introduces the idea of phase message. This phase is considered as the priority of the message. Additionally, we also add new priority-based arbitrators in on-chip network to support PPB cache coherence protocol. This mechanism in on-chip network can support effective cache access, which makes the on-chip network more efficient. Our analysis on an execution-driven full system simulator using SPLASH-2 benchmark shows that PPB cache coherence outperforms a MESI based directory, and the number of unnecessary transient states and stalling reduces up to 24%. Also it reported the speedup of 7.4%. Other advantages of this strategy are reduced delay of flits and significantly less energy consumption in on-chip network.",2013-05-14T06:56:10Z,http://arxiv.org/pdf/1305.3038v1,2024-04-28,
1306.0089v1,"A Novel Reconfigurable Architecture of a DSP Processor for Efficient Mapping of DSP Functions using Field Programmable DSP Arrays","Development of modern integrated circuit technologies makes it feasible to develop cheaper, faster and smaller special purpose signal processing function circuits. Digital Signal processing functions are generally implemented either on ASICs with inflexibility, or on FPGAs with bottlenecks of relatively smaller utilization factor or lower speed compared to ASIC. Field Programmable DSP Array (FPDA) is the proposed DSP dedicated device, redolent to FPGA, but with basic fixed common modules (CMs) (like adders, subtractors, multipliers, scaling units, shifters) instead of CLBs. This paper introduces the development of reconfigurable system architecture with a focus on FPDA that integrates different DSP functions like DFT, FFT, DCT, FIR, IIR, and DWT etc. The switching between DSP functions is occurred by reconfiguring the interconnection between CMs. Validation of the proposed architecture has been achieved on Virtex5 FPGA. The architecture provides sufficient amount of flexibility, parallelism and scalability.",2013-06-01T09:04:40Z,http://arxiv.org/pdf/1306.0089v1,2024-04-28,
1306.1889v1,"An Improved Structure Of Reversible Adder And Subtractor","In today's world everyday a new technology which is faster, smaller and more complex than its predecessor is being developed. The increased number of transistors packed onto a chip of a conventional system results in increased power consumption that is why Reversible logic has drawn attention of Researchers due to its less heat dissipating characteristics. Reversible logic can be imposed over applications such as quantum computing, optical computing, quantum dot cellular automata, low power VLSI circuits, DNA computing. This paper presents the reversible combinational circuit of adder, subtractor and parity preserving subtractor. The suggested circuit in this paper are designed using Feynman, Double Feynman and MUX gates which are better than the existing one in literature in terms of Quantum cost, Garbage output and Total logical calculations.",2013-06-08T07:21:22Z,http://arxiv.org/pdf/1306.1889v1,2024-04-28,
1306.3109v2,"Computer Architecture with Associative Processor Replacing Last Level Cache and SIMD Accelerator","This study presents a novel computer architecture where a last level cache and a SIMD accelerator are replaced by an Associative Processor. Associative Processor combines data storage and data processing and provides parallel computational capabilities and data memory at the same time. An analytic performance model of the new computer architecture is introduced. Comparative analysis supported by simulation shows that this novel architecture may outperform a conventional architecture comprising a SIMD coprocessor and a shared last level cache while consuming less power.",2013-06-13T14:02:13Z,http://arxiv.org/pdf/1306.3109v2,2024-04-28,
1306.5501v1,"A Wrapper of PCI Express with FIFO Interfaces based on FPGA","This paper proposes a PCI Express (PCIE) Wrapper core named PWrapper with FIFO interfaces. Compared with other PCIE solutions, PWrapper has several advantages such as flexibility, isolation of clock domain, etc. PWrapper is implemented and verified on Vertex -5-FX70T which is a development board provided by Xilinx Inc. Architecture of PWrapper and design of two key modules are illustrated, which timing optimization methods have been adopted. Then we explained the advantages and challenges of on-chip interfaces technology based on FIFOs. The verification results show that PWrapper can achieve the speed of 1.8Gbps (Giga bits per second).",2013-06-24T02:50:32Z,http://arxiv.org/pdf/1306.5501v1,2024-04-28,
1307.3324v1,"Power efficient carry propagate adder","Here we describe the design details and performance of proposed Carry Propagate Adder based on GDI technique. GDI technique is power efficient technique for designing digital circuit that consumes less power as compare to most commonly used CMOS technique. GDI also has an advantage of minimum propagation delay, minimum area required and less complexity for designing any digital circuit. We designed Carry Propagate Adder using GDI technique and compared its performance with CMOS technique in terms of area, delay and power dissipation. Circuit designed using CADENCE EDA tool and simulated using SPECTRE VIRTUOSO tool at 0.18m technology. Comparative performance result shows that Carry Propagate Adder using GDI technique dissipated 55.6% less power as compare to Carry Propagate Adder using CMOS technique.",2013-07-12T04:57:14Z,http://arxiv.org/pdf/1307.3324v1,2024-04-28,
1307.3853v1,"Thermal analysis of 3D associative processor","Thermal density and hot spots limit three-dimensional (3D) implementation of massively-parallel SIMD processors and prohibit stacking DRAM dies above them. This study proposes replacing SIMD by an Associative Processor (AP). AP exhibits close to uniform thermal distribution with reduced hot spots. Additionally, AP may outperform SIMD processor when the data set size is sufficiently large, while dissipating less power. Comparative performance and thermal analysis supported by simulation confirm that AP might be preferable over SIMD for 3D implementation of large scale massively parallel processing engines combined with 3D DRAM integration.",2013-07-15T08:44:19Z,http://arxiv.org/pdf/1307.3853v1,2024-04-28,
1307.6406v1,"Relative Performance of a Multi-level Cache with Last-Level Cache Replacement: An Analytic Review","Current day processors employ multi-level cache hierarchy with one or two levels of private caches and a shared last-level cache (LLC). An efficient cache replacement policy at LLC is essential for reducing the off-chip memory transfer as well as conflict for memory bandwidth. Cache replacement techniques for inclusive LLCs may not be efficient for multilevel cache as it can be shared by enormous applications with varying access behavior, running simultaneously. One application may dominate another by flooding of cache requests and evicting the useful data of the other application. From the performance point of view, an exclusive LLC make the replacement policies more demanding, as compared to an inclusive LLC. This paper analyzes some of the existing replacement techniques on the LLC with their performance assessment.",2013-07-24T13:09:43Z,http://arxiv.org/pdf/1307.6406v1,2024-04-28,
1307.8319v1,"Allocating the chains of consecutive additions for optimal fixed-point data path synthesis","Minimization of computational errors in the fixed-point data path is often difficult task. Many signal processing algorithms use chains of consecutive additions. The analyzing technique that can be applied to fixed-point data path synthesis has been proposed. This technique takes advantage of allocating the chains of consecutive additions in order to predict growing width of the data path and minimize the design complexity and computational errors.",2013-07-31T13:40:40Z,http://arxiv.org/pdf/1307.8319v1,2024-04-28,
1307.8401v1,"FpSynt: a fixed-point datapath synthesis tool for embedded systems","Digital mobile systems must function with low power, small size and weight, and low cost. High-performance desktop microprocessors, with built-in floating point hardware, are not suitable in these cases. For embedded systems, it can be advantageous to implement these calculations with fixed point arithmetic instead. We present an automated fixed-point data path synthesis tool FpSynt for designing embedded applications in fixed-point domain with sufficient accuracy for most applications. FpSynt is available under the GNU General Public License from the following GitHub repository: http://github.com/izhbannikov/FPSYNT",2013-07-31T17:45:54Z,http://arxiv.org/pdf/1307.8401v1,2024-04-28,
1308.6021v1,"Selective Decoding in Associative Memories Based on Sparse-Clustered Networks","Associative memories are structures that can retrieve previously stored information given a partial input pattern instead of an explicit address as in indexed memories. A few hardware approaches have recently been introduced for a new family of associative memories based on Sparse-Clustered Networks (SCN) that show attractive features. These architectures are suitable for implementations with low retrieval latency, but are limited to small networks that store a few hundred data entries. In this paper, a new hardware architecture of SCNs is proposed that features a new data-storage technique as well as a method we refer to as Selective Decoding (SD-SCN). The SD-SCN has been implemented using a similar FPGA used in the previous efforts and achieves two orders of magnitude higher capacity, with no error-performance penalty but with the cost of few extra clock cycles per data access.",2013-08-28T00:37:56Z,http://arxiv.org/pdf/1308.6021v1,2024-04-28,
1309.2458v1,"Low power-area designs of 1bit full adder in cadence virtuoso platform","Power consumption has emerged as a primary design constraint for integrated circuits (ICs). In the Nano meter technology regime, leakage power has become a major component of total power. Full adder is the basic functional unit of an ALU. The power consumption of a processor is lowered by lowering the power consumption of an ALU, and the power consumption of an ALU can be lowered by lowering the power consumption of Full adder. So the full adder designs with low power characteristics are becoming more popular these days. This proposed work illustrates the design of the low-power less transistor full adder designs using cadence tool and virtuoso platform, the entire simulations have been done on 180nm single n-well CMOS bulk technology, in virtuoso platform of cadence tool with the supply voltage 1.8V and frequency of 100MHz. These circuits consume less power with maximum (6T design)of 93.1% power saving compare to conventional 28T design and 80.2% power saving compare to SERF design without much delay degradation. The proposed circuit exploits the advantage of GDI technique and pass transistor logic",2013-09-10T11:19:15Z,http://arxiv.org/pdf/1309.2458v1,2024-04-28,
1309.3685v1,"On the Performance Potential of Speculative Execution based on Branch and Value Prediction","Fluid Stochastic Petri Nets are used to capture the dynamic behavior of an ILP processor, and discrete-event simulation is applied to assess the performance potential of predictions and speculative execution in boosting the performance of ILP processors that fetch, issue, execute and commit a large number of instructions per cycle.",2013-09-14T16:43:05Z,http://arxiv.org/pdf/1309.3685v1,2024-04-28,
1309.3785v1,"Energy Saving Techniques for Phase Change Memory (PCM)","In recent years, the energy consumption of computing systems has increased and a large fraction of this energy is consumed in main memory. Towards this, researchers have proposed use of non-volatile memory, such as phase change memory (PCM), which has low read latency and power; and nearly zero leakage power. However, the write latency and power of PCM are very high and this, along with limited write endurance of PCM present significant challenges in enabling wide-spread adoption of PCM. To address this, several architecture-level techniques have been proposed. In this report, we review several techniques to manage power consumption of PCM. We also classify these techniques based on their characteristics to provide insights into them. The aim of this work is encourage researchers to propose even better techniques for improving energy efficiency of PCM based main memory.",2013-09-15T18:32:21Z,http://arxiv.org/pdf/1309.3785v1,2024-04-28,
1309.5459v1,"Advances in computer architecture","In the past, efforts were taken to improve the performance of a processor via frequency scaling. However, industry has reached the limits of increasing the frequency and therefore concurrent execution of instructions on multiple cores seems the only possible option. It is not enough to provide concurrent execution by the hardware, software also have to introduce concurrency in order to exploit the parallelism.",2013-09-21T10:33:36Z,http://arxiv.org/pdf/1309.5459v1,2024-04-28,
1309.5507v1,"Microgrid - The microthreaded many-core architecture","Traditional processors use the von Neumann execution model, some other processors in the past have used the dataflow execution model. A combination of von Neuman model and dataflow model is also tried in the past and the resultant model is referred as hybrid dataflow execution model. We describe a hybrid dataflow model known as the microthreading. It provides constructs for creation, synchronization and communication between threads in an intermediate language. The microthreading model is an abstract programming and machine model for many-core architecture. A particular instance of this model is named as the microthreaded architecture or the Microgrid. This architecture implements all the concurrency constructs of the microthreading model in the hardware with the management of these constructs in the hardware.",2013-09-21T17:50:09Z,http://arxiv.org/pdf/1309.5507v1,2024-04-28,
1309.5551v1,"Design space exploration in the microthreaded many-core architecture","Design space exploration is commonly performed in embedded system, where the architecture is a complicated piece of engineering. With the current trend of many-core systems, design space exploration in general-purpose computers can no longer be avoided. Microgrid is a complicated architecture, and therefor we need to perform design space exploration. Generally, simulators are used for the design space exploration of an architecture. Different simulators with different levels of complexity, simulation time and accuracy are used. Simulators with little complexity, low simulation time and reasonable accuracy are desirable for the design space exploration of an architecture. These simulators are referred as high-level simulators and are commonly used in the design of embedded systems. However, the use of high-level simulation for design space exploration in general-purpose computers is a relatively new area of research.",2013-09-22T02:01:28Z,http://arxiv.org/pdf/1309.5551v1,2024-04-28,
1309.5647v1,"A Cache-Coloring Based Technique for Saving Leakage Energy In Multitasking Systems","There has been a significant increase in leakage energy dissipation of CMOS circuits with each technology generation. Further, due to their large size, last level caches (LLCs) spend a large fraction of their energy in the form of leakage energy and hence, addressing this has become extremely important to meet the challenges of chip power budget. For addressing this, several techniques have been proposed. However, most of these techniques require offline profiling and hence cannot be used for real-life systems which usually run multitasking programs, with possible pre-emptions. In this paper, we propose a dynamic profiling based technique for saving cache leakage energy in multitasking systems. Our technique uses a small coloring-based profiling cache, to estimate performance and energy consumption of multiple cache configurations and then selects the best (least-energy) configuration among them. Our technique uses non-intrusive profiling and saves energy despite intra-task and inter-task variations; thus, it is suitable for multitasking systems. Simulations performed using workloads from SPEC2006 suite show the superiority of our technique over an existing cache energy saving technique. With a 2MB baseline cache, the average saving in memory sub-system energy is 22.8%.",2013-09-22T20:44:49Z,http://arxiv.org/pdf/1309.5647v1,2024-04-28,
1309.7082v1,"A Cache Reconfiguration Approach for Saving Leakage and Refresh Energy in Embedded DRAM Caches","In recent years, the size and leakage energy consumption of large last level caches (LLCs) has increased. To address this, embedded DRAM (eDRAM) caches have been considered which have lower leakage energy consumption; however eDRAM caches consume a significant amount of energy in the form of refresh energy. In this paper, we present a technique for saving both leakage and refresh energy in eDRAM caches. We use dynamic cache reconfiguration approach to intelligently turn-off part of the cache to save leakage energy and refresh only valid data of the active (i.e. not turned-off) cache to save refresh energy. We evaluate our technique using an x86-64 simulator and SPEC2006 benchmarks and compare it with a recently proposed technique for saving refresh energy, named Refrint. The experiments have shown that our technique provides better performance and energy efficiency than Refrint. Using our technique, for a 2MB LLC and 40 micro-seconds eDRAM refresh period, the average saving in energy over eDRAM baseline (which periodically refreshes all cache lines) is 22.8%.",2013-09-26T22:18:43Z,http://arxiv.org/pdf/1309.7082v1,2024-04-28,
1309.7163v1,"A Low-Voltage, Low-Power 4-bit BCD Adder, designed using the Clock Gated Power Gating, and the DVT Scheme","This paper proposes a Low-Power, Energy Efficient 4-bit Binary Coded Decimal (BCD) adder design where the conventional 4-bit BCD adder has been modified with the Clock Gated Power Gating Technique. Moreover, the concept of DVT (Dual-vth) scheme has been introduced while designing the full adder blocks to reduce the Leakage Power, as well as, to maintain the overall performance of the entire circuit. The reported architecture of 4-bit BCD adder is designed using 45 nm technology and it consumes 1.384 {\mu}Watt of Average Power while operating with a frequency of 200 MHz, and a Supply Voltage (Vdd) of 1 Volt. The results obtained from different simulation runs on SPICE, indicate the superiority of the proposed design compared to the conventional 4-bit BCD adder. Considering the product of Average Power and Delay, for the operating frequency of 200 MHz, a fair 47.41 % reduction compared to the conventional design has been achieved with this proposed scheme.",2013-09-27T09:09:37Z,http://arxiv.org/pdf/1309.7163v1,2024-04-28,
1309.7321v1,"Recycled Error Bits: Energy-Efficient Architectural Support for Higher Precision Floating Point","In this work, we provide energy-efficient architectural support for floating point accuracy. Our goal is to provide accuracy that is far greater than that provided by the processor's hardware floating point unit (FPU). Specifically, for each floating point addition performed, we ""recycle"" that operation's error: the difference between the finite-precision result produced by the hardware and the result that would have been produced by an infinite-precision FPU. We make this error architecturally visible such that it can be used, if desired, by software. Experimental results on physical hardware show that software that exploits architecturally recycled error bits can achieve accuracy comparable to a 2B-bit FPU with performance and energy that are comparable to a B-bit FPU.",2013-09-27T18:24:32Z,http://arxiv.org/pdf/1309.7321v1,2024-04-28,
1309.7818v1,"Partial Sums Generation Architecture for Successive Cancellation Decoding of Polar Codes","Polar codes are a new family of error correction codes for which efficient hardware architectures have to be defined for the encoder and the decoder. Polar codes are decoded using the successive cancellation decoding algorithm that includes partial sums computations. We take advantage of the recursive structure of polar codes to introduce an efficient partial sums computation unit that can also implements the encoder. The proposed architecture is synthesized for several codelengths in 65nm ASIC technology. The area of the resulting design is reduced up to 26% and the maximum working frequency is improved by ~25%.",2013-09-30T12:20:47Z,http://arxiv.org/pdf/1309.7818v1,2024-04-28,
1310.0100v1,"Technical report: Functional Constraint Extraction From Register Transfer Level for ATPG","We proposed in ""Functional Constraint Extraction From Register Transfer Level for ATPG"" that is currently submitted to TVLSI, an automatic functional constraint extractor that can be applied on the RT level. These functional constraints are used to generate pseudo functional test patterns with ATPG tools. The patterns are then used to improve the verification process. This technical report complements the work proposed as it contains the implementation details of the proposed methodology and shows the detailed intermediate and final results of the application of this methodology on a concrete example.",2013-10-01T00:06:15Z,http://arxiv.org/pdf/1310.0100v1,2024-04-28,
1310.3356v1,"A Novel Reconfigurable Computing Architecture for Image Signal Processing Using Circuit-Switched NoC and Synchronous Dataflow Model","In this paper, a novel reconfigurable architecture is proposed for multifunctional image signal processing systems. A circuit-switched NoC is used to provide interconnection because the non-TMD links ensure fixed throughput, which is a desirable behavior for computational intensive image processing algorithms compared with packet-switched NoC. Image processing algorithms are modeled as synchronous dataflow graphs which provide a unified model for general computing procedure. An image processing system is considered as several temporally mutually exclusive algorithms. Thus, their dataflow graph representations could be considered as a group and a merging algorithm could be applied to generate a union graph while eliminating spatial redundancy for area consumption optimization. After the union graph have been mapped and routed on the NoC, the reconfigurable system could be configured to any of its target image processing algorithms by properly setting the NoC topology. Experiments show the demo reconfigurable system with two image processing applications cost 26.4% less hardware resource, compared with the non-reconfigurable implementations.",2013-10-12T10:05:07Z,http://arxiv.org/pdf/1310.3356v1,2024-04-28,
1310.4231v1,"Dynamic cache reconfiguration based techniques for improving cache energy efficiency","Modern multicore processors are employing large last-level caches, for example Intel's E7-8800 processor uses 24MB L3 cache. Further, with each CMOS technology generation, leakage energy has been dramatically increasing and hence, leakage energy is expected to become a major source of energy dissipation, especially in last-level caches (LLCs). The conventional schemes of cache energy saving either aim at saving dynamic energy or are based on properties specific to first-level caches, and thus these schemes have limited utility for last-level caches. Further, several other techniques require offline profiling or per-application tuning and hence are not suitable for product systems. In this research, we propose novel cache leakage energy saving schemes for single-core and multicore systems; desktop, QoS, real-time and server systems. We propose software-controlled, hardware-assisted techniques which use dynamic cache reconfiguration to configure the cache to the most energy efficient configuration while keeping the performance loss bounded. To profile and test a large number of potential configurations, we utilize low-overhead, micro-architecture components, which can be easily integrated into modern processor chips. We adopt a system-wide approach to save energy to ensure that cache reconfiguration does not increase energy consumption of other components of the processor. We have compared our techniques with the state-of-art techniques and have found that our techniques outperform them in their energy efficiency. This research has important applications in improving energy-efficiency of higher-end embedded, desktop, server processors and multitasking systems. We have also proposed performance estimation approach for efficient design space exploration and have implemented time-sampling based simulation acceleration approach for full-system architectural simulators.",2013-10-16T00:41:12Z,http://arxiv.org/pdf/1310.4231v1,2024-04-28,
1310.7792v1,"Evaluating Cache Coherent Shared Virtual Memory for Heterogeneous Multicore Chips","The trend in industry is towards heterogeneous multicore processors (HMCs), including chips with CPUs and massively-threaded throughput-oriented processors (MTTOPs) such as GPUs. Although current homogeneous chips tightly couple the cores with cache-coherent shared virtual memory (CCSVM), this is not the communication paradigm used by any current HMC. In this paper, we present a CCSVM design for a CPU/MTTOP chip, as well as an extension of the pthreads programming model, called xthreads, for programming this HMC. Our goal is to evaluate the potential performance benefits of tightly coupling heterogeneous cores with CCSVM.",2013-10-29T13:03:38Z,http://arxiv.org/pdf/1310.7792v1,2024-04-28,
1310.8494v1,"Using Cache-coloring to Mitigate Inter-set Write Variation in Non-volatile Caches","In recent years, researchers have explored use of non-volatile devices such as STT-RAM (spin torque transfer RAM) for designing on-chip caches, since they provide high density and consume low leakage power. A common limitation of all non-volatile devices is their limited write endurance. Further, since existing cache management policies are write-variation unaware, excessive writes to a few blocks may lead to a quick failure of the whole cache. We propose an architectural technique for wear-leveling of non-volatile last level caches (LLCs). Our technique uses cache-coloring approach which adds a software-controlled mapping layer between groups of physical pages and cache sets. Periodically the mapping is altered to ensure that write-traffic can be spread uniformly to different sets of the cache to achieve wear-leveling. Simulations performed with an x86-64 simulator and SPEC2006 benchmarks show that our technique reduces the worst-case writes to cache blocks and thus improves the cache lifetime by 4.07X.",2013-10-31T13:33:16Z,http://arxiv.org/pdf/1310.8494v1,2024-04-28,
1312.0885v4,"Row-Based Dual Vdd Assignment, for a Level Converter Free CSA Design and Its Near-Threshold Operation","Subthreshold circuit designs are very much popular for some of the ultra low power applications, where the minimum energy consumption is the primary concern. But, due to the weak driving current, these circuits generally suffer from huge performance degradation. Therefore, in this paper, we primarily targeted to analyze the performance of a Near-Threshold Circuit (NTC), which retains the excellent energy efficiency of the subthreshold design, while improving the performance to a certain extent. A modified row-based dual Vdd 4-operand CSA (Carry Save Adder) design has been reported in the present work using 45 nm technology. Moreover, to find out the effectiveness of the near-threshold operation of the 4-operand CSA design; it has been compared with the other design styles. From the simulation results, obtained for the frequency of 20 MHz, we found that the proposed scheme of CSA design consumes 3.009*10-7 Watt of Average Power (Pavg), which is almost 90.9 % lesser than that of the conventional CSA design. Whereas, looking at the perspective of maximum delay at output, the proposed scheme of CSA design provides a fair 44.37 % improvement, compared to that of the subthreshold CSA design.",2013-11-29T09:48:57Z,http://arxiv.org/pdf/1312.0885v4,2024-04-28,
1312.2207v2,"A Cache Energy Optimization Technique for STT-RAM Last Level Cache","Last level caches (LLCs) occupy a large chip-area and there size is expected to grow further to offset the limitations of memory bandwidth and speed. Due to high leakage consumption of SRAM device, caches designed with SRAM consume large amount of energy. To address this, use of emerging technologies such as spin torque transfer RAM (STT-RAM) has been investigated which have lower leakage power dissipation. However, the high write latency and power of it may lead to large energy consumption which present challenges in its use. In this report, we propose a cache reconfiguration based technique for improving the energy efficiency of STT-RAM based LLCs. Our technique dynamically adjusts the active cache size to reduce the cache leakage energy consumption with minimum performance loss. We choose a suitable value of STT-RAM retention time for avoiding refresh overhead and gaining performance. Single-core simulations have been performed using SPEC2006 benchmarks and Sniper x86-64 simulator. The results show that while, compared to an STT-RAM LLC of similar area, an SRAM LLC incurs nearly 100% loss in energy and 7.3% loss in performance; our technique using STT-RAM cache saves 21.8% energy and incurs only 1.7% loss in performance.",2013-12-08T13:03:15Z,http://arxiv.org/pdf/1312.2207v2,2024-04-28,
1312.2976v1,"A Survey of Network-On-Chip Tools","Nowadays System-On-Chips (SoCs) have evolved considerably in term of performances, reliability and integration capacity. The last advantage has induced the growth of the number of cores or Intellectual Properties (IPs) in a same chip. Unfortunately, this important number of IPs has caused a new issue which is the intra-communication between the elements of a same chip. To resolve this problem, a new paradigm has been introduced which is the Network-On-Chip (NoC). Since the introduction of the NoC paradigm in the last decade, new methodologies and approaches have been presented by research community and many of them have been adopted by industrials. The literature contains many relevant studies and surveys discussing NoC proposals and contributions. However, few of them have discussed or proposed a comparative study of NoC tools. The objective of this work is to establish a reliable survey about available design, simulation or implementation NoC tools. We collected an important amount of information and characteristics about NoC dedicated tools that we will present throughout this survey. This study is built around a respectable amount of references and we hope it will help scientists.",2013-12-10T21:39:01Z,http://arxiv.org/pdf/1312.2976v1,2024-04-28,
1401.0765v1,"A Survey of Techniques For Improving Energy Efficiency in Embedded Computing Systems","Recent technological advances have greatly improved the performance and features of embedded systems. With the number of just mobile devices now reaching nearly equal to the population of earth, embedded systems have truly become ubiquitous. These trends, however, have also made the task of managing their power consumption extremely challenging. In recent years, several techniques have been proposed to address this issue. In this paper, we survey the techniques for managing power consumption of embedded systems. We discuss the need of power management and provide a classification of the techniques on several important parameters to highlight their similarities and differences. This paper is intended to help the researchers and application-developers in gaining insights into the working of power management techniques and designing even more efficient high-performance embedded systems of tomorrow.",2014-01-04T02:06:23Z,http://arxiv.org/pdf/1401.0765v1,2024-04-28,
1401.1003v1,"On the likelihood of multiple bit upsets in logic circuits","Soft errors have a significant impact on the circuit reliability at nanoscale technologies. At the architectural level, soft errors are commonly modeled by a probabilistic bit-flip model. In developing such abstract fault models, an important issue to consider is the likelihood of multiple bit errors caused by particle strikes. This likelihood has been studied to a great extent in memories, but has not been understood to the same extent in logic circuits. In this paper, we attempt to quantify the likelihood that a single transient event can cause multiple bit errors in logic circuits consisting of combinational gates and flip-flops. In particular, we calculate the conditional probability of multiple bit-flips given that a single bit flips as a result of the transient. To calculate this conditional probability, we use a Monte Carlo technique in which samples are generated using detailed post-layout circuit simulations. Our experiments on the ISCAS'85 benchmarks and a few other circuits indicate that, this conditional probability is quite significant and can be as high as 0.31. Thus we conclude that multiple bit-flips must necessarily be considered in order to obtain a realistic architectural fault model for soft errors.",2014-01-06T07:36:13Z,http://arxiv.org/pdf/1401.1003v1,2024-04-28,
1401.2727v1,"Hardware Implementation of four byte per clock RC4 algorithm","In the field of cryptography till date the 2-byte in 1-clock is the best known RC4 hardware design [1], while 1-byte in 1-clock [2], and the 1-byte in 3 clocks [3][4] are the best known implementation. The design algorithm in[2] considers two consecutive bytes together and processes them in 2 clocks. The design [1] is a pipelining architecture of [2]. The design of 1-byte in 3-clocks is too much modular and clock hungry. In this paper considering the RC4 algorithm, as it is, a simpler RC4 hardware design providing higher throughput is proposed in which 6 different architecture has been proposed. In design 1, 1-byte is processed in 1-clock, design 2 is a dynamic KSA-PRGA architecture of Design 1. Design 3 can process 2 byte in a single clock, where as Design 4 is Dynamic KSA-PRGA architecture of Design 3. Design 5 and Design 6 are parallelization architecture design 2 and design 4 which can compute 4 byte in a single clock. The maturity in terms of throughput, power consumption and resource usage, has been achieved from design 1 to design 6. The RC4 encryption and decryption designs are respectively embedded on two FPGA boards as co-processor hardware, the communication between the two boards performed using Ethernet.",2014-01-13T06:57:21Z,http://arxiv.org/pdf/1401.2727v1,2024-04-28,
1401.2732v1,"Fault Detection for RC4 Algorithm and its Implementation on FPGA Platform","In hardware implementation of a cryptographic algorithm, one may achieve leakage of secret information by creating scopes to introduce controlled faulty bit(s) even though the algorithm is mathematically a secured one. The technique is very effective in respect of crypto processors embedded in smart cards. In this paper few fault detecting architectures for RC4 algorithm are designed and implemented on Virtex5(ML505, LX110t) FPGA board. The results indicate that the proposed architectures can handle most of the faults without loss of throughput consuming marginally additional hardware and power.",2014-01-13T07:19:52Z,http://arxiv.org/pdf/1401.2732v1,2024-04-28,
1401.2768v1,"Design of novel architectures and field programmable gate arrays implementation of two dimensional gaussian surround function","A new design and novel architecture suitable for FPGA/ASIC implementation of a 2D Gaussian surround function for image processing application is presented in this paper. The proposed scheme results in enormous savings of memory normally required for 2D Gaussian function implementation. In the present work, the Gaussian symmetric characteristics which quickly falls off toward plus/minus infinity has been used in order to save the memory. The 2D Gaussian function implementation is presented for use in applications such as image enhancement, smoothing, edge detection and filtering etc. The FPGA implementation of the proposed 2D Gaussian function is capable of processing (blurring, smoothing, and convolution) high resolution color pictures of size up to $1600\times1200$ pixels at the real time video rate of 30 frames/sec. The Gaussian design exploited here has been used in the core part of retinex based color image enhancement. Therefore, the design presented produces Gaussian output with three different scales, namely, 16, 64 and 128. The design was coded in Verilog, a popular hardware design language used in industries, conforming to RTL coding guidelines and fits onto a single chip with a gate count utilization of 89,213 gates. Experimental results presented confirms that the proposed method offers a new approach for development of large sized Gaussian pyramid while reducing the on-chip memory utilization.",2014-01-13T09:52:08Z,http://arxiv.org/pdf/1401.2768v1,2024-04-28,
1401.4629v1,"HERMES: A Hierarchical Broadcast-Based Silicon Photonic Interconnect for Scalable Many-Core Systems","Optical interconnection networks, as enabled by recent advances in silicon photonic device and fabrication technology, have the potential to address on-chip and off-chip communication bottlenecks in many-core systems. Although several designs have shown superior power efficiency and performance compared to electrical alternatives, these networks will not scale to the thousands of cores required in the future.   In this paper, we introduce Hermes, a hybrid network composed of an optimized broadcast for power-efficient low-latency global-scale coordination and circuit-switch sub-networks for high-throughput data delivery. This network will scale for use in thousand core chip systems. At the physical level, SoI-based adiabatic coupler has been designed to provide low-loss and compact optical power splitting. Based on the adiabatic coupler, a topology based on 2-ary folded butterfly is designed to provide linear power division in a thousand core layout with minimal cross-overs. To address the network agility and provide for efficient use of optical bandwidth, a flow control and routing mechanism is introduced to dynamically allocate bandwidth and provide fairness usage of network resources. At the system level, bloom filter-based filtering for localization of communication are designed for reducing global traffic. In addition, a novel greedy-based data and workload migration are leveraged to increase the locality of communication in a NUCA (non-uniform cache access) architecture. First order analytic evaluation results have indicated that Hermes is scalable to at least 1024 cores and offers significant performance improvement and power savings over prior silicon photonic designs.",2014-01-19T03:20:40Z,http://arxiv.org/pdf/1401.4629v1,2024-04-28,
1401.4891v1,"The Design of a Network-On-Chip Architecture Based On An Avionic Protocol","When the Network-On-Chip (NoC) paradigm was introduced, many researchers have proposed many novelistic NoC architectures, tools and design strategies. In this paper we introduce a new approach in the field of designing Network-On-Chip (NoC). Our inspiration came from an avionic protocol which is the AFDX protocol. The proposed NoC architecture is a switch centric architecture, with exclusive shortcuts between hosts and utilizes the flexibility, the reliability and the performances offered by AFDX.",2014-01-20T13:20:58Z,http://arxiv.org/pdf/1401.4891v1,2024-04-28,
1401.6370v1,"Design of a High Speed XAUI Based on Dynamic Reconfigurable Transceiver IP Core","By using the dynamic reconfigurable transceiver in high speed interface design, designer can solve critical technology problems such as ensuring signal integrity conveniently, with lower error binary rate. In this paper, we designed a high speed XAUI (10Gbps Ethernet Attachment Unit Interface) to transparently extend the physical reach of the XGMII. The following points are focused: (1) IP (Intellectual Property) core usage. Altera Co. offers two transceiver IP cores in Quartus II MegaWizard Plug-In Manager for XAUI design which is featured of dynamic reconfiguration performance, that is, ALTGX_RECO?FIG instance and ALTGX instance, we can get various groups by changing settings of the devices without power off. These two blocks can accomplish function of PCS (Physical Coding Sub-layer) and PMA (Physical Medium Attachment), however, with higher efficiency and reliability. (2) 1+1 protection. In our design, two ALTGX IP cores are used to work in parallel, which named XAUI0 and XAUI1. The former works as the main channel while the latter redundant channel. When XAUI0 is out of service for some reasons, XAUI1 will start to work to keep the business. (3) RTL (Register Transfer Level) coding with Verilog HDL and simulation. Create the ALTGX_RECO?FIG instance and ALTGX instance, enable dynamic reconfiguration in the ALTGXB Megafunction, then connect the ALTGX_RECO?FIG with the ALTGX instances. After RTL coding, the design was simulated on VCS simulator. The validated result indicates that the packets are transferred efficiently. FPGA makes high-speed optical communication system design simplified.",2014-01-24T15:14:52Z,http://arxiv.org/pdf/1401.6370v1,2024-04-28,
1404.1602v2,"A high-level model of embedded flash energy consumption","The alignment of code in the flash memory of deeply embedded SoCs can have a large impact on the total energy consumption of a computation. We investigate the effect of code alignment in six SoCs and find that a large proportion of this energy (up to 15% of total SoC energy consumption) can be saved by changes to the alignment.   A flexible model is created to predict the read-access energy consumption of flash memory on deeply embedded SoCs, where code is executed in place. This model uses the instruction level memory accesses performed by the processor to calculate the flash energy consumption of a sequence of instructions. We derive the model parameters for five SoCs and validate them. The error is as low as 5%, with a 11% average normalized RMS deviation overall.   The scope for using this model to optimize code alignment is explored across a range of benchmarks and SoCs. Analysis shows that over 30% of loops can be better aligned. This can significantly reduce energy while increasing code size by less than 4%. We conclude that this effect has potential as an effective optimization, saving significant energy in deeply embedded SoCs.",2014-04-06T17:34:23Z,http://arxiv.org/pdf/1404.1602v2,2024-04-28,
1404.3162v1,"A Signal Processor for Gaussian Message Passing","In this paper, we present a novel signal processing unit built upon the theory of factor graphs, which is able to address a wide range of signal processing algorithms. More specifically, the demonstrated factor graph processor (FGP) is tailored to Gaussian message passing algorithms. We show how to use a highly configurable systolic array to solve the message update equations of nodes in a factor graph efficiently. A proper instruction set and compilation procedure is presented. In a recursive least squares channel estimation example we show that the FGP can compute a message update faster than a state-ofthe- art DSP. The results demonstrate the usabilty of the FGP architecture as a flexible HW accelerator for signal-processing and communication systems.",2014-04-11T17:28:54Z,http://arxiv.org/pdf/1404.3162v1,2024-04-28,
1404.3381v1,"Modeling the Temperature Bias of Power Consumption for Nanometer-Scale CPUs in Application Processors","We introduce and experimentally validate a new macro-level model of the CPU temperature/power relationship within nanometer-scale application processors or system-on-chips. By adopting a holistic view, this model is able to take into account many of the physical effects that occur within such systems. Together with two algorithms described in the paper, our results can be used, for instance by engineers designing power or thermal management units, to cancel the temperature-induced bias on power measurements. This will help them gather temperature-neutral power data while running multiple instance of their benchmarks. Also power requirements and system failure rates can be decreased by controlling the CPU's thermal behavior.   Even though it is usually assumed that the temperature/power relationship is exponentially related, there is however a lack of publicly available physical temperature/power measurements to back up this assumption, something our paper corrects. Via measurements on two pertinent platforms sporting nanometer-scale application processors, we show that the power/temperature relationship is indeed very likely exponential over a 20{\deg}C to 85{\deg}C temperature range. Our data suggest that, for application processors operating between 20{\deg}C and 50{\deg}C, a quadratic model is still accurate and a linear approximation is acceptable.",2014-04-13T13:07:36Z,http://arxiv.org/pdf/1404.3381v1,2024-04-28,
1404.3877v1,"Design space exploration for image processing architectures on FPGA targets","Due to the emergence of embedded applications in image and video processing, communication and cryptography, improvement of pictorial information for better human perception like deblurring, denoising in several fields such as satellite imaging, medical imaging, mobile applications etc. are gaining importance for renewed research. Behind such developments, the primary responsibility lies with the advancement of semiconductor technology leading to FPGA based programmable logic devices, which combines the advantages of both custom hardware and dedicated DSP resources. In addition, FPGA provides powerful reconfiguration feature and hence is an ideal target for rapid prototyping. We have endeavoured to exploit exceptional features of FPGA technology in respect to hardware parallelism leading to higher computational density and throughput, and have observed better performances than those one can get just merely porting the image processing software algorithms to hardware. In this paper, we intend to present an elaborate review, based on our expertise and experiences, on undertaking necessary transformation to an image processing software algorithm including the optimization techniques that makes its operation in hardware comparatively faster.",2014-04-15T11:46:50Z,http://arxiv.org/pdf/1404.3877v1,2024-04-28,
1404.4629v2,"A Survey of Methods For Analyzing and Improving GPU Energy Efficiency","Recent years have witnessed a phenomenal growth in the computational capabilities and applications of GPUs. However, this trend has also led to dramatic increase in their power consumption. This paper surveys research works on analyzing and improving energy efficiency of GPUs. It also provides a classification of these techniques on the basis of their main research idea. Further, it attempts to synthesize research works which compare energy efficiency of GPUs with other computing systems, e.g. FPGAs and CPUs. The aim of this survey is to provide researchers with knowledge of state-of-the-art in GPU power management and motivate them to architect highly energy-efficient GPUs of tomorrow.",2014-04-17T19:57:51Z,http://arxiv.org/pdf/1404.4629v2,2024-04-28,
1404.5885v1,"Hardware Efficient WiMAX Deinterleaver Capable of Address Generation for Random Interleaving Depths","The variation in the prescribed modulation schemes and code rates for WiMAX interleaver design, as defined by IEEE 802.16 standard, demands a plethora of hardware if all the modulation schemes and code rates have to be unified into a single electronic device. Add to this the complexities involved with the algorithms and permutations of the WiMAX standard, invariably dependent on floor function which is extremely hardware inefficient. This paper is an attempt towards removing the complexities and excess hardware involvement in the implementation of the permutations involved in Deinterleaver designs as defined by IEEE 802.16",2014-04-22T12:38:22Z,http://arxiv.org/pdf/1404.5885v1,2024-04-28,
1404.5929v1,"FPGA design of a cdma2000 turbo decoder","This paper presents the FPGA hardware design of a turbo decoder for the cdma2000 standard. The work includes a study and mathematical analysis of the turbo decoding process, based on the MAX-Log-MAP algorithm. Results of decoding for a packet size of two hundred fifty bits are presented, as well as an analysis of area versus performance, and the key variables for hardware design in turbo decoding.",2014-04-23T19:09:19Z,http://arxiv.org/pdf/1404.5929v1,2024-04-28,
1406.1886v1,"The Z1: Architecture and Algorithms of Konrad Zuse's First Computer","This paper provides the first comprehensive description of the Z1, the mechanical computer built by the German inventor Konrad Zuse in Berlin from 1936 to 1938. The paper describes the main structural elements of the machine, the high-level architecture, and the dataflow between components. The computer could perform the four basic arithmetic operations using floating-point numbers. Instructions were read from punched tape. A program consisted of a sequence of arithmetical operations, intermixed with memory store and load instructions, interrupted possibly by input and output operations. Numbers were stored in a mechanical memory. The machine did not include conditional branching in the instruction set. While the architecture of the Z1 is similar to the relay computer Zuse finished in 1941 (the Z3) there are some significant differences. The Z1 implements operations as sequences of microinstructions, as in the Z3, but does not use rotary switches as micro-steppers. The Z1 uses a digital incrementer and a set of conditions which are translated into microinstructions for the exponent and mantissa units, as well as for the memory blocks. Microinstructions select one out of 12 layers in a machine with a 3D mechanical structure of binary mechanical elements. The exception circuits for mantissa zero, necessary for normalized floating-point, were lacking; they were first implemented in the Z3. The information for this article was extracted from careful study of the blueprints drawn by Zuse for the reconstruction of the Z1 for the German Technology Museum in Berlin, from some letters, and from sketches in notebooks. Although the machine has been in exhibition since 1989 (non-operational), no detailed high-level description of the machine's architecture had been available. This paper fills that gap.",2014-06-07T11:23:31Z,http://arxiv.org/pdf/1406.1886v1,2024-04-28,
1406.4628v1,"An Efficient Synchronous Static Memory design for Embedded System","Custom memory organization are challenging task in the area of VLSI design. This study aims to design high speed and low power consumption memory for embedded system. Synchronous SRAM has been proposed and analyzed using various simulators. Xilinx simulator simulates the Synchronous SRAM memories which can perform efficient read/write capability for embedded systems. Xinix tool also provide the access time that required selecting a word and reading it. Synchronous Static RAM which has easily read /writes capability and performs scheduled read /writes operation in efficient manner.",2014-06-18T07:50:50Z,http://arxiv.org/pdf/1406.4628v1,2024-04-28,
1406.5000v1,"Application Specific Cache Simulation Analysis for Application Specific Instruction set Processor","An Efficient Simulation of application specific instruction-set processors (ASIP) is a challenging onus in the area of VLSI design. This paper reconnoiters the possibility of use of ASIP simulators for ASIP Simulation. This proposed study allow as the simulation of the cache memory design with various ASIP simulators like Simple scalar and VEX. In this paper we have implemented the memory configuration according to desire application. These simulators performs the cache related results such as cache name, sets, cache associativity, cache block size, cache replacement policy according to specific application.",2014-06-19T10:40:23Z,http://arxiv.org/pdf/1406.5000v1,2024-04-28,
1406.7662v1,"Selective Match-Line Energizer Content Addressable Memory(SMLE -CAM)","A Content Addressable Memory (CAM) is a memory primarily designed for high speed search operation. Parallel search scheme forms the basis of CAM, thus power reduction is the challenge associated with a large amount of parallel active circuits. We are presenting a novel algorithm and architecture described as Selective Match-Line Energizer Content Addressable Memory (SMLE-CAM) which energizes only those MLs (Match-Line) whose first three bits are conditionally matched with corresponding first three search bit using special architecture which comprises of novel XNOR-CAM cell and novel XOR-CAM cell. The rest of the CAM chain is followed by NOR-CAM cell. The 256 X 144 bit SMLE-CAM is implemented in TSMC 90 nm technology and its robustness across PVT variation is verified. The post-layout simulation result shows, it has energy metric of 0.115 fJ/bit/search with search time 361.6 ps, the best reported so far. The maximum operating frequency is 1GHz.",2014-06-30T10:46:50Z,http://arxiv.org/pdf/1406.7662v1,2024-04-28,
1407.0147v1,"Application Specific Hardware Design Simulation for High Performance Embedded System","Application specific simulation is challenging task in various real time high performance embedded devices. In this study specific application is implemented with the help of Xilinx. Xilinx provides SDK and XPS tools, XPS tools used for develop complete hardware platform and SDK provides software platform for application creation and verification. Xilinx XUP-5 board have been used and implemented various specific Applications with hardware platform. In this study the base instruction set with customized instructions, supported with specific hardware resources are analyzed.",2014-07-01T09:10:26Z,http://arxiv.org/pdf/1407.0147v1,2024-04-28,
1407.2082v1,"FPGA Based Efficient Multiplier for Image Processing Applications Using Recursive Error Free Mitchell Log Multiplier and KOM Architecture","The Digital Image processing applications like medical imaging, satellite imaging, Biometric trait images etc., rely on multipliers to improve the quality of image. However, existing multiplication techniques introduce errors in the output with consumption of more time, hence error free high speed multipliers has to be designed. In this paper we propose FPGA based Recursive Error Free Mitchell Log Multiplier (REFMLM) for image Filters. The 2x2 error free Mitchell log multiplier is designed with zero error by introducing error correction term is used in higher order Karastuba-Ofman Multiplier (KOM) Architectures. The higher order KOM multipliers is decomposed into number of lower order multipliers using radix 2 till basic multiplier block of order 2x2 which is designed by error free Mitchell log multiplier. The 8x8 REFMLM is tested for Gaussian filter to remove noise in fingerprint image. The Multiplier is synthesized using Spartan 3 FPGA family device XC3S1500-5fg320. It is observed that the performance parameters such as area utilization, speed, error and PSNR are better in the case of proposed architecture compared to existing architectures",2014-07-08T13:46:42Z,http://arxiv.org/pdf/1407.2082v1,2024-04-28,
1407.5173v1,"An ECG-SoC with 535nW/channel lossless data compression for wearable sensors","This paper presents a low power ECG recording Sys-tem-on-Chip (SoC) with on-chip low complexity lossless ECG compression for data reduction in wireless/ambulatory ECG sensor devices. The proposed algorithm uses a linear slope predictor to estimate the ECG samples, and uses a novel low complexity dynamic coding-packaging scheme to frame the resulting estimation error into fixed-length 16-bit format. The proposed technique achieves an average compression ratio of 2.25x on MIT/BIH ECG database. Implemented in 0.35 {\mu}m process, the compressor uses 0.565 K gates/channel occupying 0.4 mm2 for 4-channel, and consumes 535 nW/channel at 2.4V for ECG sampled at 512 Hz. Small size and ultra-low power consumption makes the proposed technique suitable for wearable ECG sensor application.",2014-07-19T11:42:48Z,http://arxiv.org/pdf/1407.5173v1,2024-04-28,
1408.0982v1,"Modeling and simulation of multiprocessor systems MPSoC by SystemC/TLM2","The current manufacturing technology allows the integration of a complex multiprocessor system on one piece of silicon (MPSoC for Multiprocessor System-on- Chip). One way to manage the growing complexity of these systems is to increase the level of abstraction and to address the system-level design. In this paper, we focus on the implementation in SystemC language with TLM (Transaction Level Model) to model an MPSOC platform. Our main contribution is to define a comprehensive, fast and accurate method for designing and evaluating performance for MPSoC systems. The studied MPSoC is composed of MicroBlaze microprocessors, memory, a timer, a VGA and an interrupt handler with two examples of software. This paper has two novel contributions: the first is to develop this MPSOC at CABA and TLM for ISS (Instruction Set Simulator), Native simulations and timed Programmer s View (PV+T); the second is to show that with PV+T simulations we can achieve timing fidelity with higher speeds than CABA simulations and have almost the same precision.",2014-08-05T14:19:21Z,http://arxiv.org/pdf/1408.0982v1,2024-04-28,
1408.5401v1,"A Many-Core Overlay for High-Performance Embedded Computing on FPGAs","In this work, we propose a configurable many-core overlay for high-performance embedded computing. The size of internal memory, supported operations and number of ports can be configured independently for each core of the overlay. The overlay was evaluated with matrix multiplication, LU decomposition and Fast-Fourier Transform (FFT) on a ZYNQ-7020 FPGA platform. The results show that using a system-level many-core overlay avoids complex hardware design and still provides good performance results.",2014-08-21T11:57:16Z,http://arxiv.org/pdf/1408.5401v1,2024-04-28,
1410.7560v1,"Multi Core SSL/TLS Security Processor Architecture Prototype Design with automated Preferential Algorithm in FPGA","In this paper a pipelined architecture of a high speed network security processor (NSP) for SSL,TLS protocol is implemented on a system on chip (SOC) where hardware information of all encryption, hashing and key exchange algorithms are stored in flash memory in terms of bit files, in contrary to related works where all are actually implemented in hardware. The NSP finds applications in e-commerce, virtual private network (VPN) and in other fields that require data confidentiality. The motivation of the present work is to dynamically execute applications with stipulated throughput within budgeted hardware resource and power. A preferential algorithm choosing an appropriate cipher suite is proposed, which is based on Efficient System Index (ESI) budget comprising of power, throughput and resource given by the user. The bit files of the chosen security algorithms are downloaded from the flash memory to the partial region of field programmable gate array (FPGA). The proposed SOC controls data communication between an application running in a system through a PCI and the Ethernet interface of a network. Partial configuration feature is used in ISE14.4 suite with ZYNQ 7z020-clg484 FPGA platform. The performances",2014-10-28T09:11:21Z,http://arxiv.org/pdf/1410.7560v1,2024-04-28,
1411.0863v1,"Inner Loop Optimizations in Mapping Single Threaded Programs to Hardware","In the context of mapping high-level algorithms to hardware, we consider the basic problem of generating an efficient hardware implementation of a single threaded program, in particular, that of an inner loop. We describe a control-flow mechanism which provides dynamic loop-pipelining capability in hardware, so that multiple iterations of an arbitrary inner loop can be made simultaneously active in the generated hardware, We study the impact of this loop-pipelining scheme in conjunction with source-level loop-unrolling. In particular, we apply this technique to some common loop kernels: regular kernels such as the fast-fourier transform and matrix multiplication, as well as an example of an inner loop whose body has branching. The resulting resulting hardware descriptions are synthesized to an FPGA target, and then characterized for performance and resource utilization. We observe that the use of dynamic loop-pipelining mechanism alone typically results in a significant improvements in the performance of the hardware. If the loop is statically unrolled and if loop-pipelining is applied to the unrolled program, then the performance improvement is still substantial. When dynamic loop pipelining is used in conjunction with static loop unrolling, the improvement in performance ranges from 6X to 20X (in terms of number of clock cycles needed for the computation) across the loop kernels that we have studied. These optimizations do have a hardware overhead, but, in spite of this, we observe that the joint use of these loop optimizations not only improves performance, but also the performance/cost ratio of the resulting hardware.",2014-11-04T11:26:58Z,http://arxiv.org/pdf/1411.0863v1,2024-04-28,
1411.2088v1,"Energy Efficient Full Adder Cell Design With Using Carbon Nanotube Field Effect Transistors In 32 Nanometer Technology","Full Adder is one of the critical parts of logical and arithmetic units. So, presenting a low power full adder cell reduces the power consumption of the entire circuit. Also, using Nano-scale transistors, because of their unique characteristics will save energy consumption and decrease the chip area. In this paper we presented a low power full adder cell by using carbon nanotube field effect transistors (CNTFETs). Simulation results were carried out using HSPICE based on the CNTFET model in 32 nanometer technology in Different values of temperature and VDD.",2014-11-08T05:57:42Z,http://arxiv.org/pdf/1411.2088v1,2024-04-28,
1411.2212v1,"Designing high-speed, low-power full adder cells based on carbon nanotube technology","This article presents novel high speed and low power full adder cells based on carbon nanotube field effect transistor (CNFET). Four full adder cells are proposed in this article. First one (named CN9P4G) and second one (CN9P8GBUFF) utilizes 13 and 17 CNFETs respectively. Third design that we named CN10PFS uses only 10 transistors and is full swing. Finally, CN8P10G uses 18 transistors and divided into two modules, causing Sum and Cout signals are produced in a parallel manner. All inputs have been used straight, without inverting. These designs also used the special feature of CNFET that is controlling the threshold voltage by adjusting the diameters of CNFETs to achieve the best performance and right voltage levels. All simulation performed using Synopsys HSPICE software and the proposed designs are compared to other classical and modern CMOS and CNFET-based full adder cells in terms of delay, power consumption and power delay product.",2014-11-09T09:26:43Z,http://arxiv.org/pdf/1411.2212v1,2024-04-28,
1411.2917v1,"Fast Prefix Adders for Non-Uniform Input Arrival Times","We consider the problem of constructing fast and small parallel prefix adders for non-uniform input arrival times. This problem arises whenever the adder is embedded into a more complex circuit, e. g. a multiplier.   Most previous results are based on representing binary carry-propagate adders as so-called parallel prefix graphs, in which pairs of generate and propagate signals are combined using complex gates known as prefix gates. Adders constructed in this model usually minimize the delay in terms of these prefix gates. However, the delay in terms of logic gates can be worse by a factor of two.   In contrast, we aim to minimize the delay of the underlying logic circuit directly. We prove a lower bound on the delay of a carry bit computation achievable by any prefix carry bit circuit and develop an algorithm that computes a prefix carry bit circuit with optimum delay up to a small additive constant. Furthermore, we use this algorithm to construct a small parallel prefix adder.   Compared to existing algorithms we simultaneously improve the delay and size guarantee, as well as the running time for constructing prefix carry bit and adder circuits.",2014-11-11T18:52:26Z,http://arxiv.org/pdf/1411.2917v1,2024-04-28,
1411.3492v1,"Evaluation of silicon consumption for a connectionless Network-on-Chip","We present the design and evaluation of a predictable Network-on-Chip (NoC) to interconnect processing units running multimedia applications with variable-bit-rate. The design is based on a connectionless strategy in which flits from different communication flows are interleaved in the same communication channel between routers. Each flit carries routing information used by routers to perform arbitration and scheduling of the corresponding output communication channel. Analytic comparisons show that our approach keeps average latency lower than a network based on resource reservation, when both networks are working over 80% of offered load. We also evaluate the proposed NoC on FPGA and ASIC technologies to understand the trade-off due to our approach, in terms of silicon consumption.",2014-11-13T10:37:18Z,http://arxiv.org/pdf/1411.3492v1,2024-04-28,
1411.3929v1,"Analog Signal Processing Solution for Image Alignment","Imaging and Image sensors is a field that is continuously evolving. There are new products coming into the market every day. Some of these have very severe Size, Weight and Power constraints whereas other devices have to handle very high computational loads. Some require both these conditions to be met simultaneously. Current imaging architectures and digital image processing solutions will not be able to meet these ever increasing demands. There is a need to develop novel imaging architectures and image processing solutions to address these requirements. In this work we propose analog signal processing as a solution to this problem. The analog processor is not suggested as a replacement to a digital processor but it will be used as an augmentation device which works in parallel with the digital processor, making the system faster and more efficient. In order to show the merits of analog processing the highly computational Normalized Cross Correlation algorithm is implemented. We propose two novel modifications to the algorithm and a new imaging architecture which, significantly reduces the computation time.",2014-11-14T15:07:54Z,http://arxiv.org/pdf/1411.3929v1,2024-04-28,
1411.6498v1,"Correction to the 2005 paper: ""Digit Selection for SRT Division and Square Root""","It has been pointed out by counterexamples in a 2013 paper in the IEEE Transactions on Computers [1], that there is an error in the previously ibid.\ in 2005 published paper [2] on the construction of valid digit selection tables for SRT type division and square root algorithms. The error has been corrected, and new results found on selection constants for maximally redundant digit sets.",2014-11-24T16:00:47Z,http://arxiv.org/pdf/1411.6498v1,2024-04-28,
1412.1140v1,"Sphynx: A Shared Instruction Cache Exporatory Study","The Sphynx project was an exploratory study to discover what might be done to improve the heavy replication of in- structions in independent instruction caches for a massively parallel machine where a single program is executing across all of the cores. While a machine with only many cores (fewer than 50) might not have any issues replicating the instructions for each core, as we approach the era where thousands of cores can be placed on one chip, the overhead of instruction replication may become unacceptably large. We believe that a large amount of sharing should be possible when the ma- chine is configured for all of the threads to issue from the same set of instructions. We propose a technique that allows sharing an instruction cache among a number of independent processor cores to allow for inter-thread sharing and reuse of instruction memory. While we do not have test cases to demonstrate the potential magnitude of performance gains that could be achieved, the potential for sharing reduces the die area required for instruction storage on chip.",2014-12-03T00:16:43Z,http://arxiv.org/pdf/1412.1140v1,2024-04-28,
1412.2950v1,"Performance Enhancement of Routers in Networks-on-Chip Using Dynamic Virtual Channels Allocation","This study proposes a new router architecture to improve the performance of dynamic allocation of virtual channels. The proposed router is designed to reduce the hardware complexity and to improve power and area consumption, simultaneously. In the new structure of the proposed router, all of the controlling components have been implemented sequentially inside the allocator router modules. This optimizes communications between the controlling components and eliminates the most of hardware overloads of modular communications. Eliminating additional communications also reduces the hardware complexity. In order to show the validity of the proposed design in real hardware resources, the proposed router has been implemented onto a Field-Programmable Gate Array (FPGA). Since the implementation of a Network-on-Chip (NoC) requires certain amount of area on the chip, the suggested approach is also able to reduce the demand of hardware resources. In this method, the internal memory of the FPGA is used for implementing control units. This memory is faster and can be used with specific patterns. The use of the FPGA memory saves the hardware resources and allows the implementation of NoC based FPGA.",2014-12-09T13:14:29Z,http://arxiv.org/pdf/1412.2950v1,2024-04-28,
1412.3224v1,"Prophet: A Speculative Multi-threading Execution Model with Architectural Support Based on CMP","Speculative multi-threading (SpMT) has been proposed as a perspective method to exploit Chip Multiprocessors (CMP) hardware potential. It is a thread level speculation (TLS) model mainly depending on software and hardware co-design. This paper researches speculative thread-level parallelism of general purpose programs and a speculative multi-threading execution model called Prophet is presented. The architectural support for Prophet execution model is designed based on CMP. In Prophet the inter-thread data dependency are predicted by pre-computation slice (p-slice) to reduce RAW violation. Prophet multi-versioning Cache system along with thread state control mechanism in architectural support are utilized for buffering the speculative data, and a snooping bus based cache coherence protocol is used to detect data dependence violation. The simulation-based evaluation shows that the Prophet system could achieve significant speedup for general-purpose programs.",2014-12-10T08:43:27Z,http://arxiv.org/pdf/1412.3224v1,2024-04-28,
1412.3829v5,"A High-Throughput Energy-Efficient Implementation of Successive-Cancellation Decoder for Polar Codes Using Combinational Logic","This paper proposes a high-throughput energy-efficient Successive Cancellation (SC) decoder architecture for polar codes based on combinational logic. The proposed combinational architecture operates at relatively low clock frequencies compared to sequential circuits, but takes advantage of the high degree of parallelism inherent in such architectures to provide a favorable tradeoff between throughput and energy efficiency at short to medium block lengths. At longer block lengths, the paper proposes a hybrid-logic SC decoder that combines the advantageous aspects of the combinational decoder with the low-complexity nature of sequential-logic decoders. Performance characteristics on ASIC and FPGA are presented with a detailed power consumption analysis for combinational decoders. Finally, the paper presents an analysis of the complexity and delay of combinational decoders, and of the throughput gains obtained by hybrid-logic decoders with respect to purely synchronous architectures.",2014-12-11T21:29:49Z,http://arxiv.org/pdf/1412.3829v5,2024-04-28,
1412.6043v1,"A 237 Gbps Unrolled Hardware Polar Decoder","In this letter we present a new architecture for a polar decoder using a reduced complexity successive cancellation decoding algorithm. This novel fully-unrolled, deeply-pipelined architecture is capable of achieving a coded throughput of over 237 Gbps for a (1024,512) polar code implemented using an FPGA. This decoder is two orders of magnitude faster than state-of-the-art polar decoders.",2014-12-18T20:07:22Z,http://arxiv.org/pdf/1412.6043v1,2024-04-28,
1412.7692v1,"A Feasibility Study on Programmer Specific Instruction Set Processors (PSISPs)","ASIPs are designed in order to execute instructions of a particular domain of applications. The designing of ASIPs addresses the major challenges faced by a system on chip such as size, cost, performance and energy consumption. The higher the number of similar instructions within the domain to be mapped the lesser the energy consumption, the smaller the size and the higher the performance of the ASIP. Thus, designing processors for domains with more similar programs would overcome these issues. This paper describes the investigation of whether the domains of programmer specific programs have any significance like application specific program domains and thus, whether the approach of designing processors known as Programmer Specific Instruction Set Processors is worthwhile. We performed the evaluation at the instruction level by using four different measures to obtain the similarity of programs: (1) by the existence of each instruction, (2) by the frequency of each instruction, (3) by two consecutive instruction patterns and (4) by three consecutive instruction patterns of application specific and programmer specific programs. We found that although programmer specific instructions show some impact on the similarity measures, they are much smaller and therefore insignificant compared to the impact from application specific programs.",2014-12-24T15:35:44Z,http://arxiv.org/pdf/1412.7692v1,2024-04-28,
1503.02354v1,"A General Scheme for Noise-Tolerant Logic Design Based on Probabilistic and DCVS Approaches","In this paper, a general circuit scheme for noise-tolerant logic design based on Markov Random Field theory and differential Cascade Voltage Switch technique has been proposed, which is an extension of the work in [1-3], [4]. A block with only four transistors has been successfully inserted to the original circuit scheme from [3] and extensive simulation results show that our proposed design can operate correctly with the input signal of 1 dB signal-noise-ratio. When using the evaluation parameter from [5], the output value of our design decreases by 76.5% on average than [3] which means that superior noise-immunity could be obtained through our work.",2015-03-09T01:36:50Z,http://arxiv.org/pdf/1503.02354v1,2024-04-28,
1503.03166v1,"Design of High Performance MIPS Cryptography Processor Based on T-DES Algorithm","The paper describes the design of high performance MIPS Cryptography processor based on triple data encryption standard. The organization of pipeline stages in such a way that pipeline can be clocked at high frequency. Encryption and Decryption blocks of triple data encryption standard (T-DES) crypto system and dependency among themselves are explained in detail with the help of block diagram. In order to increase the processor functionality and performance, especially for security applications we include three new 32-bit instructions LKLW, LKUW and CRYPT. The design has been synthesized at 40nm process technology targeting using Xilinx Virtex-6 device. The overall MIPS Crypto processor works at 209MHz.",2015-03-08T18:23:32Z,http://arxiv.org/pdf/1503.03166v1,2024-04-28,
1503.03169v1,"Dynamic Partitioning of Physical Memory Among Virtual Machines, ASMI:Architectural Support for Memory Isolation","Cloud computing relies on secure and efficient virtualization. Software level security solutions compromise the performance of virtual machines (VMs), as a large amount of computational power would be utilized for running the security modules. Moreover, software solutions are only as secure as the level that they work on. For example a security module on a hypervisor cannot provide security in the presence of an infected hypervisor. It is a challenge for virtualization technology architects to enhance the security of VMs without degrading their performance. Currently available server machines are not fully equipped to support a secure VM environment without compromising on performance. A few hardware modifications have been introduced by manufactures like Intel and AMD to provide a secure VM environment with low performance degradation. In this paper we propose a novel memory architecture model named \textit{ Architectural Support for Memory Isolation(ASMI)}, that can achieve a true isolated physical memory region to each VM without degrading performance. Along with true memory isolation, ASMI is designed to provide lower memory access times, better utilization of available memory, support for DMA isolation and support for platform independence for users of VMs.",2015-03-10T09:38:51Z,http://arxiv.org/pdf/1503.03169v1,2024-04-28,
1503.04628v1,"Logic BIST: State-of-the-Art and Open Problems","Many believe that in-field hardware faults are too rare in practice to justify the need for Logic Built-In Self-Test (LBIST) in a design. Until now, LBIST was primarily used in safety-critical applications. However, this may change soon. First, even if costly methods like burn-in are applied, it is no longer possible to get rid of all latent defects in devices at leading-edge technology. Second, demands for high reliability spread to consumer electronics as smartphones replace our wallets and IDs. However, today many ASIC vendors are reluctant to use LBIST. In this paper, we describe the needs for successful deployment of LBIST in the industrial practice and discuss how these needs can be addressed. Our work is hoped to attract a wider attention to this important research topic.",2015-03-16T12:45:04Z,http://arxiv.org/pdf/1503.04628v1,2024-04-28,
1503.05694v2,"Improving GPU Performance Through Resource Sharing","Graphics Processing Units (GPUs) consisting of Streaming Multiprocessors (SMs) achieve high throughput by running a large number of threads and context switching among them to hide execution latencies. The number of thread blocks, and hence the number of threads that can be launched on an SM, depends on the resource usage--e.g. number of registers, amount of shared memory--of the thread blocks. Since the allocation of threads to an SM is at the thread block granularity, some of the resources may not be used up completely and hence will be wasted.   We propose an approach that shares the resources of SM to utilize the wasted resources by launching more thread blocks. We show the effectiveness of our approach for two resources: register sharing, and scratchpad (shared memory) sharing. We further propose optimizations to hide long execution latencies, thus reducing the number of stall cycles. We implemented our approach in GPGPU-Sim simulator and experimentally validated it on several applications from 4 different benchmark suites: GPGPU-Sim, Rodinia, CUDA-SDK, and Parboil. We observed that with register sharing, applications show maximum improvement of 24%, and average improvement of 11%. With scratchpad sharing, we observed a maximum improvement of 30% and an average improvement of 12.5%.",2015-03-19T10:21:42Z,http://arxiv.org/pdf/1503.05694v2,2024-04-28,
1504.03437v1,"Low-latency List Decoding Of Polar Codes With Double Thresholding","For polar codes with short-to-medium code length, list successive cancellation decoding is used to achieve a good error-correcting performance. However, list pruning in the current list decoding is based on the sorting strategy and its timing complexity is high. This results in a long decoding latency for large list size. In this work, aiming at a low-latency list decoding implementation, a double thresholding algorithm is proposed for a fast list pruning. As a result, with a negligible performance degradation, the list pruning delay is greatly reduced. Based on the double thresholding, a low-latency list decoding architecture is proposed and implemented using a UMC 90nm CMOS technology. Synthesis results show that, even for a large list size of 16, the proposed low-latency architecture achieves a decoding throughput of 220 Mbps at a frequency of 641 MHz.",2015-04-14T07:12:24Z,http://arxiv.org/pdf/1504.03437v1,2024-04-28,
1504.04297v1,"MigrantStore: Leveraging Virtual Memory in DRAM-PCM Memory Architecture","With the imminent slowing down of DRAM scaling, Phase Change Memory (PCM) is emerging as a lead alternative for main memory technology. While PCM achieves low energy due to various technology-specific advantages, PCM is significantly slower than DRAM (especially for writes) and can endure far fewer writes before wearing out. Previous work has proposed to use a large, DRAM-based hardware cache to absorb writes and provide faster access. However, due to ineffectual caching where blocks are evicted before sufficient number of accesses, hardware caches incur significant overheads in energy and bandwidth, two key but scarce resources in modern multicores. Because using hardware for detecting and removing such ineffectual caching would incur additional hardware cost and complexity, we leverage the OS virtual memory support for this purpose. We propose a DRAM-PCM hybrid memory architecture where the OS migrates pages on demand from the PCM to DRAM. We call the DRAM part of our memory as MigrantStore which includes two ideas. First, to reduce the energy, bandwidth, and wear overhead of ineffectual migrations, we propose migration hysteresis. Second, to reduce the software overhead of good replacement policies, we propose recently- accessed-page-id (RAPid) buffer, a hardware buffer to track the addresses of recently-accessed MigrantStore pages.",2015-04-16T16:36:14Z,http://arxiv.org/pdf/1504.04297v1,2024-04-28,
1504.04586v1,"A Reconfigurable Vector Instruction Processor for Accelerating a Convection Parametrization Model on FPGAs","High Performance Computing (HPC) platforms allow scientists to model computationally intensive algorithms. HPC clusters increasingly use General-Purpose Graphics Processing Units (GPGPUs) as accelerators; FPGAs provide an attractive alternative to GPGPUs for use as co-processors, but they are still far from being mainstream due to a number of challenges faced when using FPGA-based platforms. Our research aims to make FPGA-based high performance computing more accessible to the scientific community. In this work we present the results of investigating the acceleration of a particular atmospheric model, Flexpart, on FPGAs. We focus on accelerating the most computationally intensive kernel from this model. The key contribution of our work is the architectural exploration we undertook to arrive at a solution that best exploits the parallelism available in the legacy code, and is also convenient to program, so that eventually the compilation of high-level legacy code to our architecture can be fully automated. We present the three different types of architecture, comparing their resource utilization and performance, and propose that an architecture where there are a number of computational cores, each built along the lines of a vector instruction processor, works best in this particular scenario, and is a promising candidate for a generic FPGA-based platform for scientific computation. We also present the results of experiments done with various configuration parameters of the proposed architecture, to show its utility in adapting to a range of scientific applications.",2015-04-17T17:27:27Z,http://arxiv.org/pdf/1504.04586v1,2024-04-28,
1506.03160v1,"Simultaneous Multi Layer Access: A High Bandwidth and Low Cost 3D-Stacked Memory Interface","Limited memory bandwidth is a critical bottleneck in modern systems. 3D-stacked DRAM enables higher bandwidth by leveraging wider Through-Silicon-Via (TSV) channels, but today's systems cannot fully exploit them due to the limited internal bandwidth of DRAM. DRAM reads a whole row simultaneously from the cell array to a row buffer, but can transfer only a fraction of the data from the row buffer to peripheral IO circuit, through a limited and expensive set of wires referred to as global bitlines. In presence of wider memory channels, the major bottleneck becomes the limited data transfer capacity through these global bitlines. Our goal in this work is to enable higher bandwidth in 3D-stacked DRAM without the increased cost of adding more global bitlines. We instead exploit otherwise-idle resources, such as global bitlines, already existing within the multiple DRAM layers by accessing the layers simultaneously. Our architecture, Simultaneous Multi Layer Access (SMLA), provides higher bandwidth by aggregating the internal bandwidth of multiple layers and transferring the available data at a higher IO frequency.   To implement SMLA, simultaneous data transfer from multiple layers through the same IO TSVs requires coordination between layers to avoid channel conflict. We first study coordination by static partitioning, which we call Dedicated-IO, that assigns groups of TSVs to each layer. We then provide a simple, yet sophisticated mechanism, called Cascaded-IO, which enables simultaneous access to each layer by time-multiplexing the IOs. By operating at a frequency proportional to the number of layers, SMLA provides a higher bandwidth (4X for a four-layer stacked DRAM). Our evaluations show that SMLA provides significant performance improvement and energy reduction (55%/18% on average for multi-programmed workloads, respectively) over a baseline 3D-stacked DRAM with very low area overhead.",2015-06-10T04:14:29Z,http://arxiv.org/pdf/1506.03160v1,2024-04-28,
1506.03181v2,"DEW: A Fast Level 1 Cache Simulation Approach for Embedded Processors with FIFO Replacement Policy","Increasing the speed of cache simulation to obtain hit/miss rates en- ables performance estimation, cache exploration for embedded sys- tems and energy estimation. Previously, such simulations, particu- larly exact approaches, have been exclusively for caches which uti- lize the least recently used (LRU) replacement policy. In this paper, we propose a new, fast and exact cache simulation method for the First In First Out(FIFO) replacement policy. This method, called DEW, is able to simulate multiple level 1 cache configurations (dif- ferent set sizes, associativities, and block sizes) with FIFO replace- ment policy. DEW utilizes a binomial tree based representation of cache configurations and a novel searching method to speed up sim- ulation over single cache simulators like Dinero IV. Depending on different cache block sizes and benchmark applications, DEW oper- ates around 8 to 40 times faster than Dinero IV. Dinero IV compares 2.17 to 19.42 times more cache ways than DEW to determine accu- rate miss rates.",2015-06-10T06:14:33Z,http://arxiv.org/pdf/1506.03181v2,2024-04-28,
1506.03182v2,"TRISHUL: A Single-pass Optimal Two-level Inclusive Data Cache Hierarchy Selection Process for Real-time MPSoCs","Hitherto discovered approaches analyze the execution time of a real time application on all the possible cache hierarchy setups to find the application specific optimal two level inclusive data cache hierarchy to reduce cost, space and energy consumption while satisfying the time deadline in real time Multiprocessor Systems on Chip. These brute force like approaches can take years to complete. Alternatively, memory access trace driven crude estimation methods can find a cache hierarchy quickly by compromising the accuracy of results. In this article, for the first time, we propose a fast and accurate trace driven approach to find the optimal real time application specific two level inclusive data cache hierarchy. Our proposed approach TRISHUL predicts the optimal cache hierarchy performance first and then utilizes that information to find the optimal cache hierarchy quickly. TRISHUL can suggest a cache hierarchy, which has up to 128 times smaller size, up to 7 times faster compared to the suggestion of the state of the art crude trace driven two level inclusive cache hierarchy selection approach for the application traces analyzed.",2015-06-10T06:26:02Z,http://arxiv.org/pdf/1506.03182v2,2024-04-28,
1506.03186v2,"CIPARSim: Cache Intersection Property Assisted Rapid Single-pass FIFO Cache Simulation Technique","In this paper, for the first time, we introduce a cache property called the Intersection Property that helps to reduce singlepass simulation time in a manner similar to inclusion property. An intersection property defines conditions that if met, prove a particular element exists in larger caches, thus avoiding further search time. We have discussed three such intersection properties for caches using the FIFO replacement policy in this paper. A rapid singlepass FIFO cache simulator CIPARSim has also been proposed. CIPARSim is the first singlepass simulator dependent on the FIFO cache properties to reduce simulation time significantly. CIPARSim simulation time was up to 5 times faster compared to the state of the art singlepass FIFO cache simulator for the cache configurations tested. CIPARSim produces the cache hit and miss rates of an application accurately on various cache configurations. During simulation, CIPARSim intersection properties alone predict up to 90% of the total hits, reducing simulationtime immensely",2015-06-10T06:57:55Z,http://arxiv.org/pdf/1506.03186v2,2024-04-28,
1506.03193v2,"Accelerating Non-volatile/Hybrid Processor Cache Design Space Exploration for Application Specific Embedded Systems","In this article, we propose a technique to accelerate nonvolatile or hybrid of volatile and nonvolatile processor cache design space exploration for application specific embedded systems. Utilizing a novel cache behavior modeling equation and a new accurate cache miss prediction mechanism, our proposed technique can accelerate NVM or hybrid FIFO processor cache design space exploration for SPEC CPU 2000 applications up to 249 times compared to the conventional approach.",2015-06-10T07:14:45Z,http://arxiv.org/pdf/1506.03193v2,2024-04-28,
1507.01777v1,"FPGA based Novel High Speed DAQ System Design with Error Correction","Present state of the art applications in the area of high energy physics experiments (HEP), radar communication, satellite communication and bio medical instrumentation require fault resilient data acquisition (DAQ) system with the data rate in the order of Gbps. In order to keep the high speed DAQ system functional in such radiation environment where direct intervention of human is not possible, a robust and error free communication system is necessary. In this work we present an efficient DAQ design and its implementation on field programmable gate array (FPGA). The proposed DAQ system supports high speed data communication (~4.8 Gbps) and achieves multi-bit error correction capabilities. BCH code (named after Raj Bose and D. K. RayChaudhuri) has been used for multi-bit error correction. The design has been implemented on Xilinx Kintex-7 board and is tested for board to board communication as well as for board to PC using PCIe (Peripheral Component Interconnect express) interface. To the best of our knowledge, the proposed FPGA based high speed DAQ system utilizing optical link and multi-bit error resiliency can be considered first of its kind. Performance estimation of the implemented DAQ system is done based on resource utilization, critical path delay, efficiency and bit error rate (BER).",2015-07-07T12:24:25Z,http://arxiv.org/pdf/1507.01777v1,2024-04-28,
1507.03303v1,"Managing Hybrid Main Memories with a Page-Utility Driven Performance Model","Hybrid memory systems comprised of dynamic random access memory (DRAM) and non-volatile memory (NVM) have been proposed to exploit both the capacity advantage of NVM and the latency and dynamic energy advantages of DRAM. An important problem for such systems is how to place data between DRAM and NVM to improve system performance.   In this paper, we devise the first mechanism, called UBM (page Utility Based hybrid Memory management), that systematically estimates the system performance benefit of placing a page in DRAM versus NVM and uses this estimate to guide data placement. UBM's estimation method consists of two major components. First, it estimates how much an application's stall time can be reduced if the accessed page is placed in DRAM. To do this, UBM comprehensively considers access frequency, row buffer locality, and memory level parallelism (MLP) to estimate the application's stall time reduction. Second, UBM estimates how much each application's stall time reduction contributes to overall system performance. Based on this estimation method, UBM can determine and place the most critical data in DRAM to directly optimize system performance. Experimental results show that UBM improves system performance by 14% on average (and up to 39%) compared to the best of three state-of-the-art mechanisms for a large number of data-intensive workloads from the SPEC CPU2006 and Yahoo Cloud Serving Benchmark (YCSB) suites.",2015-07-13T01:47:16Z,http://arxiv.org/pdf/1507.03303v1,2024-04-28,
1508.06811v1,"Model-based Hardware Design for FPGAs using Folding Transformations based on Subcircuits","We present a tool flow and results for a model-based hardware design for FPGAs from Simulink descriptions which nicely integrates into existing environments. While current commercial tools do not exploit some high-level optimizations, we investigate the promising approach of using reusable subcircuits for folding transformations to control embedded multiplier usage and to optimize logic block usage. We show that resource improvements of up to 70% compared to the original model are possible, but it is also shown that subcircuit selection is a critical task. While our tool flow provides good results already, the investigation and optimization of subcircuit selection is clearly identified as an additional keypoint to extend high-level control on low-level FPGA mapping properties.",2015-08-27T11:37:36Z,http://arxiv.org/pdf/1508.06811v1,2024-04-28,
1508.06832v1,"Designing Hardware/Software Systems for Embedded High-Performance Computing","In this work, we propose an architecture and methodology to design hardware/software systems for high-performance embedded computing on FPGA. The hardware side is based on a many-core architecture whose design is generated automatically given a set of architectural parameters. Both the architecture and the methodology were evaluated running dense matrix multiplication and sparse matrix-vector multiplication on a ZYNQ-7020 FPGA platform. The results show that using a system-level design of the system avoids complex hardware design and still provides good performance results.",2015-08-27T12:44:22Z,http://arxiv.org/pdf/1508.06832v1,2024-04-28,
1508.07139v1,"Using System Hyper Pipelining (SHP) to Improve the Performance of a Coarse-Grained Reconfigurable Architecture (CGRA) Mapped on an FPGA","The well known method C-Slow Retiming (CSR) can be used to automatically convert a given CPU into a multithreaded CPU with independent threads. These CPUs are then called streaming or barrel processors. System Hyper Pipelining (SHP) adds a new flexibility on top of CSR by allowing a dynamic number of threads to be executed and by enabling the threads to be stalled, bypassed and reordered. SHP is now applied on the programming elements (PE) of a coarse-grained reconfigurable architecture (CGRA). By using SHP, more performance can be achieved per PE. Fork-Join operations can be implemented on a PE using the flexibility provided by SHP to dynamically adjust the number of threads per PE. Multiple threads can share the same data locally, which greatly reduces the data traffic load on the CGRA's routing structure. The paper shows the results of a CGRA using SHP-ed RISC-V cores as PEs implemented on a FPGA.",2015-08-28T09:19:57Z,http://arxiv.org/pdf/1508.07139v1,2024-04-28,
1509.03575v1,"FPGA Implementation of High Speed Baugh-Wooley Multiplier using Decomposition Logic","The Baugh-Wooley algorithm is a well-known iterative algorithm for performing multiplication in digital signal processing applications. Decomposition logic is used with Baugh-Wooley algorithm to enhance the speed and to reduce the critical path delay. In this paper a high speed multiplier is designed and implemented using decomposition logic and Baugh-Wooley algorithm. The result is compared with booth multiplier. FPGA based architecture is presented and design has been implemented using Xilinx 12.3 device.",2015-09-11T16:20:07Z,http://arxiv.org/pdf/1509.03575v1,2024-04-28,
1509.03721v1,"DReAM: Dynamic Re-arrangement of Address Mapping to Improve the Performance of DRAMs","The initial location of data in DRAMs is determined and controlled by the 'address-mapping' and even modern memory controllers use a fixed and run-time-agnostic address mapping. On the other hand, the memory access pattern seen at the memory interface level will dynamically change at run-time. This dynamic nature of memory access pattern and the fixed behavior of address mapping process in DRAM controllers, implied by using a fixed address mapping scheme, means that DRAM performance cannot be exploited efficiently. DReAM is a novel hardware technique that can detect a workload-specific address mapping at run-time based on the application access pattern which improves the performance of DRAMs. The experimental results show that DReAM outperforms the best evaluated address mapping on average by 9%, for mapping-sensitive workloads, by 2% for mapping-insensitive workloads, and up to 28% across all the workloads. DReAM can be seen as an insurance policy capable of detecting which scenarios are not well served by the predefined address mapping.",2015-09-12T08:02:39Z,http://arxiv.org/pdf/1509.03721v1,2024-04-28,
1509.03740v1,"HAPPY: Hybrid Address-based Page Policy in DRAMs","Memory controllers have used static page closure policies to decide whether a row should be left open, open-page policy, or closed immediately, close-page policy, after the row has been accessed. The appropriate choice for a particular access can reduce the average memory latency. However, since application access patterns change at run time, static page policies cannot guarantee to deliver optimum execution time. Hybrid page policies have been investigated as a means of covering these dynamic scenarios and are now implemented in state-of-the-art processors. Hybrid page policies switch between open-page and close-page policies while the application is running, by monitoring the access pattern of row hits/conflicts and predicting future behavior. Unfortunately, as the size of DRAM memory increases, fine-grain tracking and analysis of memory access patterns does not remain practical. We propose a compact memory address-based encoding technique which can improve or maintain the performance of DRAMs page closure predictors while reducing the hardware overhead in comparison with state-of-the-art techniques. As a case study, we integrate our technique, HAPPY, with a state-of-the-art monitor, the Intel-adaptive open-page policy predictor employed by the Intel Xeon X5650, and a traditional Hybrid page policy. We evaluate them across 70 memory intensive workload mixes consisting of single-thread and multi-thread applications. The experimental results show that using the HAPPY encoding applied to the Intel-adaptive page closure policy can reduce the hardware overhead by 5X for the evaluated 64 GB memory (up to 40X for a 512 GB memory) while maintaining the prediction accuracy.",2015-09-12T13:03:04Z,http://arxiv.org/pdf/1509.03740v1,2024-04-28,
1509.04268v1,"High Speed VLSI Architecture for 3-D Discrete Wavelet Transform","This paper presents a memory efficient, high throughput parallel lifting based running three dimensional discrete wavelet transform (3-D DWT) architecture. 3-D DWT is constructed by combining the two spatial and four temporal processors. Spatial processor (SP) apply the two dimensional DWT on a frame, using lifting based 9/7 filter bank through the row rocessor (RP) in row direction and then apply in the colum direction through column processor (CP). To reduce the temporal memory and the latency, the temporal processor (TP) has been designed with lifting based 1-D Haar wavelet filter. The proposed architecture replaced the multiplications by pipeline shift-add operations to reduce the CPD. Two spatial processors works simultaneously on two adjacent frames and provide 2-D DWT coefficients as inputs to the temporal processors. TPs apply the one dimensional DWT in temporal direction and provide eight 3-D DWT coefficients per clock (throughput). Higher throughput reduces the computing cycles per frame and enable the lower power consumption. Implementation results shows that the proposed architecture has the advantage in reduced memory, low power consumption, low latency, and high throughput over the existing designs. The RTL of the proposed architecture is described using verilog and synthesized using 90-nm technology CMOS standard cell library and results show that it consumes 43.42 mW power and occupies an area equivalent to 231.45 K equivalent gate at frequency of 200 MHz. The proposed architecture has also been synthesised for the Xilinx zynq 7020 series field programmable gate array (FPGA).",2015-09-14T03:46:00Z,http://arxiv.org/pdf/1509.04268v1,2024-04-28,
1509.04618v1,"Cost Efficient Design of Reversible Adder Circuits for Low Power Applications","A large amount of research is currently going on in the field of reversible logic, which have low heat dissipation, low power consumption, which is the main factor to apply reversible in digital VLSI circuit design. This paper introduces reversible gate named as Inventive0 gate. The novel gate is synthesis the efficient adder modules with minimum garbage output and gate count. The Inventive0 gate capable of implementing a 4-bit ripple carry adder and carry skip adders.It is presented that Inventive0 gate is much more efficient and optimized approach as compared to their existing design, in terms of gate count, garbage outputs and constant inputs. In addition, some popular available reversible gates are implemented in the MOS transistor design the implementation kept in mind for minimum MOS transistor count and are completely reversible in behavior more precise forward and backward computation. Lesser architectural complexity show that the novel designs are compact, fast as well as low power.",2015-09-11T16:38:57Z,http://arxiv.org/pdf/1509.04618v1,2024-04-28,
1509.06891v1,"A Novel Method for Soft Error Mitigation in FPGA using Adaptive Cross Parity Code","Field Programmable Gate Arrays (FPGAs) are more prone to be affected by transient faults in presence of radiation and other environmental hazards compared to Application Specific Integrated Circuits (ASICs). Hence, error mitigation and recovery techniques are absolutely necessary to protect the FPGA hardware from soft errors arising due to such transient faults. In this paper, a new efficient multi-bit error correcting method for FPGAs is proposed using adaptive cross parity check (ACPC) code. ACPC is easy to implement and the needed decoding circuit is also simple. In the proposed scheme total configuration memory is partitioned into two parts. One part will contain ACPC hardware, which is static and assumed to be unaffected by any kind of errors. Other portion will store the binary file for logic, which is to be protected from transient error and is assumed to be dynamically reconfigurable (Partial reconfigurable area). Binary file from the secondary memory passes through ACPC hardware and the bits for forward error correction (FEC) field are calculated before entering into the reconfigurable portion. In the runtime scenario, the data from the dynamically reconfigurable portion of the configuration memory will be read back and passed through the ACPC hardware. The ACPC hardware will correct the errors before the data enters into the dynamic configuration memory. We propose a first of its kind methodology for novel transient fault correction using ACPC code for FPGAs. To validate the design we have tested the proposed methodology with Kintex FPGA. We have also measured different parameters like critical path, power consumption, overhead resource and error correction efficiency to estimate the performance of our proposed method.",2015-09-23T09:02:49Z,http://arxiv.org/pdf/1509.06891v1,2024-04-28,
1509.08111v3,"Automatic latency balancing in VHDL-implemented complex pipelined systems","Balancing (equalization) of latency in parallel paths in the pipelined data processing system is an important problem. Without that the data from different paths arrive at the processing blocks in different clock cycles, and incorrect results are produced. Manual correction of latencies is a tedious and error-prone work. This paper presents an automatic method of latency equalization in systems described in VHDL. The method is based on simulation and is portable between different simulation and synthesis tools. The method does not increase the complexity of the synthesized design comparing to the solution based on manual latency adjustment. The example implementation of the proposed methodology together with a simple design demonstrating its use is available as an open source project under BSD license.",2015-09-27T17:32:42Z,http://arxiv.org/pdf/1509.08111v3,2024-04-28,
1509.09249v1,"In-Field Logic Repair of Deep Sub-Micron CMOS Processors","Ultra Deep-Sub-Micron CMOS chips have to function correctly and reliably, not only during their early post-fabrication life, but also for their entire life span. In this paper, we present an architectural-level in-field repair technique. The key idea is to trade area for reliability by adding repair features to the system while keeping the power and the performance overheads as low as possible. In the case of permanent faults, spare blocks will replace the faulty blocks on the fly. Meanwhile by shutting down the main logic blocks, partial threshold voltage recovery can be achieved which will alleviate the ageing-related delays and timing issues. The technique can avoid fatal shut-downs in the system and will decrease the down-time, hence the availability of such a system will be preserved. We have implemented the proposed idea on a pipelined processor core using a conventional ASIC design flow. The simulation results show that by tolerating about 70% area overhead and less than 18% power overhead we can dramatically increase the reliability and decrease the downtime of the processor.",2015-09-30T16:36:32Z,http://arxiv.org/pdf/1509.09249v1,2024-04-28,
1510.02574v1,"A High Throughput List Decoder Architecture for Polar Codes","While long polar codes can achieve the capacity of arbitrary binary-input discrete memoryless channels when decoded by a low complexity successive cancelation (SC) algorithm, the error performance of the SC algorithm is inferior for polar codes with finite block lengths. The cyclic redundancy check (CRC) aided successive cancelation list (SCL) decoding algorithm has better error performance than the SC algorithm. However, current CRC aided SCL (CA-SCL) decoders still suffer from long decoding latency and limited throughput. In this paper, a reduced latency list decoding (RLLD) algorithm for polar codes is proposed. Our RLLD algorithm performs the list decoding on a binary tree, whose leaves correspond to the bits of a polar code. In existing SCL decoding algorithms, all the nodes in the tree are traversed and all possibilities of the information bits are considered. Instead, our RLLD algorithm visits much fewer nodes in the tree and considers fewer possibilities of the information bits. When configured properly, our RLLD algorithm significantly reduces the decoding latency and hence improves throughput, while introducing little performance degradation. Based on our RLLD algorithm, we also propose a high throughput list decoder architecture, which is suitable for larger block lengths due to its scalable partial sum computation unit. Our decoder architecture has been implemented for different block lengths and list sizes using the TSMC 90nm CMOS technology. The implementation results demonstrate that our decoders achieve significant latency reduction and area efficiency improvement compared with other list polar decoders in the literature.",2015-10-09T06:11:34Z,http://arxiv.org/pdf/1510.02574v1,2024-04-28,
1510.04241v1,"A Clock Synchronizer for Repeaterless Low Swing On-Chip Links","A clock synchronizing circuit for repeaterless low swing interconnects is presented in this paper. The circuit uses a delay locked loop (DLL) to generate multiple phases of the clock, of which the one closest to the center of the eye is picked by a phase detector loop. The picked phase is then further fine tuned by an analog voltage controlled delay to position the sampling clock at the center of the eye. A clock domain transfer circuit then transfers the sampled data to the receiver clock domain with a maximum latency of three clock cycles. The proposed synchronizer has been designed and fabricated in 130 nm UMC MM CMOS technology. The circuit consumes 1.4 mW from a 1.2 V supply at a data rate of 1.3 Gbps. Further, the proposed synchronizer has been designed and simulated in TSMC 65 nm CMOS technology. Post layout simulations show that the synchronizer consumes 1.5 mW from a 1 V supply, at a data rate of 4 Gbps in this technology.",2015-10-14T19:09:21Z,http://arxiv.org/pdf/1510.04241v1,2024-04-28,
1510.06791v1,"Network-on-Chip with load balancing based on interleave of flits technique","This paper presents the evaluation of a Network-on-Chip (NoC) that offers load balancing for Systems-on-Chip (SoCs) dedicated for multimedia applications that require high traffic of variable bitrate communication. The NoC is based on a technique that allows the interleaving of flits from diferente flows in the same communication channel, and keep the load balancing without a centralized control in the network. For this purpose, all flits in the network received extra bits, such that every flit carries routing information. The routers use this extra information to perform arbitration and schedule the flits to the corresponding output ports. Analytic comparisons and experimental data show that the approach adopted in the network keeps average latency lower for variable bitrate flows than a network based on resource reservation when both networks are working over 80% of offered load.",2015-10-23T00:14:10Z,http://arxiv.org/pdf/1510.06791v1,2024-04-28,
1511.01946v1,"SecureD: A Secure Dual Core Embedded Processor","Security of embedded computing systems is becoming of paramount concern as these devices become more ubiquitous, contain personal information and are increasingly used for financial transactions. Security attacks targeting embedded systems illegally gain access to the information in these devices or destroy information. The two most common types of attacks embedded systems encounter are code-injection and power analysis attacks. In the past, a number of countermeasures, both hardware- and software-based, were proposed individually against these two types of attacks. However, no single system exists to counter both of these two prominent attacks in a processor based embedded system. Therefore, this paper, for the first time, proposes a hardware/software based countermeasure against both code-injection attacks and power analysis based side-channel attacks in a dual core embedded system. The proposed processor, named SecureD, has an area overhead of just 3.80% and an average runtime increase of 20.0% when compared to a standard dual processing system. The overhead were measured using a set of industry standard application benchmarks, with two encryption and five other programs.",2015-11-05T23:00:13Z,http://arxiv.org/pdf/1511.01946v1,2024-04-28,
1511.06726v1,"Testable Design of Repeaterless Low Swing On-Chip Interconnect","Repeaterless low swing interconnects use mixed signal circuits to achieve high performance at low power. When these interconnects are used in large scale and high volume digital systems their testability becomes very important. This paper discusses the testability of low swing repeaterless on-chip interconnects with equalization and clock synchronization. A capacitively coupled transmitter with a weak driver is used as the transmitter. The receiver samples the low swing input data at the center of the data eye and converts it to rail to rail levels and also synchronizes the data to the receiver's clock domain. The system is a mixed signal circuit and the digital components are all scan testable. For the analog section, just a DC test has a fault coverage of 50% of the structural faults. Simple techniques allow integration of the analog components into the digital scan chain increasing the coverage to 74%. Finally, a BIST with low overhead enhances the coverage to 95% of the structural faults. The design and simulations have been done in UMC 130 nm CMOS technology.",2015-11-20T19:10:00Z,http://arxiv.org/pdf/1511.06726v1,2024-04-28,
1511.08774v3,"Tardis 2.0: Optimized Time Traveling Coherence for Relaxed Consistency Models","Cache coherence scalability is a big challenge in shared memory systems. Traditional protocols do not scale due to the storage and traffic overhead of cache invalidation. Tardis, a recently proposed coherence protocol, removes cache invalidation using logical timestamps and achieves excellent scalability. The original Tardis protocol, however, only supports the Sequential Consistency (SC) memory model, limiting its applicability. Tardis also incurs extra network traffic on some benchmarks due to renew messages, and has suboptimal performance when the program uses spinning to communicate between threads.   In this paper, we address these downsides of Tardis protocol and make it significantly more practical. Specifically, we discuss the architectural, memory system and protocol changes required in order to implement the TSO consistency model on Tardis, and prove that the modified protocol satisfies TSO. We also describe modifications for Partial Store Order (PSO) and Release Consistency (RC). Finally, we propose optimizations for better leasing policies and to handle program spinning. On a set of benchmarks, optimized Tardis improves on a full-map directory protocol in the metrics of performance, storage and network traffic, while being simpler to implement.",2015-11-27T19:44:36Z,http://arxiv.org/pdf/1511.08774v3,2024-04-28,
1511.09074v1,"Digital LDO with Time-Interleaved Comparators for Fast Response and Low Ripple","On-chip voltage regulation using distributed Digital Low Drop Out (LDO) voltage regulators has been identified as a promising technique for efficient power-management for emerging multi-core processors. Digital LDOs (DLDO) can offer low voltage operation, faster transient response, and higher current efficiency. Response time as well as output voltage ripple can be reduced by increasing the speed of the dynamic comparators. However, the comparator offset steeply increases for high clock frequencies, thereby leading to enhanced variations in output voltage. In this work we explore the design of digital LDOs with multiple dynamic comparators that can overcome this bottleneck. In the proposed topology, we apply time-interleaved comparators with the same voltage threshold and uniform current step in order to accomplish the aforementioned features. Simulation based analysis shows that the DLDO with time-interleaved comparators can achieve better overall performance in terms of current efficiency, ripple and settling time. For a load step of 50mA, a DLDO with 8 time-interleaved comparators could achieve an output ripple of less than 5mV, while achieving a settling time of less than 0.5us. Load current dependant dynamic adjustment of clock frequency is proposed to maintain high current efficiency of ~97%.",2015-11-29T19:42:09Z,http://arxiv.org/pdf/1511.09074v1,2024-04-28,
1604.03062v2,"CLEAR: Cross-Layer Exploration for Architecting Resilience - Combining Hardware and Software Techniques to Tolerate Soft Errors in Processor Cores","We present a first of its kind framework which overcomes a major challenge in the design of digital systems that are resilient to reliability failures: achieve desired resilience targets at minimal costs (energy, power, execution time, area) by combining resilience techniques across various layers of the system stack (circuit, logic, architecture, software, algorithm). This is also referred to as cross-layer resilience. In this paper, we focus on radiation-induced soft errors in processor cores. We address both single-event upsets (SEUs) and single-event multiple upsets (SEMUs) in terrestrial environments. Our framework automatically and systematically explores the large space of comprehensive resilience techniques and their combinations across various layers of the system stack (586 cross-layer combinations in this paper), derives cost-effective solutions that achieve resilience targets at minimal costs, and provides guidelines for the design of new resilience techniques. We demonstrate the practicality and effectiveness of our framework using two diverse designs: a simple, in-order processor core and a complex, out-of-order processor core. Our results demonstrate that a carefully optimized combination of circuit-level hardening, logic-level parity checking, and micro-architectural recovery provides a highly cost-effective soft error resilience solution for general-purpose processor cores. For example, a 50x improvement in silent data corruption rate is achieved at only 2.1% energy cost for an out-of-order core (6.1% for an in-order core) with no speed impact. However, selective circuit-level hardening alone, guided by a thorough analysis of the effects of soft errors on application benchmarks, provides a cost-effective soft error resilience solution as well (with ~1% additional energy cost for a 50x improvement in silent data corruption rate).",2016-04-11T18:44:27Z,http://arxiv.org/pdf/1604.03062v2,2024-04-28,
cs/9301111v1,"Nested satisfiability","A special case of the satisfiability problem, in which the clauses have a hierarchical structure, is shown to be solvable in linear time, assuming that the clauses have been represented in a convenient way.",1990-01-01T00:00:00Z,http://arxiv.org/pdf/cs/9301111v1,2024-04-28,
cs/9301113v1,"Textbook examples of recursion","We discuss properties of recursive schemas related to McCarthy's ``91 function'' and to Takeuchi's triple recursion. Several theorems are proposed as interesting candidates for machine verification, and some intriguing open questions are raised.",1991-08-01T00:00:00Z,http://arxiv.org/pdf/cs/9301113v1,2024-04-28,
cs/9808002v1,"Downward Collapse from a Weaker Hypothesis","Hemaspaandra et al. proved that, for $m > 0$ and $0 < i < k - 1$: if $\Sigma_i^p \BoldfaceDelta DIFF_m(\Sigma_k^p)$ is closed under complementation, then $DIFF_m(\Sigma_k^p) = coDIFF_m(\Sigma_k^p)$. This sharply asymmetric result fails to apply to the case in which the hypothesis is weakened by allowing the $\Sigma_i^p$ to be replaced by any class in its difference hierarchy. We so extend the result by proving that, for $s,m > 0$ and $0 < i < k - 1$: if $DIFF_s(\Sigma_i^p) \BoldfaceDelta DIFF_m(\Sigma_k^p)$ is closed under complementation, then $DIFF_m(\Sigma_k^p) = coDIFF_m(\Sigma_k^p)$.",1998-08-24T21:37:54Z,http://arxiv.org/pdf/cs/9808002v1,2024-04-28,
cs/9809001v1,"Immunity and Simplicity for Exact Counting and Other Counting Classes","Ko [RAIRO 24, 1990] and Bruschi [TCS 102, 1992] showed that in some relativized world, PSPACE (in fact, ParityP) contains a set that is immune to the polynomial hierarchy (PH). In this paper, we study and settle the question of (relativized) separations with immunity for PH and the counting classes PP, C_{=}P, and ParityP in all possible pairwise combinations. Our main result is that there is an oracle A relative to which C_{=}P contains a set that is immune to BPP^{ParityP}. In particular, this C_{=}P^A set is immune to PH^{A} and ParityP^{A}. Strengthening results of Tor\'{a}n [J.ACM 38, 1991] and Green [IPL 37, 1991], we also show that, in suitable relativizations, NP contains a C_{=}P-immune set, and ParityP contains a PP^{PH}-immune set. This implies the existence of a C_{=}P^{B}-simple set for some oracle B, which extends results of Balc\'{a}zar et al. [SIAM J.Comp. 14, 1985; RAIRO 22, 1988] and provides the first example of a simple set in a class not known to be contained in PH. Our proof technique requires a circuit lower bound for ``exact counting'' that is derived from Razborov's [Mat. Zametki 41, 1987] lower bound for majority.",1998-09-01T11:16:45Z,http://arxiv.org/pdf/cs/9809001v1,2024-04-28,
cs/9809002v1,"Tally NP Sets and Easy Census Functions","We study the question of whether every P set has an easy (i.e., polynomial-time computable) census function. We characterize this question in terms of unlikely collapses of language and function classes such as the containment of #P_1 in FP, where #P_1 is the class of functions that count the witnesses for tally NP sets. We prove that every #P_{1}^{PH} function can be computed in FP^{#P_{1}^{#P_{1}}}. Consequently, every P set has an easy census function if and only if every set in the polynomial hierarchy does. We show that the assumption of #P_1 being contained in FP implies P = BPP and that PH is contained in MOD_{k}P for each k \geq 2, which provides further evidence that not all sets in P have an easy census function. We also relate a set's property of having an easy census function to other well-studied properties of sets, such as rankability and scalability (the closure of the rankable sets under P-isomorphisms). Finally, we prove that it is no more likely that the census function of any set in P can be approximated (more precisely, can be n^{\alpha}-enumerated in time n^{\beta} for fixed \alpha and \beta) than that it can be precisely computed in polynomial time.",1998-09-01T12:15:55Z,http://arxiv.org/pdf/cs/9809002v1,2024-04-28,
cs/9809114v1,"The descriptive complexity approach to LOGCFL","Building upon the known generalized-quantifier-based first-order characterization of LOGCFL, we lay the groundwork for a deeper investigation. Specifically, we examine subclasses of LOGCFL arising from varying the arity and nesting of groupoidal quantifiers. Our work extends the elaborate theory relating monoidal quantifiers to NC1 and its subclasses. In the absence of the BIT predicate, we resolve the main issues: we show in particular that no single outermost unary groupoidal quantifier with FO can capture all the context-free languages, and we obtain the surprising result that a variant of Greibach's ``hardest context-free language'' is LOGCFL-complete under quantifier-free BIT-free projections. We then prove that FO with unary groupoidal quantifiers is strictly more expressive with the BIT predicate than without. Considering a particular groupoidal quantifier, we prove that first-order logic with majority of pairs is strictly more expressive than first-order with majority of individuals. As a technical tool of independent interest, we define the notion of an aperiodic nondeterministic finite automaton and prove that FO translations are precisely the mappings computed by single-valued aperiodic nondeterministic finite transducers.",1998-09-28T07:57:32Z,http://arxiv.org/pdf/cs/9809114v1,2024-04-28,
cs/9809115v1,"A Generalized Quantifier Concept in Computational Complexity Theory","A notion of generalized quantifier in computational complexity theory is explored and used to give a unified treatment of leaf language definability, oracle separations, type 2 operators, and circuits with monoidal gates. Relations to Lindstroem quantifiers are pointed out.",1998-09-28T08:20:25Z,http://arxiv.org/pdf/cs/9809115v1,2024-04-28,
cs/9809116v1,"The Complexity of Computing Optimal Assignments of Generalized Propositional Formulae","We consider the problems of finding the lexicographically minimal (or maximal) satisfying assignment of propositional formulae for different restricted formula classes. It turns out that for each class from our framework, the above problem is either polynomial time solvable or complete for OptP. We also consider the problem of deciding if in the optimal assignment the largest variable gets value 1. We show that this problem is either in P or P^NP complete.",1998-09-28T08:38:27Z,http://arxiv.org/pdf/cs/9809116v1,2024-04-28,
cs/9809117v2,"Hard instance generation for SAT","We propose an algorithm of generating hard instances for the Satisfying Assignment Search Problem (in short, SAT). The algorithm transforms instances of the integer factorization problem into SAT instances efficiently by using the Chinese Remainder Theorem. For example, it is possible to construct SAT instances with about 5,600 variables that is as hard as factorizing 100 bit integers.",1998-09-28T10:50:03Z,http://arxiv.org/pdf/cs/9809117v2,2024-04-28,
cs/9906017v1,"Generalization of automatic sequences for numeration systems on a regular language","Let L be an infinite regular language on a totally ordered alphabet (A,<). Feeding a finite deterministic automaton (with output) with the words of L enumerated lexicographically with respect to < leads to an infinite sequence over the output alphabet of the automaton. This process generalizes the concept of k-automatic sequence for abstract numeration systems on a regular language (instead of systems in base k). Here, I study the first properties of these sequences and their relations with numeration systems.",1999-06-22T10:01:27Z,http://arxiv.org/pdf/cs/9906017v1,2024-04-28,
cs/9906028v1,"On the Power of Positive Turing Reductions","In the early 1980s, Selman's seminal work on positive Turing reductions showed that positive Turing reduction to NP yields no greater computational power than NP itself. Thus, positive Turing and Turing reducibility to NP differ sharply unless the polynomial hierarchy collapses.   We show that the situation is quite different for DP, the next level of the boolean hierarchy. In particular, positive Turing reduction to DP already yields all (and only) sets Turing reducibility to NP. Thus, positive Turing and Turing reducibility to DP yield the same class. Additionally, we show that an even weaker class, P(NP[1]), can be substituted for DP in this context.",1999-06-26T23:31:10Z,http://arxiv.org/pdf/cs/9906028v1,2024-04-28,
cs/9906033v1,"Robust Reductions","We continue the study of robust reductions initiated by Gavalda and Balcazar. In particular, a 1991 paper of Gavalda and Balcazar claimed an optimal separation between the power of robust and nondeterministic strong reductions. Unfortunately, their proof is invalid. We re-establish their theorem.   Generalizing robust reductions, we note that robustly strong reductions are built from two restrictions, robust underproductivity and robust overproductivity, both of which have been separately studied before in other contexts. By systematically analyzing the power of these reductions, we explore the extent to which each restriction weakens the power of reductions. We show that one of these reductions yields a new, strong form of the Karp-Lipton Theorem.",1999-06-29T18:26:12Z,http://arxiv.org/pdf/cs/9906033v1,2024-04-28,
cs/9907033v1,"Unambiguous Computation: Boolean Hierarchies and Sparse Turing-Complete Sets","It is known that for any class C closed under union and intersection, the Boolean closure of C, the Boolean hierarchy over C, and the symmetric difference hierarchy over C all are equal. We prove that these equalities hold for any complexity class closed under intersection; in particular, they thus hold for unambiguous polynomial time (UP). In contrast to the NP case, we prove that the Hausdorff hierarchy and the nested difference hierarchy over UP both fail to capture the Boolean closure of UP in some relativized worlds.   Karp and Lipton proved that if nondeterministic polynomial time has sparse Turing-complete sets, then the polynomial hierarchy collapses. We establish the first consequences from the assumption that unambiguous polynomial time has sparse Turing-complete sets: (a) UP is in Low_2, where Low_2 is the second level of the low hierarchy, and (b) each level of the unambiguous polynomial hierarchy is contained one level lower in the promise unambiguous polynomial hierarchy than is otherwise known to be the case.",1999-07-26T10:09:58Z,http://arxiv.org/pdf/cs/9907033v1,2024-04-28,
cs/9907034v1,"Polynomial-Time Multi-Selectivity","We introduce a generalization of Selman's P-selectivity that yields a more flexible notion of selectivity, called (polynomial-time) multi-selectivity, in which the selector is allowed to operate on multiple input strings. Since our introduction of this class, it has been used to prove the first known (and optimal) lower bounds for generalized selectivity-like classes in terms of EL_2, the second level of the extended low hierarchy. We study the resulting selectivity hierarchy, denoted by SH, which we prove does not collapse. In particular, we study the internal structure and the properties of SH and completely establish, in terms of incomparability and strict inclusion, the relations between our generalized selectivity classes and Ogihara's P-mc (polynomial-time membership-comparable) classes. Although SH is a strictly increasing infinite hierarchy, we show that the core results that hold for the P-selective sets and that prove them structurally simple also hold for SH. In particular, all sets in SH have small circuits; the NP sets in SH are in Low_2, the second level of the low hierarchy within NP; and SAT cannot be in SH unless P = NP. Finally, it is known that P-Sel, the class of P-selective sets, is not closed under union or intersection. We provide an extended selectivity hierarchy that is based on SH and that is large enough to capture those closures of the P-selective sets, and yet, in contrast with the P-mc classes, is refined enough to distinguish them.",1999-07-25T20:55:21Z,http://arxiv.org/pdf/cs/9907034v1,2024-04-28,
cs/9907035v1,"Easy Sets and Hard Certificate Schemes","Can easy sets only have easy certificate schemes? In this paper, we study the class of sets that, for all NP certificate schemes (i.e., NP machines), always have easy acceptance certificates (i.e., accepting paths) that can be computed in polynomial time. We also study the class of sets that, for all NP certificate schemes, infinitely often have easy acceptance certificates.   In particular, we provide equivalent characterizations of these classes in terms of relative generalized Kolmogorov complexity, showing that they are robust. We also provide structural conditions---regarding immunity and class collapses---that put upper and lower bounds on the sizes of these two classes. Finally, we provide negative results showing that some of our positive claims are optimal with regard to being relativizable. Our negative results are proven using a novel observation: we show that the classical ``wide spacing'' oracle construction technique yields instant non-bi-immunity results. Furthermore, we establish a result that improves upon Baker, Gill, and Solovay's classical result that NP \neq P = NP \cap coNP holds in some relativized world.",1999-07-25T21:05:19Z,http://arxiv.org/pdf/cs/9907035v1,2024-04-28,
cs/9907036v1,"Exact Analysis of Dodgson Elections: Lewis Carroll's 1876 Voting System is Complete for Parallel Access to NP","In 1876, Lewis Carroll proposed a voting system in which the winner is the candidate who with the fewest changes in voters' preferences becomes a Condorcet winner---a candidate who beats all other candidates in pairwise majority-rule elections. Bartholdi, Tovey, and Trick provided a lower bound---NP-hardness---on the computational complexity of determining the election winner in Carroll's system. We provide a stronger lower bound and an upper bound that matches our lower bound. In particular, determining the winner in Carroll's system is complete for parallel access to NP, i.e., it is complete for $\thetatwo$, for which it becomes the most natural complete problem known. It follows that determining the winner in Carroll's elections is not NP-complete unless the polynomial hierarchy collapses.",1999-07-25T21:16:56Z,http://arxiv.org/pdf/cs/9907036v1,2024-04-28,
cs/9907037v1,"Boolean Operations, Joins, and the Extended Low Hierarchy","We prove that the join of two sets may actually fall into a lower level of the extended low hierarchy than either of the sets. In particular, there exist sets that are not in the second level of the extended low hierarchy, EL_2, yet their join is in EL_2. That is, in terms of extended lowness, the join operator can lower complexity. Since in a strong intuitive sense the join does not lower complexity, our result suggests that the extended low hierarchy is unnatural as a complexity measure. We also study the closure properties of EL_ and prove that EL_2 is not closed under certain Boolean operations. To this end, we establish the first known (and optimal) EL_2 lower bounds for certain notions generalizing Selman's P-selectivity, which may be regarded as an interesting result in its own right.",1999-07-25T21:30:10Z,http://arxiv.org/pdf/cs/9907037v1,2024-04-28,
cs/9907038v1,"A Second Step Towards Complexity-Theoretic Analogs of Rice's Theorem","Rice's Theorem states that every nontrivial language property of the recursively enumerable sets is undecidable. Borchert and Stephan initiated the search for complexity-theoretic analogs of Rice's Theorem. In particular, they proved that every nontrivial counting property of circuits is UP-hard, and that a number of closely related problems are SPP-hard.   The present paper studies whether their UP-hardness result itself can be improved to SPP-hardness. We show that their UP-hardness result cannot be strengthened to SPP-hardness unless unlikely complexity class containments hold. Nonetheless, we prove that every P-constructibly bi-infinite counting property of circuits is SPP-hard. We also raise their general lower bound from unambiguous nondeterminism to constant-ambiguity nondeterminism.",1999-07-25T21:39:03Z,http://arxiv.org/pdf/cs/9907038v1,2024-04-28,
cs/9907039v1,"Raising NP Lower Bounds to Parallel NP Lower Bounds","A decade ago, a beautiful paper by Wagner developed a ``toolkit'' that in certain cases allows one to prove problems hard for parallel access to NP. However, the problems his toolkit applies to most directly are not overly natural. During the past year, problems that previously were known only to be NP-hard or coNP-hard have been shown to be hard even for the class of sets solvable via parallel access to NP. Many of these problems are longstanding and extremely natural, such as the Minimum Equivalent Expression problem (which was the original motivation for creating the polynomial hierarchy), the problem of determining the winner in the election system introduced by Lewis Carroll in 1876, and the problem of determining on which inputs heuristic algorithms perform well. In the present article, we survey this recent progress in raising lower bounds.",1999-07-25T21:47:02Z,http://arxiv.org/pdf/cs/9907039v1,2024-04-28,
cs/9907041v1,"Restrictive Acceptance Suffices for Equivalence Problems","One way of suggesting that an NP problem may not be NP-complete is to show that it is in the class UP. We suggest an analogous new approach---weaker in strength of evidence but more broadly applicable---to suggesting that concrete~NP problems are not NP-complete. In particular we introduce the class EP, the subclass of NP consisting of those languages accepted by NP machines that when they accept always have a number of accepting paths that is a power of two. Since if any NP-complete set is in EP then all NP sets are in EP, it follows---with whatever degree of strength one believes that EP differs from NP---that membership in EP can be viewed as evidence that a problem is not NP-complete.   We show that the negation equivalence problem for OBDDs (ordered binary decision diagrams) and the interchange equivalence problem for 2-dags are in EP. We also show that for boolean negation the equivalence problem is in EP^{NP}, thus tightening the existing NP^{NP} upper bound. We show that FewP, bounded ambiguity polynomial time, is contained in EP, a result that is not known to follow from the previous SPP upper bound. For the three problems and classes just mentioned with regard to EP, no proof of membership/containment in UP is known, and for the problem just mentioned with regard to EP^{NP}, no proof of membership in UP^{NP} is known. Thus, EP is indeed a tool that gives evidence against NP-completeness in natural cases where UP cannot currently be applied.",1999-07-26T10:50:48Z,http://arxiv.org/pdf/cs/9907041v1,2024-04-28,
cs/9908018v1,"Construction of regular languages and recognizability of polynomials","A generalization of numeration system in which the set N of the natural numbers is recognizable by finite automata can be obtained by describing a lexicographically ordered infinite regular language. Here we show that if P belonging to Q[x] is a polynomial such that P(N) is a subset of N then we can construct a numeration system in which the set of representations of P(N) is regular. The main issue in this construction is to setup a regular language with a density function equals to P(n+1)-P(n) for n large enough.",1999-08-27T07:33:28Z,http://arxiv.org/pdf/cs/9908018v1,2024-04-28,
cs/9909020v1,"Query Order","We study the effect of query order on computational power, and show that $\pjk$-the languages computable via a polynomial-time machine given one query to the jth level of the boolean hierarchy followed by one query to the kth level of the boolean hierarchy-equals $\redttnp{j+2k-1}$ if j is even and k is odd, and equals $\redttnp{j+2k}$ otherwise. Thus, unless the polynomial hierarchy collapses, it holds that for each $1\leq j \leq k$: $\pjk = \pkj \iff (j=k) \lor (j{is even} \land k=j+1)$. We extend our analysis to apply to more general query classes.",1999-09-30T17:06:40Z,http://arxiv.org/pdf/cs/9909020v1,2024-04-28,
cs/9910002v1,"What's Up with Downward Collapse: Using the Easy-Hard Technique to Link Boolean and Polynomial Hierarchy Collapses","During the past decade, nine papers have obtained increasingly strong consequences from the assumption that boolean or bounded-query hierarchies collapse. The final four papers of this nine-paper progression actually achieve downward collapse---that is, they show that high-level collapses induce collapses at (what beforehand were thought to be) lower complexity levels. For example, for each $k\geq 2$ it is now known that if $\psigkone=\psigktwo$ then $\ph=\sigmak$. This article surveys the history, the results, and the technique---the so-called easy-hard method---of these nine papers.",1999-10-01T15:45:25Z,http://arxiv.org/pdf/cs/9910002v1,2024-04-28,
cs/9910003v1,"R_{1-tt}^{SN}(NP) Distinguishes Robust Many-One and Turing Completeness","Do complexity classes have many-one complete sets if and only if they have Turing-complete sets? We prove that there is a relativized world in which a relatively natural complexity class-namely a downward closure of NP, \rsnnp - has Turing-complete sets but has no many-one complete sets. In fact, we show that in the same relativized world this class has 2-truth-table complete sets but lacks 1-truth-table complete sets. As part of the groundwork for our result, we prove that \rsnnp has many equivalent forms having to do with ordered and parallel access to $\np$ and $\npinterconp$.",1999-10-01T18:55:20Z,http://arxiv.org/pdf/cs/9910003v1,2024-04-28,
cs/9910004v1,"An Introduction to Query Order","Hemaspaandra, Hempel, and Wechsung [cs.CC/9909020] raised the following questions: If one is allowed one question to each of two different information sources, does the order in which one asks the questions affect the class of problems that one can solve with the given access? If so, which order yields the greater computational power?   The answers to these questions have been learned-inasfar as they can be learned without resolving whether or not the polynomial hierarchy collapses-for both the polynomial hierarchy and the boolean hierarchy. In the polynomial hierarchy, query order never matters. In the boolean hierarchy, query order sometimes does not matter and, unless the polynomial hierarchy collapses, sometimes does matter. Furthermore, the study of query order has yielded dividends in seemingly unrelated areas, such as bottleneck computations and downward translation of equality.   In this article, we present some of the central results on query order. The article is written in such a way as to encourage the reader to try his or her own hand at proving some of these results. We also give literature pointers to the quickly growing set of related results and applications.",1999-10-01T19:08:32Z,http://arxiv.org/pdf/cs/9910004v1,2024-04-28,
cs/9910005v2,"Query Order and the Polynomial Hierarchy","Hemaspaandra, Hempel, and Wechsung [cs.CC/9909020] initiated the field of query order, which studies the ways in which computational power is affected by the order in which information sources are accessed. The present paper studies, for the first time, query order as it applies to the levels of the polynomial hierarchy. We prove that the levels of the polynomial hierarchy are order-oblivious. Yet, we also show that these ordered query classes form new levels in the polynomial hierarchy unless the polynomial hierarchy collapses. We prove that all leaf language classes - and thus essentially all standard complexity classes - inherit all order-obliviousness results that hold for P.",1999-10-01T19:20:07Z,http://arxiv.org/pdf/cs/9910005v2,2024-04-28,
cs/9910006v1,"Self-Specifying Machines","We study the computational power of machines that specify their own acceptance types, and show that they accept exactly the languages that $\manyonesharp$-reduce to NP sets. A natural variant accepts exactly the languages that $\manyonesharp$-reduce to P sets. We show that these two classes coincide if and only if $\psone = \psnnoplusbigohone$, where the latter class denotes the sets acceptable via at most one question to $\sharpp$ followed by at most a constant number of questions to $\np$.",1999-10-01T19:29:53Z,http://arxiv.org/pdf/cs/9910006v1,2024-04-28,
cs/9910007v1,"A Downward Collapse within the Polynomial Hierarchy","Downward collapse (a.k.a. upward separation) refers to cases where the equality of two larger classes implies the equality of two smaller classes. We provide an unqualified downward collapse result completely within the polynomial hierarchy. In particular, we prove that, for k > 2, if $\psigkone = \psigktwo$ then $\sigmak = \pik = \ph$. We extend this to obtain a more general downward collapse result.",1999-10-01T19:48:25Z,http://arxiv.org/pdf/cs/9910007v1,2024-04-28,
cs/9910008v2,"Translating Equality Downwards","Downward translation of equality refers to cases where a collapse of some pair of complexity classes would induce a collapse of some other pair of complexity classes that (a priori) one expects are smaller. Recently, the first downward translation of equality was obtained that applied to the polynomial hierarchy-in particular, to bounded access to its levels [cs.CC/9910007]. In this paper, we provide a much broader downward translation that extends not only that downward translation but also that translation's elegant enhancement by Buhrman and Fortnow. Our work also sheds light on previous research on the structure of refined polynomial hierarchies, and strengthens the connection between the collapse of bounded query hierarchies and the collapse of the polynomial hierarchy.",1999-10-01T19:58:41Z,http://arxiv.org/pdf/cs/9910008v2,2024-04-28,
cs/9911002v2,"Numeration systems on a regular language: Arithmetic operations, Recognizability and Formal power series","Generalizations of numeration systems in which N is recognizable by a finite automaton are obtained by describing a lexicographically ordered infinite regular language L over a finite alphabet A. For these systems, we obtain a characterization of recognizable sets of integers in terms of rational formal series. We also show that, if the complexity of L is Theta (n^q) (resp. if L is the complement of a polynomial language), then multiplication by an integer k preserves recognizability only if k=t^{q+1} (resp. if k is not a power of the cardinality of A) for some integer t. Finally, we obtain sufficient conditions for the notions of recognizability and U-recognizability to be equivalent, where U is some positional numeration system related to a sequence of integers.",1999-11-08T13:03:50Z,http://arxiv.org/pdf/cs/9911002v2,2024-04-28,
cs/0004009v2,"Separating the complexity classes NL and NP","Withdrawn since -order- was overlooked. First order reductions without order are much too weak to separate.",2000-04-17T22:17:33Z,http://arxiv.org/pdf/cs/0004009v2,2024-04-28,
cs/0007025v1,"A Moment of Perfect Clarity I: The Parallel Census Technique","We discuss the history and uses of the parallel census technique---an elegant tool in the study of certain computational objects having polynomially bounded census functions. A sequel will discuss advances (including Cai, Naik, and Sivakumar [CNS95] and Glasser [Gla00]), some related to the parallel census technique and some due to other approaches, in the complexity-class collapses that follow if NP has sparse hard sets under reductions weaker than (full) truth-table reductions.",2000-07-13T22:14:04Z,http://arxiv.org/pdf/cs/0007025v1,2024-04-28,
cs/0010005v1,"Low Ambiguity in Strong, Total, Associative, One-Way Functions","Rabi and Sherman present a cryptographic paradigm based on associative, one-way functions that are strong (i.e., hard to invert even if one of their arguments is given) and total. Hemaspaandra and Rothe proved that such powerful one-way functions exist exactly if (standard) one-way functions exist, thus showing that the associative one-way function approach is as plausible as previous approaches. In the present paper, we study the degree of ambiguity of one-way functions. Rabiand Sherman showed that no associative one-way function (over a universe having at least two elements) can be unambiguous (i.e., one-to-one). Nonetheless, we prove that if standard, unambiguous, one-way functions exist, then there exist strong, total, associative, one-way functions that are $\mathcal{O}(n)$-to-one. This puts a reasonable upper bound on the ambiguity.",2000-10-02T05:25:20Z,http://arxiv.org/pdf/cs/0010005v1,2024-04-28,
cs/0010011v1,"If P \neq NP then Some Strongly Noninvertible Functions are Invertible","Rabi, Rivest, and Sherman alter the standard notion of noninvertibility to a new notion they call strong noninvertibility, and show -- via explicit cryptographic protocols for secret-key agreement ([RS93,RS97] attribute this to Rivest and Sherman) and digital signatures [RS93,RS97] -- that strongly noninvertible functions would be very useful components in protocol design. Their definition of strong noninvertibility has a small twist (``respecting the argument given'') that is needed to ensure cryptographic usefulness. In this paper, we show that this small twist has a large, unexpected consequence: Unless P=NP, some strongly noninvertible functions are invertible.",2000-10-06T18:45:21Z,http://arxiv.org/pdf/cs/0010011v1,2024-04-28,
cs/0102024v1,"P-Immune Sets with Holes Lack Self-Reducibility Properties","No P-immune set having exponential gaps is positive-Turing self-reducible.",2001-02-23T17:10:50Z,http://arxiv.org/pdf/cs/0102024v1,2024-04-28,
cs/0106037v1,"Using the No-Search Easy-Hard Technique for Downward Collapse","The top part of the preceding figure [figure appears in actual paper] shows some classes from the (truth-table) bounded-query and boolean hierarchies. It is well-known that if either of these hierarchies collapses at a given level, then all higher levels of that hierarchy collapse to that same level. This is a standard ``upward translation of equality'' that has been known for over a decade. The issue of whether these hierarchies can translate equality {\em downwards\/} has proven vastly more challenging. In particular, with regard to the figure above, consider the following claim:   $$P_{m-tt}^{\Sigma_k^p} = P_{m+1-tt}^{\Sigma_k^p} \implies   DIFF_m(\Sigma_k^p) coDIFF_m(\Sigma_k^p) = BH(\Sigma_k^p). (*) $$   This claim, if true, says that equality translates downwards between levels of the bounded-query hierarchy and the boolean hierarchy levels that (before the fact) are immediately below them.   Until recently, it was not known whether (*) {\em ever\/} held, except for the degenerate cases $m=0$ and $k=0$. Then Hemaspaandra, Hemaspaandra, and Hempel \cite{hem-hem-hem:j:downward-translation} proved that (*) holds for all $m$, for $k > 2$. Buhrman and Fortnow~\cite{buh-for:j:two-queries} then showed that, when $k=2$, (*) holds for the case $m = 1$. In this paper, we prove that for the case $k=2$, (*) holds for all values of $m$. Since there is an oracle relative to which ``for $k=1$, (*) holds for all $m$'' fails \cite{buh-for:j:two-queries}, our achievement of the $k=2$ case cannot to be strengthened to $k=1$ by any relativizable proof technique. The new downward translation we obtain also tightens the collapse in the polynomial hierarchy implied by a collapse in the bounded-query hierarchy of the second level of the polynomial hierarchy.",2001-06-15T11:07:02Z,http://arxiv.org/pdf/cs/0106037v1,2024-04-28,
cs/0106041v1,"Computing Complete Graph Isomorphisms and Hamiltonian Cycles from Partial Ones","We prove that computing a single pair of vertices that are mapped onto each other by an isomorphism $\phi$ between two isomorphic graphs is as hard as computing $\phi$ itself. This result optimally improves upon a result of G\'{a}l et al. We establish a similar, albeit slightly weaker, result about computing complete Hamiltonian cycles of a graph from partial Hamiltonian cycles.",2001-06-19T16:10:55Z,http://arxiv.org/pdf/cs/0106041v1,2024-04-28,
cs/0106045v2,"A Note on the Complexity of Computing the Smallest Four-Coloring of Planar Graphs","We show that computing the lexicographically first four-coloring for planar graphs is P^{NP}-hard. This result optimally improves upon a result of Khuller and Vazirani who prove this problem to be NP-hard, and conclude that it is not self-reducible in the sense of Schnorr, assuming P \neq NP. We discuss this application to non-self-reducibility and provide a general related result.",2001-06-21T07:04:31Z,http://arxiv.org/pdf/cs/0106045v2,2024-04-28,
cs/0106049v2,"Recursively Undecidable Properties of NP","We show that there cannot be any algorithm that for a given nondeterministic polynomial-time Turing machine determinates whether or not the language recognized by this machine belongs to P",2001-06-25T13:37:59Z,http://arxiv.org/pdf/cs/0106049v2,2024-04-28,
cs/0108010v3,"A Note on Tiling under Tomographic Constraints","Given a tiling of a 2D grid with several types of tiles, we can count for every row and column how many tiles of each type it intersects. These numbers are called the_projections_. We are interested in the problem of reconstructing a tiling which has given projections. Some simple variants of this problem, involving tiles that are 1x1 or 1x2 rectangles, have been studied in the past, and were proved to be either solvable in polynomial time or NP-complete. In this note we make progress toward a comprehensive classification of various tiling reconstruction problems, by proving NP-completeness results for several sets of tiles.",2001-08-21T10:29:32Z,http://arxiv.org/pdf/cs/0108010v3,2024-04-28,
cs/0109018v1,"Exact Complexity of Exact-Four-Colorability","Let $M_k \seq \nats$ be a given set that consists of $k$ noncontiguous integers. Define $\exactcolor{M_k}$ to be the problem of determining whether $\chi(G)$, the chromatic number of a given graph $G$, equals one of the $k$ elements of the set $M_k$ exactly. In 1987, Wagner \cite{wag:j:min-max} proved that $\exactcolor{M_k}$ is $\bhlevel{2k}$-complete, where $M_k = \{6k+1, 6k+3, >..., 8k-1 \}$ and $\bhlevel{2k}$ is the $2k$th level of the boolean hierarchy over $\np$. In particular, for $k = 1$, it is DP-complete to determine whether $\chi(G) = 7$, where $\DP = \bhlevel{2}$. Wagner raised the question of how small the numbers in a $k$-element set $M_k$ can be chosen such that $\exactcolor{M_k}$ still is $\bhlevel{2k}$-complete. In particular, for $k = 1$, he asked if it is DP-complete to determine whether $\chi(G) = 4$. In this note, we solve this question of Wagner and determine the precise threshold $t \in \{4, 5, 6, 7\}$ for which the problem $\exactcolor{\{t\}}$ jumps from NP to DP-completeness: It is DP-complete to determine whether $\chi(G) = 4$, yet $\exactcolor{\{3\}}$ is in $\np$. More generally, for each $k \geq 1$, we show that $\exactcolor{M_k}$ is $\bhlevel{2k}$-complete for $M_k = \{3k+1, 3k+3,..., 5k-1\}$.",2001-09-14T15:07:08Z,http://arxiv.org/pdf/cs/0109018v1,2024-04-28,
cs/0110025v2,"Recognizing When Heuristics Can Approximate Minimum Vertex Covers Is Complete for Parallel Access to NP","For both the edge deletion heuristic and the maximum-degree greedy heuristic, we study the problem of recognizing those graphs for which that heuristic can approximate the size of a minimum vertex cover within a constant factor of r, where r is a fixed rational number. Our main results are that these problems are complete for the class of problems solvable via parallel access to NP. To achieve these main results, we also show that the restriction of the vertex cover problem to those graphs for which either of these heuristics can find an optimal solution remains NP-hard.",2001-10-10T15:02:35Z,http://arxiv.org/pdf/cs/0110025v2,2024-04-28,
cs/0110039v1,"Two heads are better than two tapes","We show that a Turing machine with two single-head one-dimensional tapes cannot recognize the set {x2x'| x \in {0,1}^* and x' is a prefix of x} in real time, although it can do so with three tapes, two two-dimensional tapes, or one two-head tape, or in linear time with just one tape. In particular, this settles the longstanding conjecture that a two-head Turing machine can recognize more languages in real time if its heads are on the same one-dimensional tape than if they are on separate one-dimensional tapes.",2001-10-18T14:58:26Z,http://arxiv.org/pdf/cs/0110039v1,2024-04-28,
cs/0110040v1,"A New Approach to Formal Language Theory by Kolmogorov Complexity","We present a new approach to formal language theory using Kolmogorov complexity. The main results presented here are an alternative for pumping lemma(s), a new characterization for regular languages, and a new method to separate deterministic context-free languages and nondeterministic context-free languages. The use of the new `incompressibility arguments' is illustrated by many examples. The approach is also successful at the high end of the Chomsky hierarchy since one can quantify nonrecursiveness in terms of Kolmogorov complexity. (This is a preliminary uncorrected version. The final version is the one published in SIAM J. Comput., 24:2(1995), 398-410.)",2001-10-18T16:43:09Z,http://arxiv.org/pdf/cs/0110040v1,2024-04-28,
cs/0111009v1,"On the complexity of inducing categorical and quantitative association rules","Inducing association rules is one of the central tasks in data mining applications. Quantitative association rules induced from databases describe rich and hidden relationships holding within data that can prove useful for various application purposes (e.g., market basket analysis, customer profiling, and others). Even though such association rules are quite widely used in practice, a thorough analysis of the computational complexity of inducing them is missing. This paper intends to provide a contribution in this setting. To this end, we first formally define quantitative association rule mining problems, which entail boolean association rules as a special case, and then analyze their computational complexities, by considering both the standard cases, and a some special interesting case, that is, association rule induction over databases with null values, fixed-size attribute set databases, sparse databases, fixed threshold problems.",2001-11-06T15:42:06Z,http://arxiv.org/pdf/cs/0111009v1,2024-04-28,
cs/0111052v1,"A comparison of Zeroes and Ones of a Boolean Polynomial","In this paper we consider the computational complexity of the following problem. Let $f$ be a Boolean polynomial. What value of $f$, 0 or 1, is taken more frequently? The problem is solved in polynomial time for polynomials of degrees 1,2. The next case of degree 3 appears to be PP-complete under polynomial reductions in the class of promise problems. The proof is based on techniques of quantum computation.",2001-11-20T15:03:19Z,http://arxiv.org/pdf/cs/0111052v1,2024-04-28,
cs/0112001v10,"An Average Case NP-Complete Graph Coloring Problem","NP-complete problems should be hard on some instances but those may be extremely rare. On generic instances many such problems, especially related to random graphs, have been proven easy. We show the intractability of random instances of a graph coloring problem: this graph problem is hard on average unless all NP problem under all samplable (i.e., generatable in polynomial time) distributions are easy. Worst case reductions use special gadgets and typically map instances into a negligible fraction of possible outputs. Ours must output nearly random graphs and avoid any super-polynomial distortion of probabilities.",2001-12-02T21:44:23Z,http://arxiv.org/pdf/cs/0112001v10,2024-04-28,
cs/0112021v1,"Exact Complexity of the Winner Problem for Young Elections","In 1977, Young proposed a voting scheme that extends the Condorcet Principle based on the fewest possible number of voters whose removal yields a Condorcet winner. We prove that both the winner and the ranking problem for Young elections is complete for the class of problems solvable in polynomial time by parallel access to NP. Analogous results for Lewis Carroll's 1876 voting scheme were recently established by Hemaspaandra et al. In contrast, we prove that the winner and ranking problems in Fishburn's homogeneous variant of Carroll's voting scheme can be solved efficiently by linear programming.",2001-12-20T11:37:33Z,http://arxiv.org/pdf/cs/0112021v1,2024-04-28,
cs/0201001v1,"Lower Bounds for Matrix Product","We prove lower bounds on the number of product gates in bilinear and quadratic circuits that compute the product of two $n \cross n$ matrices over finite fields. In particular we obtain the following results:   1. We show that the number of product gates in any bilinear (or quadratic) circuit that computes the product of two $n \cross n$ matrices over $F_2$ is at least $3 n^2 - o(n^2)$.   2. We show that the number of product gates in any bilinear circuit that computes the product of two $n \cross n$ matrices over $F_p$ is at least $(2.5 + \frac{1.5}{p^3 -1})n^2 -o(n^2)$.   These results improve the former results of Bshouty '89 and Blaser '99 who proved lower bounds of $2.5 n^2 - o(n^2)$.",2002-01-02T09:50:57Z,http://arxiv.org/pdf/cs/0201001v1,2024-04-28,
cs/0203016v1,"Dimension in Complexity Classes","A theory of resource-bounded dimension is developed using gales, which are natural generalizations of martingales. When the resource bound \Delta (a parameter of the theory) is unrestricted, the resulting dimension is precisely the classical Hausdorff dimension (sometimes called fractal dimension). Other choices of the parameter \Delta yield internal dimension theories in E, E2, ESPACE, and other complexity classes, and in the class of all decidable problems. In general, if C is such a class, then every set X of languages has a dimension in C, which is a real number dim(X|C) in [0,1]. Along with the elements of this theory, two preliminary applications are presented:   1. For every real number \alpha in (0,1/2), the set FREQ(<=\alpha), consisting of all languages that asymptotically contain at most \alpha of all strings, has dimension H(\alpha) -- the binary entropy of \alpha -- in E and in E2.   2. For every real number \alpha in (0,1), the set SIZE(\alpha* (2^n)/n), consisting of all languages decidable by Boolean circuits of at most \alpha*(2^n)/n gates, has dimension \alpha in ESPACE.",2002-03-12T20:16:27Z,http://arxiv.org/pdf/cs/0203016v1,2024-04-28,
cs/0203017v1,"The Dimensions of Individual Strings and Sequences","A constructive version of Hausdorff dimension is developed using constructive supergales, which are betting strategies that generalize the constructive supermartingales used in the theory of individual random sequences. This constructive dimension is used to assign every individual (infinite, binary) sequence S a dimension, which is a real number dim(S) in the interval [0,1]. Sequences that are random (in the sense of Martin-Lof) have dimension 1, while sequences that are decidable, \Sigma^0_1, or \Pi^0_1 have dimension 0. It is shown that for every \Delta^0_2-computable real number \alpha in [0,1] there is a \Delta^0_2 sequence S such that \dim(S) = \alpha.   A discrete version of constructive dimension is also developed using termgales, which are supergale-like functions that bet on the terminations of (finite, binary) strings as well as on their successive bits. This discrete dimension is used to assign each individual string w a dimension, which is a nonnegative real number dim(w). The dimension of a sequence is shown to be the limit infimum of the dimensions of its prefixes.   The Kolmogorov complexity of a string is proven to be the product of its length and its dimension. This gives a new characterization of algorithmic information and a new proof of Mayordomo's recent theorem stating that the dimension of a sequence is the limit infimum of the average Kolmogorov complexity of its first n bits.   Every sequence that is random relative to any computable sequence of coin-toss biases that converge to a real number \beta in (0,1) is shown to have dimension \H(\beta), the binary entropy of \beta.",2002-03-12T20:28:19Z,http://arxiv.org/pdf/cs/0203017v1,2024-04-28,
cs/0203029v21,"Forbidden Information","Goedel Incompleteness Theorem leaves open a way around it, vaguely perceived for a long time but not clearly identified. (Thus, Goedel believed informal arguments can answer any math question.) Closing this loophole does not seem obvious and involves Kolmogorov complexity. (This is unrelated to, well studied before, complexity quantifications of the usual Goedel effects.) I consider extensions U of the universal partial recursive predicate (or, say, Peano Arithmetic). I prove that any U either leaves an n-bit input (statement) unresolved or contains nearly all information about the n-bit prefix of any r.e. real r (which is n bits for some r). I argue that creating significant information about a SPECIFIC math sequence is impossible regardless of the methods used. Similar problems and answers apply to other unsolvability results for tasks allowing multiple solutions, e.g. non-recursive tilings.",2002-03-27T17:28:22Z,http://arxiv.org/pdf/cs/0203029v21,2024-04-28,
cs/0205031v1,"Lecture Notes on Evasiveness of Graph Properties","This report presents notes from the first eight lectures of the class Many Models of Complexity taught by Laszlo Lovasz at Princeton University in the fall of 1990. The topic is evasiveness of graph properties: given a graph property, how many edges of the graph an algorithm must check in the worst case before it knows whether the property holds.",2002-05-18T01:20:37Z,http://arxiv.org/pdf/cs/0205031v1,2024-04-28,
cs/0205056v1,"Parameterized Intractability of Motif Search Problems","We show that Closest Substring, one of the most important problems in the field of biological sequence analysis, is W[1]-hard when parameterized by the number k of input strings (and remains so, even over a binary alphabet). This problem is therefore unlikely to be solvable in time O(f(k)\cdot n^{c}) for any function f of k and constant c independent of k. The problem can therefore be expected to be intractable, in any practical sense, for k>=3. Our result supports the intuition that Closest Substring is computationally much harder than the special case of Closest String, although both problems are NP-complete. We also prove W[1]-hardness for other parameterizations in the case of unbounded alphabet size. Our W[1]-hardness result for Closest Substring generalizes to Consensus Patterns, a problem of similar significance in computational biology.",2002-05-21T10:23:18Z,http://arxiv.org/pdf/cs/0205056v1,2024-04-28,
cs/0205064v6,"Three complete deterministic polynomial algorithms for 3SAT","Three algorithms are presented that determine the existence of satisfying assignments for 3SAT Boolean satisfiability expressions. One algorithm is presented for determining an instance of a satisfying assignment, where such exists. The algorithms are each deterministic and of polynomial complexity. The algorithms determining existence are complete as each produces a certificate of non-satisfiability, for instances where no satisfying assignment exists, and of satisfiability for such assignment does exist.",2002-05-24T21:09:09Z,http://arxiv.org/pdf/cs/0205064v6,2024-04-28,
cs/0208006v1,"Rectangle Size Bounds and Threshold Covers in Communication Complexity","We investigate the power of the most important lower bound technique in randomized communication complexity, which is based on an evaluation of the maximal size of approximately monochromatic rectangles, minimized over all distributions on the inputs. While it is known that the 0-error version of this bound is polynomially tight for deterministic communication, nothing in this direction is known for constant error and randomized communication complexity. We first study a one-sided version of this bound and obtain that its value lies between the MA- and AM-complexities of the considered function. Hence the lower bound actually works for a (communication complexity) class between MA cap co-MA and AM cap co-AM. We also show that the MA-complexity of the disjointness problem is Omega(sqrt(n)). Following this we consider the conjecture that the lower bound method is polynomially tight for randomized communication complexity. First we disprove a distributional version of this conjecture. Then we give a combinatorial characterization of the value of the lower bound method, in which the optimization over all distributions is absent. This characterization is done by what we call a uniform threshold cover. We also study relaxations of this notion, namely approximate majority covers and majority covers, and compare these three notions in power, exhibiting exponential separations. Each of these covers captures a lower bound method previously used for randomized communication complexity.",2002-08-05T14:36:27Z,http://arxiv.org/pdf/cs/0208006v1,2024-04-28,
cs/0208018v2,"Does P = NP?","This paper considers the question of P = NP in context of the polynomial time SAT algorithm. It posits proposition dependent on existence of conjectured problem that even where the algorithm is shown to solve SAT in polynomial time it remains theoretically possible for there to yet exist a non-deterministically polynomial (NP) problem for which the algorithm does not provide a polynomial (P) time solution. The paper leaves open as subject of continuing research the question of existence of instance of conjectured problem.",2002-08-12T04:32:36Z,http://arxiv.org/pdf/cs/0208018v2,2024-04-28,
cs/0208043v1,"Gales Suffice for Constructive Dimension","Supergales, generalizations of supermartingales, have been used by Lutz (2002) to define the constructive dimensions of individual binary sequences. Here it is shown that gales, the corresponding generalizations of martingales, can be equivalently used to define constructive dimension.",2002-08-29T20:48:14Z,http://arxiv.org/pdf/cs/0208043v1,2024-04-28,
cs/0208044v1,"Gales and supergales are equivalent for defining constructive Hausdorff dimension","We show that for a wide range of probability measures, constructive gales are interchangable with constructive supergales for defining constructive Hausdorff dimension, thus generalizing a previous independent result of Hitchcock (cs.CC/0208043) and partially answering an open question of Lutz (cs.CC/0203017).",2002-08-29T21:25:47Z,http://arxiv.org/pdf/cs/0208044v1,2024-04-28,
cs/0209015v2,"Does NP not equal P?","Stephen Cook posited SAT is NP-Complete in 1971. If SAT is NP-Complete then, as is generally accepted, any polynomial solution of it must also present a polynomial solution of all NP decision problems. It is here argued, however, that NP is not of necessity equivalent to P where it is shown that SAT is contained in P. This due to a paradox, of nature addressed by both Godel and Russell, in regards to the P-NP system in total.",2002-09-10T16:08:21Z,http://arxiv.org/pdf/cs/0209015v2,2024-04-28,
cs/0210008v1,"Cellular automata and communication complexity","The model of cellular automata is fascinating because very simple local rules can generate complex global behaviors. The relationship between local and global function is subject of many studies. We tackle this question by using results on communication complexity theory and, as a by-product, we provide (yet another) classification of cellular automata.",2002-10-11T16:29:19Z,http://arxiv.org/pdf/cs/0210008v1,2024-04-28,
cs/0211012v2,"Phase Transitions and all that","The paper (as posted originally) contains several errors. It has been subsequently split into two papers, the corrected (and accepted for publication) versions appear in the archive as papers cs.CC/0503082 and cs.DM/0503083.",2002-11-12T21:56:15Z,http://arxiv.org/pdf/cs/0211012v2,2024-04-28,
cs/0211025v3,"Effective Strong Dimension, Algorithmic Information, and Computational Complexity","The two most important notions of fractal dimension are {\it Hausdorff dimension}, developed by Hausdorff (1919), and {\it packing dimension}, developed by Tricot (1982).   Lutz (2000) has recently proven a simple characterization of Hausdorff dimension in terms of {\it gales}, which are betting strategies that generalize martingales. Imposing various computability and complexity constraints on these gales produces a spectrum of effective versions of Hausdorff dimension.   In this paper we show that packing dimension can also be characterized in terms of gales. Moreover, even though the usual definition of packing dimension is considerably more complex than that of Hausdorff dimension, our gale characterization of packing dimension is an exact dual of -- and every bit as simple as -- the gale characterization of Hausdorff dimension.   Effectivizing our gale characterization of packing dimension produces a variety of {\it effective strong dimensions}, which are exact duals of the effective dimensions mentioned above.   We develop the basic properties of effective strong dimensions and prove a number of results relating them to fundamental aspects of randomness, Kolmogorov complexity, prediction, Boolean circuit-size complexity, polynomial-time degrees, and data compression.",2002-11-21T04:46:02Z,http://arxiv.org/pdf/cs/0211025v3,2024-04-28,
cs/0211026v2,"How long is a Proof? - A short note","Withdrawn. Silly notion and out of context.",2002-11-21T10:26:44Z,http://arxiv.org/pdf/cs/0211026v2,2024-04-28,
cs/0211032v2,"Solution Bounds for a Hypothetical Polynomial Time Aproximation Algorithm for the TSP","Bounds for the optimal tour length for a hypothetical TSP algorithm are derived.",2002-11-25T08:24:37Z,http://arxiv.org/pdf/cs/0211032v2,2024-04-28,
cs/0212001v1,"Traveling Salesmen in the Presence of Competition","We propose the ``Competing Salesmen Problem'' (CSP), a 2-player competitive version of the classical Traveling Salesman Problem. This problem arises when considering two competing salesmen instead of just one. The concern for a shortest tour is replaced by the necessity to reach any of the customers before the opponent does. In particular, we consider the situation where players take turns, moving along one edge at a time within a graph G=(V,E). The set of customers is given by a subset V_C V of the vertices. At any given time, both players know of their opponent's position. A player wins if he is able to reach a majority of the vertices in V_C before the opponent does. We prove that the CSP is PSPACE-complete, even if the graph is bipartite, and both players start at distance 2 from each other. We show that the starting player may lose the game, even if both players start from the same vertex. For bipartite graphs, we show that the starting player always can avoid a loss. We also show that the second player can avoid to lose by more than one customer, when play takes place on a graph that is a tree T, and V_C consists of leaves of T. For the case where T is a star and V_C consists of n leaves of T, we give a simple and fast strategy which is optimal for both players. If V_C consists not only of leaves, the situation is more involved.",2002-12-03T17:42:08Z,http://arxiv.org/pdf/cs/0212001v1,2024-04-28,
cs/0212016v3,"Complexity of the Exact Domatic Number Problem and of the Exact Conveyor Flow Shop Problem","We prove that the exact versions of the domatic number problem are complete for the levels of the boolean hierarchy over NP. The domatic number problem, which arises in the area of computer networks, is the problem of partitioning a given graph into a maximum number of disjoint dominating sets. This number is called the domatic number of the graph. We prove that the problem of determining whether or not the domatic number of a given graph is {\em exactly} one of k given values is complete for the 2k-th level of the boolean hierarchy over NP. In particular, for k = 1, it is DP-complete to determine whether or not the domatic number of a given graph equals exactly a given integer. Note that DP is the second level of the boolean hierarchy over NP. We obtain similar results for the exact versions of generalized dominating set problems and of the conveyor flow shop problem. Our reductions apply Wagner's conditions sufficient to prove hardness for the levels of the boolean hierarchy over NP.",2002-12-09T19:46:02Z,http://arxiv.org/pdf/cs/0212016v3,2024-04-28,
cs/0212056v1,"On the Work of Madhu Sudan: the 2002 Nevalinna Prize Winner","Madhu Sudan's work spans many areas of computer science theory including computational complexity theory, the design of efficient algorithms, algorithmic coding theory, and the theory of program checking and correcting.   Two results of Sudan stand out in the impact they have had on the mathematics of computation. The first work shows a probabilistic characterization of the class NP -- those sets for which short and easily checkable proofs of membership exist, and demonstrates consequences of this characterization to classifying the complexity of approximation problems. The second work shows a polynomial time algorithm for list decoding the Reed Solomon error correcting codes.   This short note will be devoted to describing Sudan's work on probabilistically checkable proofs -- the so called {\it PCP theorem} and its implications.",2002-12-01T00:00:00Z,http://arxiv.org/pdf/cs/0212056v1,2024-04-28,
cs/0301012v1,"Hard satisfiable formulas for DPLL-type algorithms","We address lower bounds on the time complexity of algorithms solving the propositional satisfiability problem. Namely, we consider two DPLL-type algorithms, enhanced with the unit clause and pure literal heuristics. Exponential lower bounds for solving satisfiability on provably satisfiable formulas are proven.",2003-01-15T08:44:21Z,http://arxiv.org/pdf/cs/0301012v1,2024-04-28,
cs/0301013v1,"Independence Properties of Algorithmically Random Sequences","A bounded Kolmogorov-Loveland selection rule is an adaptive strategy for recursively selecting a subsequence of an infinite binary sequence; such a subsequence may be interpreted as the query sequence of a time-bounded Turing machine. In this paper we show that if A is an algorithmically random sequence, A_0 is selected from A via a bounded Kolmogorov-Loveland selection rule, and A_1 denotes the sequence of nonselected bits of A, then A_1 is independent of A_0; that is, A_1 is algorithmically random relative to A_0. This result has been used by Kautz and Miltersen [1] to show that relative to a random oracle, NP does not have p-measure zero (in the sense of Lutz [2]).   [1] S. M. Kautz and P. B. Miltersen. Relative to a random oracle, NP is not small. Journal of Computer and System Sciences, 53:235-250, 1996.   [2] J. H. Lutz. Almost everywhere high nonuniform complexity. Journal of Computer and System Sciences, 44:220-258, 1992.",2003-01-16T02:56:59Z,http://arxiv.org/pdf/cs/0301013v1,2024-04-28,
cs/0301016v1,"Lower Bounds on the Bounded Coefficient Complexity of Bilinear Maps","We prove lower bounds of order $n\log n$ for both the problem to multiply polynomials of degree $n$, and to divide polynomials with remainder, in the model of bounded coefficient arithmetic circuits over the complex numbers. These lower bounds are optimal up to order of magnitude. The proof uses a recent idea of R. Raz [Proc. 34th STOC 2002] proposed for matrix multiplication. It reduces the linear problem to multiply a random circulant matrix with a vector to the bilinear problem of cyclic convolution. We treat the arising linear problem by extending J. Morgenstern's bound [J. ACM 20, pp. 305-306, 1973] in a unitarily invariant way. This establishes a new lower bound on the bounded coefficient complexity of linear forms in terms of the singular values of the corresponding matrix. In addition, we extend these lower bounds for linear and bilinear maps to a model of circuits that allows a restricted number of unbounded scalar multiplications.",2003-01-16T15:36:00Z,http://arxiv.org/pdf/cs/0301016v1,2024-04-28,
cs/0304020v2,"A direct sum theorem in communication complexity via message compression","We prove lower bounds for the direct sum problem for two-party bounded error randomised multiple-round communication protocols. Our proofs use the notion of information cost of a protocol, as defined by Chakrabarti, Shi, Wirth and Yao and refined further by Bar-Yossef, Jayram, Kumar and Sivakumar. Our main technical result is a `compression' theorem saying that, for any probability distribution $\mu$ over the inputs, a $k$-round private coin bounded error protocol for a function $f$ with information cost $c$ can be converted into a $k$-round deterministic protocol for $f$ with bounded distributional error and communication cost $O(kc)$. We prove this result using a substate theorem about relative entropy and a rejection sampling argument. Our direct sum result follows from this `compression' result via elementary information theoretic arguments.   We also consider the direct sum problem in quantum communication. Using a probabilistic argument, we show that messages cannot be compressed in this manner even if they carry small information. Hence, new techniques may be necessary to tackle the direct sum problem in quantum communication.",2003-04-12T02:26:26Z,http://arxiv.org/pdf/cs/0304020v2,2024-04-28,
cs/0304026v1,"A New Multilayered PCP and the Hardness of Hypergraph Vertex Cover","Given a $k$-uniform hyper-graph, the E$k$-Vertex-Cover problem is to find the smallest subset of vertices that intersects every hyper-edge. We present a new multilayered PCP construction that extends the Raz verifier. This enables us to prove that E$k$-Vertex-Cover is NP-hard to approximate within factor $(k-1-\epsilon)$ for any $k \geq 3$ and any $\epsilon>0$. The result is essentially tight as this problem can be easily approximated within factor $k$. Our construction makes use of the biased Long-Code and is analyzed using combinatorial properties of $s$-wise $t$-intersecting families of subsets.",2003-04-19T17:59:33Z,http://arxiv.org/pdf/cs/0304026v1,2024-04-28,
cs/0304030v1,"Small Spans in Scaled Dimension","Juedes and Lutz (1995) proved a small span theorem for polynomial-time many-one reductions in exponential time. This result says that for language A decidable in exponential time, either the class of languages reducible to A (the lower span) or the class of problems to which A can be reduced (the upper span) is small in the sense of resource-bounded measure and, in particular, that the degree of A is small. Small span theorems have been proven for increasingly stronger polynomial-time reductions, and a small span theorem for polynomial-time Turing reductions would imply BPP != EXP. In contrast to the progress in resource-bounded measure, Ambos-Spies, Merkle, Reimann, and Stephan (2001) showed that there is no small span theorem for the resource-bounded dimension of Lutz (2000), even for polynomial-time many-one reductions.   Resource-bounded scaled dimension, recently introduced by Hitchcock, Lutz, and Mayordomo (2003), provides rescalings of resource-bounded dimension. We use scaled dimension to further understand the contrast between measure and dimension regarding polynomial-time spans and degrees. We strengthen prior results by showing that the small span theorem holds for polynomial-time many-one reductions in the -3rd-order scaled dimension, but fails to hold in the -2nd-order scaled dimension. Our results also hold in exponential space.   As an application, we show that determining the -2nd- or -1st-order scaled dimension in ESPACE of the many-one complete languages for E would yield a proof of P = BPP or P != PSPACE. On the other hand, it is shown unconditionally that the complete languages for E have -3rd-order scaled dimension 0 in ESPACE and -2nd- and -1st-order scaled dimension 1 in E.",2003-04-22T19:43:35Z,http://arxiv.org/pdf/cs/0304030v1,2024-04-28,
cs/0304038v1,"How NP got a new definition: a survey of probabilistically checkable proofs","We survey a collective achievement of a group of researchers: the PCP Theorems. They give new definitions of the class \np, and imply that computing approximate solutions to many \np-hard problems is itself \np-hard. Techniques developed to prove them have had many other consequences.",2003-04-28T22:49:52Z,http://arxiv.org/pdf/cs/0304038v1,2024-04-28,
cs/0304039v1,"Approximation thresholds for combinatorial optimization problems","An NP-hard combinatorial optimization problem $\Pi$ is said to have an {\em approximation threshold} if there is some $t$ such that the optimal value of $\Pi$ can be approximated in polynomial time within a ratio of $t$, and it is NP-hard to approximate it within a ratio better than $t$. We survey some of the known approximation threshold results, and discuss the pattern that emerges from the known results.",2003-04-28T22:50:21Z,http://arxiv.org/pdf/cs/0304039v1,2024-04-28,
cs/0304040v1,"Hardness as randomness: a survey of universal derandomization","We survey recent developments in the study of probabilistic complexity classes. While the evidence seems to support the conjecture that probabilism can be deterministically simulated with relatively low overhead, i.e., that $P=BPP$, it also indicates that this may be a difficult question to resolve. In fact, proving that probabilistic algorithms have non-trivial deterministic simulations is basically equivalent to proving circuit lower bounds, either in the algebraic or Boolean models.",2003-04-28T22:50:34Z,http://arxiv.org/pdf/cs/0304040v1,2024-04-28,
cs/0304041v1,"$P \ne NP$, propositional proof complexity, and resolution lower bounds for the weak pigeonhole principle","Recent results established exponential lower bounds for the length of any Resolution proof for the weak pigeonhole principle. More formally, it was proved that any Resolution proof for the weak pigeonhole principle, with $n$ holes and any number of pigeons, is of length $\Omega(2^{n^{\epsilon}})$, (for a constant $\epsilon = 1/3$). One corollary is that certain propositional formulations of the statement $P \ne NP$ do not have short Resolution proofs. After a short introduction to the problem of $P \ne NP$ and to the research area of propositional proof complexity, I will discuss the above mentioned lower bounds for the weak pigeonhole principle and the connections to the hardness of proving $P \ne NP$.",2003-04-28T22:51:57Z,http://arxiv.org/pdf/cs/0304041v1,2024-04-28,
cs/0304044v1,"Hardness of approximating the weight enumerator of a binary linear code","We consider the problem of evaluation of the weight enumerator of a binary linear code. We show that the exact evaluation is hard for polynomial hierarchy. More exactly, if WE is an oracle answering the solution of the evaluation problem then P^WE=P^GapP. Also we consider the approximative evaluation of the weight enumerator. In the case of approximation with additive accuracy $2^{\alpha n}$, $\alpha$ is constant the problem is hard in the above sense. We also prove that approximate evaluation at a single point $e^{\pi i/4}$ is hard for $0<\al<\al_0\approx0.88$.",2003-04-30T03:13:23Z,http://arxiv.org/pdf/cs/0304044v1,2024-04-28,
cs/0305035v27,"A mathematical definition of ""simplify""","Even though every mathematician knows intuitively what it means to ""simplify"" a mathematical expression, there is still no universally accepted rigorous mathematical definition of ""simplify"". In this paper, we shall give a simple and plausible definition of ""simplify"" in terms of the computational complexity of integer functions. We shall also use this definition to show that there is no deterministic and exact algorithm which can compute the permanent of an $n \times n$ matrix in $o(2^n)$ time.",2003-05-19T16:02:54Z,http://arxiv.org/pdf/cs/0305035v27,2024-04-28,
cs/0306131v1,"Complexity of Cycle Length Modularity Problems in Graphs","The even cycle problem for both undirected and directed graphs has been the topic of intense research in the last decade. In this paper, we study the computational complexity of \emph{cycle length modularity problems}. Roughly speaking, in a cycle length modularity problem, given an input (undirected or directed) graph, one has to determine whether the graph has a cycle $C$ of a specific length (or one of several different lengths), modulo a fixed integer. We denote the two families (one for undirected graphs and one for directed graphs) of problems by $(S,m)\hbox{-}{\rm UC}$ and $(S,m)\hbox{-}{\rm DC}$, where $m \in \mathcal{N}$ and $S \subseteq \{0,1, ..., m-1\}$. $(S,m)\hbox{-}{\rm UC}$ (respectively, $(S,m)\hbox{-}{\rm DC}$) is defined as follows: Given an undirected (respectively, directed) graph $G$, is there a cycle in $G$ whose length, modulo $m$, is a member of $S$? In this paper, we fully classify (i.e., as either polynomial-time solvable or as ${\rm NP}$-complete) each problem $(S,m)\hbox{-}{\rm UC}$ such that $0 \in S$ and each problem $(S,m)\hbox{-}{\rm DC}$ such that $0 \notin S$. We also give a sufficient condition on $S$ and $m$ for the following problem to be polynomial-time computable: $(S,m)\hbox{-}{\rm UC}$ such that $0 \notin S$.",2003-06-25T15:37:43Z,http://arxiv.org/pdf/cs/0306131v1,2024-04-28,
cs/0307020v1,"Defying Dimensions Mod 6","We show that a certain representation of the matrix-product can be computed with $n^{o(1)}$ multiplications. We also show, that siumilar representations of matrices can be compressed enormously.",2003-07-08T15:10:37Z,http://arxiv.org/pdf/cs/0307020v1,2024-04-28,
cs/0309052v1,"Minimal DFAs for Testing Divisibility","We present and prove a theorem answering the question ""how many states does a minimal deterministic finite automaton (DFA) that recognizes the set of base-b numbers divisible by k have?""",2003-09-29T19:34:36Z,http://arxiv.org/pdf/cs/0309052v1,2024-04-28,
cs/0310046v3,"Theory of One Tape Linear Time Turing Machines","A theory of one-tape (one-head) linear-time Turing machines is essentially different from its polynomial-time counterpart since these machines are closely related to finite state automata. This paper discusses structural-complexity issues of one-tape Turing machines of various types (deterministic, nondeterministic, reversible, alternating, probabilistic, counting, and quantum Turing machines) that halt in linear time, where the running time of a machine is defined as the length of any longest computation path. We explore structural properties of one-tape linear-time Turing machines and clarify how the machines' resources affect their computational patterns and power.",2003-10-23T21:08:22Z,http://arxiv.org/pdf/cs/0310046v3,2024-04-28,
cs/0310060v19,"Puzzle: Zermelo-Fraenkel set theory is inconsistent","In this note, we present a puzzle. We prove that Zermelo-Fraenkel set theory is inconsistent by proving, using Zermelo-Fraenkel set theory, the false statement that any algorithm that determines whether any $n \times n$ matrix over $\mathbb F_2$, the finite field of order 2, is nonsingular must run in exponential time in the worst-case scenario. The object of the puzzle is to find the error in the proof.",2003-10-31T18:32:00Z,http://arxiv.org/pdf/cs/0310060v19,2024-04-28,
cs/0312039v3,"Uniform test of algorithmic randomness over a general space","The algorithmic theory of randomness is well developed when the underlying space is the set of finite or infinite sequences and the underlying probability distribution is the uniform distribution or a computable distribution. These restrictions seem artificial. Some progress has been made to extend the theory to arbitrary Bernoulli distributions (by Martin-Loef), and to arbitrary distributions (by Levin). We recall the main ideas and problems of Levin's theory, and report further progress in the same framework.   - We allow non-compact spaces (like the space of continuous functions, underlying the Brownian motion).   - The uniform test (deficiency of randomness) d_P(x) (depending both on the outcome x and the measure P should be defined in a general and natural way.   - We see which of the old results survive: existence of universal tests, conservation of randomness, expression of tests in terms of description complexity, existence of a universal measure, expression of mutual information as ""deficiency of independence.   - The negative of the new randomness test is shown to be a generalization of complexity in continuous spaces; we show that the addition theorem survives.   The paper's main contribution is introducing an appropriate framework for studying these questions and related ones (like statistics for a general family of distributions).",2003-12-17T19:25:30Z,http://arxiv.org/pdf/cs/0312039v3,2024-04-28,
cs/0404044v2,"A note on dimensions of polynomial size circuits","In this paper, we use resource-bounded dimension theory to investigate polynomial size circuits. We show that for every $i\geq 0$, $\Ppoly$ has $i$th order scaled $\pthree$-strong dimension 0. We also show that $\Ppoly^\io$ has $\pthree$-dimension 1/2, $\pthree$-strong dimension 1. Our results improve previous measure results of Lutz (1992) and dimension results of Hitchcock and Vinodchandran (2004).",2004-04-22T14:45:48Z,http://arxiv.org/pdf/cs/0404044v2,2024-04-28,
cs/0406009v1,"Implementation of Logical Functions in the Game of Life","The Game of Life cellular automaton is a classical example of a massively parallel collision-based computing device. The automaton exhibits mobile patterns, gliders, and generators of the mobile patterns, glider guns, in its evolution. We show how to construct the basic logical operations, AND, OR, NOT in space-time configurations of the cellular automaton. Also decomposition of complicated Boolean functions is discussed. Advantages of our technique are demonstrated on an example of binary adder, realized via collision of glider streams.",2004-06-04T14:53:51Z,http://arxiv.org/pdf/cs/0406009v1,2024-04-28,
cs/0406044v4,"On the Computational Complexity of the Forcing Chromatic Number","We consider vertex colorings of graphs in which adjacent vertices have distinct colors. A graph is $s$-chromatic if it is colorable in $s$ colors and any coloring of it uses at least $s$ colors. The forcing chromatic number $F(G)$ of an $s$-chromatic graph $G$ is the smallest number of vertices which must be colored so that, with the restriction that $s$ colors are used, every remaining vertex has its color determined uniquely. We estimate the computational complexity of $F(G)$ relating it to the complexity class US introduced by Blass and Gurevich. We prove that recognizing if $F(G)\le 2$ is US-hard with respect to polynomial-time many-one reductions. Moreover, this problem is coNP-hard even under the promises that $F(G)\le 3$ and $G$ is 3-chromatic. On the other hand, recognizing if $F(G)\le k$, for each constant $k$, is reducible to a problem in US via disjunctive truth-table reduction.   Similar results are obtained also for forcing variants of the clique and the domination numbers of a graph.",2004-06-23T15:21:46Z,http://arxiv.org/pdf/cs/0406044v4,2024-04-28,
cs/0409012v3,"A New Look at Survey Propagation and its Generalizations","This paper provides a new conceptual perspective on survey propagation, which is an iterative algorithm recently introduced by the statistical physics community that is very effective in solving random k-SAT problems even with densities close to the satisfiability threshold. We first describe how any SAT formula can be associated with a novel family of Markov random fields (MRFs), parameterized by a real number \rho \in [0,1]. We then show that applying belief propagation--a well-known ``message-passing'' technique for estimating marginal probabilities--to this family of MRFs recovers a known family of algorithms, ranging from pure survey propagation at one extreme (\rho = 1) to standard belief propagation on the uniform distribution over SAT assignments at the other extreme (\rho = 0). Configurations in these MRFs have a natural interpretation as partial satisfiability assignments, on which a partial order can be defined. We isolate cores as minimal elements in this partial ordering, which are also fixed points of survey propagation and the only assignments with positive probability in the MRF for \rho=1. Our experimental results for k=3 suggest that solutions of random formulas typically do not possess non-trivial cores. This makes it necessary to study the structure of the space of partial assignments for \rho<1 and investigate the role of assignments that are very close to being cores. To that end, we investigate the associated lattice structure, and prove a weight-preserving identity that shows how any MRF with \rho>0 can be viewed as a ``smoothed'' version of the uniform distribution over satisfying assignments (\rho=0). Finally, we isolate properties of Gibbs sampling and message-passing algorithms that are typical for an ensemble of k-SAT problems.",2004-09-08T01:27:14Z,http://arxiv.org/pdf/cs/0409012v3,2024-04-28,
cs/0409043v1,"Inapproximability of Combinatorial Optimization Problems","We survey results on the hardness of approximating combinatorial optimization problems.",2004-09-24T02:13:23Z,http://arxiv.org/pdf/cs/0409043v1,2024-04-28,
cs/0410035v1,"Overhead-Free Computation, DCFLs, and CFLs","We study Turing machines that are allowed absolutely no space overhead. The only work space the machines have, beyond the fixed amount of memory implicit in their finite-state control, is that which they can create by cannibalizing the input bits' own space. This model more closely reflects the fixed-sized memory of real computers than does the standard complexity-theoretic model of linear space.   Though some context-sensitive languages cannot be accepted by such machines, we show that all context-free languages can be accepted nondeterministically in polynomial time with absolutely no space overhead, and that all deterministic context-free languages can be accepted deterministically in polynomial time with absolutely no space overhead.",2004-10-15T18:18:22Z,http://arxiv.org/pdf/cs/0410035v1,2024-04-28,
cs/0410037v1,"Hardware-Oriented Group Solutions for Hard Problems","Group and individual solutions are considered for hard problems such as satisfiability problem. Time-space trade-off in a structured active memory provides means to achieve lower time complexity for solutions of these problems.",2004-10-15T23:12:12Z,http://arxiv.org/pdf/cs/0410037v1,2024-04-28,
cs/0410057v2,"Generalized Counters and Reversal Complexity","We generalize the definition of a counter and counter reversal complexity and investigate the power of generalized deterministic counter automata in terms of language recognition.",2004-10-25T19:36:07Z,http://arxiv.org/pdf/cs/0410057v2,2024-04-28,
cs/0411007v1,"Basic properties for sand automata","We prove several results about the relations between injectivity and surjectivity for sand automata. Moreover, we begin the exploration of the dynamical behavior of sand automata proving that the property of nilpotency is undecidable. We believe that the proof technique used for this last result might reveal useful for many other results in this context.",2004-11-04T12:33:37Z,http://arxiv.org/pdf/cs/0411007v1,2024-04-28,
cs/0411033v1,"On Invariance and Convergence in Time Complexity theory","This article introduces three invariance principles under which P is different from NP. In the second part a theorem of convergence is proven. This theorem states that for any language L there exists an infinite sequence of languages from O(n) that converges to L.",2004-11-11T21:32:52Z,http://arxiv.org/pdf/cs/0411033v1,2024-04-28,
cs/0411037v2,"A Note on Bulk Quantum Turing Machine","Recently, among experiments for realization of quantum computers, NMR quantum computers have achieved the most impressive succession. There is a model of the NMR quantum computation,namely Atsumi and Nishino's bulk quantum Turing Machine. It assumes, however, an unnatural assumption with quantum mechanics. We, then, define a more natural and quantum mechanically realizable modified bulk quantum Turing Machine, and show its computational ability by comparing complexity classes with quantum Turing Machine's counter part.",2004-11-12T18:56:15Z,http://arxiv.org/pdf/cs/0411037v2,2024-04-28,
cs/0412022v3,"Zeno machines and hypercomputation","This paper reviews the Church-Turing Thesis (or rather, theses) with reference to their origin and application and considers some models of ""hypercomputation"", concentrating on perhaps the most straight-forward option: Zeno machines (Turing machines with accelerating clock). The halting problem is briefly discussed in a general context and the suggestion that it is an inevitable companion of any reasonable computational model is emphasised. It is hinted that claims to have ""broken the Turing barrier"" could be toned down and that the important and well-founded role of Turing computability in the mathematical sciences stands unchallenged.",2004-12-06T12:18:05Z,http://arxiv.org/pdf/cs/0412022v3,2024-04-28,
cs/0412042v1,"The approximability of three-valued MAX CSP","In the maximum constraint satisfaction problem (Max CSP), one is given a finite collection of (possibly weighted) constraints on overlapping sets of variables, and the goal is to assign values from a given domain to the variables so as to maximize the number (or the total weight, for the weighted case) of satisfied constraints. This problem is NP-hard in general, and, therefore, it is natural to study how restricting the allowed types of constraints affects the approximability of the problem. It is known that every Boolean (that is, two-valued) Max CSP problem with a finite set of allowed constraint types is either solvable exactly in polynomial time or else APX-complete (and hence can have no polynomial time approximation scheme unless P=NP. It has been an open problem for several years whether this result can be extended to non-Boolean Max CSP, which is much more difficult to analyze than the Boolean case. In this paper, we make the first step in this direction by establishing this result for Max CSP over a three-element domain. Moreover, we present a simple description of all polynomial-time solvable cases of our problem. This description uses the well-known algebraic combinatorial property of supermodularity. We also show that every hard three-valued Max CSP problem contains, in a certain specified sense, one of the two basic hard Max CSP problems which are the Maximum k-colourable subgraph problems for k=2,3.",2004-12-10T15:34:54Z,http://arxiv.org/pdf/cs/0412042v1,2024-04-28,
cs/0412048v1,"On computing fixed points for generalized sandpiles","We prove fixed points results for sandpiles starting with arbitrary initial conditions. We give an effective algorithm for computing such fixed points, and we refine it in the particular case of SPM.",2004-12-11T07:51:35Z,http://arxiv.org/pdf/cs/0412048v1,2024-04-28,
cs/0412062v2,"Isomorphic Implication","We study the isomorphic implication problem for Boolean constraints. We show that this is a natural analog of the subgraph isomorphism problem. We prove that, depending on the set of constraints, this problem is in P, NP-complete, or NP-hard, coNP-hard, and in parallel access to NP. We show how to extend the NP-hardness and coNP-hardness to hardness for parallel access to NP for some cases, and conjecture that this can be done in all cases.",2004-12-14T17:15:29Z,http://arxiv.org/pdf/cs/0412062v2,2024-04-28,
cs/0412096v1,"Complexity of Self-Assembled Shapes","The connection between self-assembly and computation suggests that a shape can be considered the output of a self-assembly ``program,'' a set of tiles that fit together to create a shape. It seems plausible that the size of the smallest self-assembly program that builds a shape and the shape's descriptional (Kolmogorov) complexity should be related. We show that when using a notion of a shape that is independent of scale, this is indeed so: in the Tile Assembly Model, the minimal number of distinct tile types necessary to self-assemble a shape, at some scale, can be bounded both above and below in terms of the shape's Kolmogorov complexity. As part of the proof of the main result, we sketch a general method for converting a program outputting a shape as a list of locations into a set of tile types that self-assembles into a scaled up version of that shape. Our result implies, somewhat counter-intuitively, that self-assembly of a scaled-up version of a shape often requires fewer tile types. Furthermore, the independence of scale in self-assembly theory appears to play the same crucial role as the independence of running time in the theory of computability. This leads to an elegant formulation of languages of shapes generated by self-assembly. Considering functions from integers to shapes, we show that the running-time complexity, with respect to Turing machines, is polynomially equivalent to the scale complexity of the same function implemented via self-assembly by a finite set of tile types. Our results also hold for shapes defined by Wang tiling -- where there is no sense of a self-assembly process -- except that here time complexity must be measured with respect to non-deterministic Turing machines.",2004-12-21T10:30:46Z,http://arxiv.org/pdf/cs/0412096v1,2024-04-28,
cs/0412097v1,"The Computational Power of Benenson Automata","The development of autonomous molecular computers capable of making independent decisions in vivo regarding local drug administration may revolutionize medical science. Recently Benenson at el (2004) have envisioned one form such a ``smart drug'' may take by implementing an in vitro scheme, in which a long DNA state molecule is cut repeatedly by a restriction enzyme in a manner dependent upon the presence of particular short DNA ``rule molecules.'' To analyze the potential of their scheme in terms of the kinds of computations it can perform, we study an abstraction assuming that a certain class of restriction enzymes is available and reactions occur without error. We also discuss how our molecular algorithms could perform with known restriction enzymes. By exhibiting a way to simulate arbitrary circuits, we show that these ``Benenson automata'' are capable of computing arbitrary Boolean functions. Further, we show that they are able to compute efficiently exactly those functions computable by log-depth circuits. Computationally, we formalize a new variant of limited width branching programs with a molecular implementation.",2004-12-21T10:57:15Z,http://arxiv.org/pdf/cs/0412097v1,2024-04-28,
cs/0501009v1,"On The Liniar Time Complexity of Finite Languages","The present paper presents and proves a proposition concerning the time complexity of finite languages. It is shown herein, that for any finite language (a language for which the set of words composing it is finite) there is a Turing machine that computes the language in such a way that for any input of length k the machine stops in, at most, k + 1 steps.",2005-01-05T19:13:05Z,http://arxiv.org/pdf/cs/0501009v1,2024-04-28,
cs/0501022v1,"Algebraic Properties for Selector Functions","The nondeterministic advice complexity of the P-selective sets is known to be exactly linear. Regarding the deterministic advice complexity of the P-selective sets--i.e., the amount of Karp--Lipton advice needed for polynomial-time machines to recognize them in general--the best current upper bound is quadratic [Ko, 1983] and the best current lower bound is linear [Hemaspaandra and Torenvliet, 1996].   We prove that every associatively P-selective set is commutatively, associatively P-selective. Using this, we establish an algebraic sufficient condition for the P-selective sets to have a linear upper bound (which thus would match the existing lower bound) on their deterministic advice complexity: If all P-selective sets are associatively P-selective then the deterministic advice complexity of the P-selective sets is linear. The weakest previously known sufficient condition was P=NP.   We also establish related results for algebraic properties of, and advice complexity of, the nondeterministically selective sets.",2005-01-11T23:55:17Z,http://arxiv.org/pdf/cs/0501022v1,2024-04-28,
cs/0501026v1,"On the Sensitivity of Cyclically-Invariant Boolean Functions","In this paper we construct a cyclically invariant Boolean function whose sensitivity is $\Theta(n^{1/3})$. This result answers two previously published questions. Tur\'an (1984) asked if any Boolean function, invariant under some transitive group of permutations, has sensitivity $\Omega(\sqrt{n})$. Kenyon and Kutin (2004) asked whether for a ``nice'' function the product of 0-sensitivity and 1-sensitivity is $\Omega(n)$. Our function answers both questions in the negative.   We also prove that for minterm-transitive functions (a natural class of Boolean functions including our example) the sensitivity is $\Omega(n^{1/3})$. Hence for this class of functions sensitivity and block sensitivity are polynomially related.",2005-01-13T22:06:25Z,http://arxiv.org/pdf/cs/0501026v1,2024-04-28,
cs/0502030v36,"Fixed Type Theorems","This submission has been withdrawn at the request of the author.",2005-02-05T17:13:40Z,http://arxiv.org/pdf/cs/0502030v36,2024-04-28,
cs/0502068v1,"Limits of Rush Hour Logic Complexity","Rush Hour Logic was introduced in [Flake&Baum99] as a model of computation inspired by the ``Rush Hour'' toy puzzle, in which cars can move horizontally or vertically within a parking lot. The authors show how the model supports polynomial space computation, using certain car configurations as building blocks to construct boolean circuits for a cpu and memory. They consider the use of cars of length 3 crucial to their construction, and conjecture that cars of size 2 only, which we'll call `Size 2 Rush Hour', do not support polynomial space computation. We settle this conjecture by showing that the required building blocks are constructible in Size 2 Rush Hour. Furthermore, we consider Unit Rush Hour, which was hitherto believed to be trivial, show its relation to maze puzzles, and provide empirical support for its hardness.",2005-02-15T14:22:35Z,http://arxiv.org/pdf/cs/0502068v1,2024-04-28,
cs/0503034v1,"Comment on ""Some non-conventional ideas about algorithmic complexity""","We comment on a recent paper by D'Abramo [Chaos, Solitons & Fractals, 25 (2005) 29], focusing on the author's statement that an algorithm can produce a list of strings containing at least one string whose algorithmic complexity is greater than that of the entire list. We show that this statement, although perplexing, is not as paradoxical as it seems when the definition of algorithmic complexity is applied correctly.",2005-03-16T11:09:06Z,http://arxiv.org/pdf/cs/0503034v1,2024-04-28,
cs/0503049v3,"Enforcing and Defying Associativity, Commutativity, Totality, and Strong Noninvertibility for One-Way Functions in Complexity Theory","Rabi and Sherman [RS97,RS93] proved that the hardness of factoring is a sufficient condition for there to exist one-way functions (i.e., p-time computable, honest, p-time noninvertible functions; this paper is in the worst-case model, not the average-case model) that are total, commutative, and associative but not strongly noninvertible. In this paper we improve the sufficient condition to ``P does not equal NP.''   More generally, in this paper we completely characterize which types of one-way functions stand or fall together with (plain) one-way functions--equivalently, stand or fall together with P not equaling NP. We look at the four attributes used in Rabi and Sherman's seminal work on algebraic properties of one-way functions (see [RS97,RS93]) and subsequent papers--strongness (of noninvertibility), totality, commutativity, and associativity--and for each attribute, we allow it to be required to hold, required to fail, or ``don't care.'' In this categorization there are 3^4 = 81 potential types of one-way functions. We prove that each of these 81 feature-laden types stand or fall together with the existence of (plain) one-way functions.",2005-03-21T19:19:54Z,http://arxiv.org/pdf/cs/0503049v3,2024-04-28,
cs/0504088v1,"Time, Space, and Energy in Reversible Computing","We survey results of a quarter century of work on computation by reversible general-purpose computers (in this setting Turing machines), and general reversible simulation of irreversible computations, with respect to energy-, time- and space requirements.",2005-04-20T17:13:33Z,http://arxiv.org/pdf/cs/0504088v1,2024-04-28,
cs/0504096v2,"P-Selectivity, Immunity, and the Power of One Bit","We prove that P-sel, the class of all P-selective sets, is EXP-immune, but is not EXP/1-immune. That is, we prove that some infinite P-selective set has no infinite EXP-time subset, but we also prove that every infinite P-selective set has some infinite subset in EXP/1. Informally put, the immunity of P-sel is so fragile that it is pierced by a single bit of information.   The above claims follow from broader results that we obtain about the immunity of the P-selective sets. In particular, we prove that for every recursive function f, P-sel is DTIME(f)-immune. Yet we also prove that P-sel is not \Pi_2^p/1-immune.",2005-04-25T15:18:17Z,http://arxiv.org/pdf/cs/0504096v2,2024-04-28,
cs/0505076v1,"On the Solution of Graph Isomorphism by Dynamical Algorithms","In the recent years, several polynomial algorithms of a dynamical nature have been proposed to address the graph isomorphism problem. In this paper we propose a generalization of an approach exposed in cond-mat/0209112 and find that this dynamical algorithm is covered by a combinatorial approach. It is possible to infer that polynomial dynamical algorithms addressing graph isomorphism are covered by suitable polynomial combinatorial approaches and thus are tackled by the same weaknesses as the last ones.",2005-05-27T00:09:12Z,http://arxiv.org/pdf/cs/0505076v1,2024-04-28,
cs/0505079v1,"Application of Kolmogorov complexity and universal codes to identity testing and nonparametric testing of serial independence for time series","We show that Kolmogorov complexity and such its estimators as universal codes (or data compression methods) can be applied for hypotheses testing in a framework of classical mathematical statistics. The methods for identity testing and nonparametric testing of serial independence for time series are suggested.",2005-05-29T17:12:47Z,http://arxiv.org/pdf/cs/0505079v1,2024-04-28,
cs/0506081v2,"Three lines proof of the lower bound for the matrix rigidity","The rigidity of a matrix describes the minimal number of entries one has to change to reduce matrix's rank to r. We give very simple combinatorial proof of the lower bound for the rigidity of Sylvester (special case of Hadamard) matrix that matches the best known result by de Wolf(2005) for Hadamard matrices proved by quantum information theoretical arguments.",2005-06-20T19:54:48Z,http://arxiv.org/pdf/cs/0506081v2,2024-04-28,
cs/0506082v1,"Open Questions in the Theory of Semifeasible Computation","The study of semifeasible algorithms was initiated by Selman's work a quarter of century ago [Sel79,Sel81,Sel82]. Informally put, this research stream studies the power of those sets L for which there is a deterministic (or in some cases, the function may belong to one of various nondeterministic function classes) polynomial-time function f such that when at least one of x and y belongs to L, then f(x,y) \in L \cap \{x,y\}. The intuition here is that it is saying: ``Regarding membership in L, if you put a gun to my head and forced me to bet on one of x or y as belonging to L, my money would be on f(x,y).''   In this article, we present a number of open problems from the theory of semifeasible algorithms. For each we present its background and review what partial results, if any, are known.",2005-06-20T22:33:51Z,http://arxiv.org/pdf/cs/0506082v1,2024-04-28,
cs/0506090v1,"An Exact 2.9416^n Algorithm for the Three Domatic Number Problem","The three domatic number problem asks whether a given undirected graph can be partitioned into at least three dominating sets, i.e., sets whose closed neighborhood equals the vertex set of the graph. Since this problem is NP-complete, no polynomial-time algorithm is known for it. The naive deterministic algorithm for this problem runs in time 3^n, up to polynomial factors. In this paper, we design an exact deterministic algorithm for this problem running in time 2.9416^n. Thus, our algorithm can handle problem instances of larger size than the naive algorithm in the same amount of time. We also present another deterministic and a randomized algorithm for this problem that both have an even better performance for graphs with small maximum degree.",2005-06-24T12:58:15Z,http://arxiv.org/pdf/cs/0506090v1,2024-04-28,
cs/0506100v1,"On the NP-Completeness of Some Graph Cluster Measures","Graph clustering is the problem of identifying sparsely connected dense subgraphs (clusters) in a given graph. Proposed clustering algorithms usually optimize various fitness functions that measure the quality of a cluster within the graph. Examples of such cluster measures include the conductance, the local and relative densities, and single cluster editing. We prove that the decision problems associated with the optimization tasks of finding the clusters that are optimal with respect to these fitness measures are NP-complete.",2005-06-29T18:12:28Z,http://arxiv.org/pdf/cs/0506100v1,2024-04-28,
cs/0507057v1,"A new sibling of BQP","We present a new quantum complexity class, called MQ^2, which is contained in AWPP. This class has a compact and simple mathematical definition, involving only polynomial-time computable functions and a unitarity condition. It contains both Deutsch-Jozsa's and Shor's algorithm, while its relation to BQP is unknown. This shows that in the complexity class hierarchy, BQP is not an extraordinary isolated island, but has ''siblings'' which as well can solve prime-factorization.",2005-07-21T22:36:41Z,http://arxiv.org/pdf/cs/0507057v1,2024-04-28,
cs/0508037v2,"The Phase Transition in Exact Cover","We study EC3, a variant of Exact Cover which is equivalent to Positive 1-in-3 SAT. Random instances of EC3 were recently used as benchmarks for simulations of an adiabatic quantum algorithm. Empirical results suggest that EC3 has a phase transition from satisfiability to unsatisfiability when the number of clauses per variable r exceeds some threshold r* ~= 0.62 +- 0.01. Using the method of differential equations, we show that if r <= 0.546 w.h.p. a random instance of EC3 is satisfiable. Combined with previous results this limits the location of the threshold, if it exists, to the range 0.546 < r* < 0.644.",2005-08-04T15:01:48Z,http://arxiv.org/pdf/cs/0508037v2,2024-04-28,
cs/0511085v2,"Proving that P is not equal to NP and that P is not equal to the intersection of NP and co-NP","The open question, P=NP?, was presented by Cook (1971). In this paper, a proof that P is not equal to NP is presented. In addition, it is shown that P is not equal to the intersection of NP and co-NP. Finally, the exact inclusion relationships between the classes P, NP and co-NP are presented.",2005-11-25T15:23:18Z,http://arxiv.org/pdf/cs/0511085v2,2024-04-28,
cs/0512035v1,"Semidefinite programming and arithmetic circuit evaluation","A rational number can be naturally presented by an arithmetic computation (AC): a sequence of elementary arithmetic operations starting from a fixed constant, say 1. The asymptotic complexity issues of such a representation are studied e.g. in the framework of the algebraic complexity theory over arbitrary field.   Here we study a related problem of the complexity of performing arithmetic operations and computing elementary predicates, e.g. ``='' or ``>'', on rational numbers given by AC.   In the first place, we prove that AC can be efficiently simulated by the exact semidefinite programming (SDP).   Secondly, we give a BPP-algorithm for the equality predicate.   Thirdly, we put ``>''-predicate into the complexity class PSPACE.   We conjecture that ``>''-predicate is hard to compute. This conjecture, if true, would clarify the complexity status of the exact SDP - a well known open problem in the field of mathematical programming.",2005-12-09T16:01:16Z,http://arxiv.org/pdf/cs/0512035v1,2024-04-28,
cs/0601086v2,"Comments on Beckmann's Uniform Reducts","Arnold Beckmann defined the uniform reduct of a propositional proof system f to be the set of those bounded arithmetical formulas whose propositional translations have polynomial size f-proofs. We prove that the uniform reduct of f + Extended Frege consists of all true bounded arithmetical formulas iff f + Extended Frege simulates every proof system.",2006-01-19T22:56:49Z,http://arxiv.org/pdf/cs/0601086v2,2024-04-28,
cs/0602001v1,"Query-Monotonic Turing Reductions","We study reductions that limit the extreme adaptivity of Turing reductions. In particular, we study reductions that make a rapid, structured progression through the set to which they are reducing: Each query is strictly longer (shorter) than the previous one. We call these reductions query-increasing (query-decreasing) Turing reductions. We also study query-nonincreasing (query-nondecreasing) Turing reductions. These are Turing reductions in which the sequence of query lengths is nonincreasing (nondecreasing). We ask whether these restrictions in fact limit the power of reductions. We prove that query-increasing and query-decreasing Turing reductions are incomparable with (that is, are neither strictly stronger than nor strictly weaker than) truth-table reductions and are strictly weaker than Turing reductions. In addition, we prove that query-nonincreasing and query-nondecreasing Turing reductions are strictly stronger than truth-table reductions and strictly weaker than Turing reductions. Despite the fact that we prove query-increasing and query-decreasing Turing reductions to in the general case be strictly weaker than Turing reductions, we identify a broad class of sets A for which any set that Turing reduces to A will also reduce to A via both query-increasing and query-decreasing Turing reductions. In particular, this holds for all tight paddable sets, where a set is said to be tight paddable exactly if it is paddable via a function whose output length is bounded tightly both from above and from below in the length of the input. We prove that many natural NP-complete problems such as satisfiability, clique, and vertex cover are tight paddable.",2006-01-31T22:02:05Z,http://arxiv.org/pdf/cs/0602001v1,2024-04-28,
cs/0602010v1,"Reducing Tile Complexity for Self-Assembly Through Temperature Programming","We consider the tile self-assembly model and how tile complexity can be eliminated by permitting the temperature of the self-assembly system to be adjusted throughout the assembly process. To do this, we propose novel techniques for designing tile sets that permit an arbitrary length $m$ binary number to be encoded into a sequence of $O(m)$ temperature changes such that the tile set uniquely assembles a supertile that precisely encodes the corresponding binary number. As an application, we show how this provides a general tile set of size O(1) that is capable of uniquely assembling essentially any $n\times n$ square, where the assembled square is determined by a temperature sequence of length $O(\log n)$ that encodes a binary description of $n$. This yields an important decrease in tile complexity from the required $\Omega(\frac{\log n}{\log\log n})$ for almost all $n$ when the temperature of the system is fixed. We further show that for almost all $n$, no tile system can simultaneously achieve both $o(\log n)$ temperature complexity and $o(\frac{\log n}{\log\log n})$ tile complexity, showing that both versions of an optimal square building scheme have been discovered. This work suggests that temperature change can constitute a natural, dynamic method for providing input to self-assembly systems that is potentially superior to the current technique of designing large tile sets with specific inputs hardwired into the tileset.",2006-02-05T01:00:32Z,http://arxiv.org/pdf/cs/0602010v1,2024-04-28,
cs/0602047v3,"Approximability of Integer Programming with Generalised Constraints","We study a family of problems, called \prob{Maximum Solution}, where the objective is to maximise a linear goal function over the feasible integer assignments to a set of variables subject to a set of constraints. When the domain is Boolean (i.e. restricted to $\{0,1\}$), the maximum solution problem is identical to the well-studied \prob{Max Ones} problem, and the approximability is completely understood for all restrictions on the underlying constraints [Khanna et al., SIAM J. Comput., 30 (2001), pp. 1863-1920]. We continue this line of research by considering domains containing more than two elements. We present two main results: a complete classification for the approximability of all maximal constraint languages over domains of cardinality at most 4, and a complete classification of the approximability of the problem when the set of allowed constraints contains all permutation constraints. Under the assumption that a conjecture due to Szczepara holds, we give a complete classification for all maximal constraint languages. These classes of languages are well-studied in universal algebra and computer science; they have, for instance, been considered in connection with machine learning and constraint satisfaction. Our results are proved by using algebraic results from clone theory and the results indicates that this approach is very powerful for classifying the approximability of certain optimisation problems.",2006-02-13T18:48:52Z,http://arxiv.org/pdf/cs/0602047v3,2024-04-28,
cs/0602075v1,"The approximability of MAX CSP with fixed-value constraints","In the maximum constraint satisfaction problem (MAX CSP), one is given a finite collection of (possibly weighted) constraints on overlapping sets of variables, and the goal is to assign values from a given finite domain to the variables so as to maximize the number (or the total weight, for the weighted case) of satisfied constraints. This problem is NP-hard in general, and, therefore, it is natural to study how restricting the allowed types of constraints affects the approximability of the problem. In this paper, we show that any MAX CSP problem with a finite set of allowed constraint types, which includes all fixed-value constraints (i.e., constraints of the form x=a), is either solvable exactly in polynomial-time or else is APX-complete, even if the number of occurrences of variables in instances are bounded. Moreover, we present a simple description of all polynomial-time solvable cases of our problem. This description relies on the well-known algebraic combinatorial property of supermodularity.",2006-02-21T14:13:37Z,http://arxiv.org/pdf/cs/0602075v1,2024-04-28,
cs/0603017v2,"A Measure of Space for Computing over the Reals","We propose a new complexity measure of space for the BSS model of computation. We define LOGSPACE\_W and PSPACE\_W complexity classes over the reals. We prove that LOGSPACE\_W is included in NC^2\_R and in P\_W, i.e. is small enough for being relevant. We prove that the Real Circuit Decision Problem is P\_R-complete under LOGSPACE\_W reductions, i.e. that LOGSPACE\_W is large enough for containing natural algorithms. We also prove that PSPACE\_W is included in PAR\_R.",2006-03-03T13:27:45Z,http://arxiv.org/pdf/cs/0603017v2,2024-04-28,
cs/0603060v1,"An Improved Exact Algorithm for the Domatic Number Problem","The 3-domatic number problem asks whether a given graph can be partitioned intothree dominating sets. We prove that this problem can be solved by a deterministic algorithm in time 2.695^n (up to polynomial factors). This result improves the previous bound of 2.8805^n, which is due to Fomin, Grandoni, Pyatkin, and Stepanov. To prove our result, we combine an algorithm by Fomin et al. with Yamamoto's algorithm for the satisfiability problem. In addition, we show that the 3-domatic number problem can be solved for graphs G with bounded maximum degree Delta(G) by a randomized algorithm, whose running time is better than the previous bound due to Riege and Rothe whenever Delta(G) >= 5. Our new randomized algorithm employs Schoening's approach to constraint satisfaction problems.",2006-03-16T16:05:40Z,http://arxiv.org/pdf/cs/0603060v1,2024-04-28,
cs/0604003v1,"Hypercomputing the Mandelbrot Set?","The Mandelbrot set is an extremely well-known mathematical object that can be described in a quite simple way but has very interesting and non-trivial properties. This paper surveys some results that are known concerning the (non-)computability of the set. It considers two models of decidability over the reals (which have been treated much more thoroughly and technically by Hertling (2005), Blum, Shub and Smale, Brattka (2003) and Weihrauch (1999 and 2003) among others), two over the computable reals (the Russian school and hypercomputation) and a model over the rationals.",2006-04-02T15:31:32Z,http://arxiv.org/pdf/cs/0604003v1,2024-04-28,
cs/0604014v2,"Towards Analog Reverse Time Computation","We report the consequences of a destabilization process on a simulated General Purpose Analog Computer. This new technology overcomes problems linked with serial ambiguity, and provides an analog bias to encode algorithms whose complexity is over polynomial. We also implicitly demonstrate how countermesures of the Stochastic Aperture Degeneracy could efficiently reach higher computational classes, and would open a road towards Analog Reverse Time Computation.",2006-04-05T23:40:56Z,http://arxiv.org/pdf/cs/0604014v2,2024-04-28,
cs/0604047v1,"Efficient algorithms for deciding the type of growth of products of integer matrices","For a given finite set $\Sigma$ of matrices with nonnegative integer entries we study the growth of $$ \max_t(\Sigma) = \max\{\|A_{1}... A_{t}\|: A_i \in \Sigma\}.$$ We show how to determine in polynomial time whether the growth with $t$ is bounded, polynomial, or exponential, and we characterize precisely all possible behaviors.",2006-04-11T14:10:37Z,http://arxiv.org/pdf/cs/0604047v1,2024-04-28,
cs/0604113v1,"One-in-Two-Matching Problem is NP-complete","2-dimensional Matching Problem, which requires to find a matching of left- to right-vertices in a balanced $2n$-vertex bipartite graph, is a well-known polynomial problem, while various variants, like the 3-dimensional analogoue (3DM, with triangles on a tripartite graph), or the Hamiltonian Circuit Problem (HC, a restriction to ``unicyclic'' matchings) are among the main examples of NP-hard problems, since the first Karp reduction series of 1972. The same holds for the weighted variants of these problems, the Linear Assignment Problem being polynomial, and the Numerical 3-Dimensional Matching and Travelling Salesman Problem being NP-complete.   In this paper we show that a small modification of the 2-dimensional Matching and Assignment Problems in which for each $i \leq n/2$ it is required that either $\pi(2i-1)=2i-1$ or $\pi(2i)=2i$, is a NP-complete problem. The proof is by linear reduction from SAT (or NAE-SAT), with the size $n$ of the Matching Problem being four times the number of edges in the factor graph representation of the boolean problem. As a corollary, in combination with the simple linear reduction of One-in-Two Matching to 3-Dimensional Matching, we show that SAT can be linearly reduced to 3DM, while the original Karp reduction was only cubic.",2006-04-28T16:40:09Z,http://arxiv.org/pdf/cs/0604113v1,2024-04-28,
cs/0606009v4,"The Consequences of Eliminating NP Solutions","Given a function based on the computation of an NP machine, can one in general eliminate some solutions? That is, can one in general decrease the ambiguity? This simple question remains, even after extensive study by many researchers over many years, mostly unanswered. However, complexity-theoretic consequences and enabling conditions are known. In this tutorial-style article we look at some of those, focusing on the most natural framings: reducing the number of solutions of NP functions, refining the solutions of NP functions, and subtracting from or otherwise shrinking #P functions. We will see how small advice strings are important here, but we also will see how increasing advice size to achieve robustness is central to the proof of a key ambiguity-reduction result for NP functions.",2006-06-02T01:34:26Z,http://arxiv.org/pdf/cs/0606009v4,2024-04-28,
cs/0606033v2,"Natural Halting Probabilities, Partial Randomness, and Zeta Functions","We introduce the zeta number, natural halting probability and natural complexity of a Turing machine and we relate them to Chaitin's Omega number, halting probability, and program-size complexity. A classification of Turing machines according to their zeta numbers is proposed: divergent, convergent and tuatara. We prove the existence of universal convergent and tuatara machines. Various results on (algorithmic) randomness and partial randomness are proved. For example, we show that the zeta number of a universal tuatara machine is c.e. and random. A new type of partial randomness, asymptotic randomness, is introduced. Finally we show that in contrast to classical (algorithmic) randomness--which cannot be naturally characterised in terms of plain complexity--asymptotic randomness admits such a characterisation.",2006-06-07T20:18:13Z,http://arxiv.org/pdf/cs/0606033v2,2024-04-28,
cs/0606037v3,"Average-Case Complexity","We survey the average-case complexity of problems in NP.   We discuss various notions of good-on-average algorithms, and present completeness results due to Impagliazzo and Levin. Such completeness results establish the fact that if a certain specific (but somewhat artificial) NP problem is easy-on-average with respect to the uniform distribution, then all problems in NP are easy-on-average with respect to all samplable distributions. Applying the theory to natural distributional problems remain an outstanding open question. We review some natural distributional problems whose average-case complexity is of particular interest and that do not yet fit into this theory.   A major open question whether the existence of hard-on-average problems in NP can be based on the P$\neq$NP assumption or on related worst-case assumptions. We review negative results showing that certain proof techniques cannot prove such a result. While the relation between worst-case and average-case complexity for general NP problems remains open, there has been progress in understanding the relation between different ""degrees"" of average-case complexity. We discuss some of these ""hardness amplification"" results.",2006-06-08T18:40:21Z,http://arxiv.org/pdf/cs/0606037v3,2024-04-28,
cs/0606057v1,"Approximability of Bounded Occurrence Max Ones","We study the approximability of Max Ones when the number of variable occurrences is bounded by a constant. For conservative constraint languages (i.e., when the unary relations are included) we give a complete classification when the number of occurrences is three or more and a partial classification when the bound is two.   For the non-conservative case we prove that it is either trivial or equivalent to the corresponding conservative problem under polynomial-time many-one reductions.",2006-06-13T06:44:21Z,http://arxiv.org/pdf/cs/0606057v1,2024-04-28,
cs/0606064v1,"Improved Exponential Time Lower Bound of Knapsack Problem under BT model","M.Alekhnovich et al. recently have proposed a model of algorithms, called BT model, which covers Greedy, Backtrack and Simple Dynamic Programming methods and can be further divided into fixed, adaptive and fully adaptive three kinds, and have proved exponential time lower bounds of exact and approximation algorithms under adaptive BT model for Knapsack problem which are $\Omega(2^{n/2}/\sqrt n)=\Omega(2^{0.5n}/\sqrt n)$ and $\Omega((1/\epsilon)^{1/3.17})\approx\Omega((1/\epsilon)^{0.315})$(for approximation ratio $1-\epsilon$) respectively (M. Alekhovich, A. Borodin, J. Buresh-Oppenheim, R. Impagliazzo, A. Magen, and T. Pitassi, Toward a Model for Backtracking and Dynamic Programming, \emph{Proceedings of Twentieth Annual IEEE Conference on Computational Complexity}, pp308-322, 2005). In this note, we slightly improved their lower bounds to $\Omega(2^{(2-\epsilon)n/3}/\sqrt{n})\approx \Omega(2^{0.66n}/\sqrt{n})$ and $\Omega((1/\epsilon)^{1/2.38})\approx\Omega((1/\epsilon)^{0.420})$, and proposed as an open question what is the best achievable lower bounds for knapsack under adaptive BT models.",2006-06-14T07:54:14Z,http://arxiv.org/pdf/cs/0606064v1,2024-04-28,
cs/0606080v1,"On the structure of linear-time reducibility","In 1975, Ladner showed that under the hypothesis that P is not equal to NP, there exists a language which is neither in P, nor NP-complete. This result was latter generalized by Schoning and several authors to various polynomial-time complexity classes. We show here that such results also apply to linear-time reductions on RAMs (resp. Turing machines), and hence allow for separation results in linear-time classes similar to Ladner's ones for polynomial time.",2006-06-19T08:48:58Z,http://arxiv.org/pdf/cs/0606080v1,2024-04-28,
cs/0607054v1,"Elementary Proof of a Theorem of Jean Ville","Considerable thought has been devoted to an adequate definition of the class of infinite, random binary sequences (the sort of sequence that almost certainly arises from flipping a fair coin indefinitely). The first mathematical exploration of this problem was due to R. Von Mises, and based on his concept of a ""selection function."" A decisive objection to Von Mises' idea was formulated in a theorem offered by Jean Ville in 1939. It shows that some sequences admitted by Von Mises as ""random"" in fact manifest a certain kind of systematicity. Ville's proof is challenging, and an alternative approach has appeared only in condensed form. We attempt to provide an expanded version of the latter, alternative argument.",2006-07-11T19:52:56Z,http://arxiv.org/pdf/cs/0607054v1,2024-04-28,
cs/0607093v21,"An Elegant Argument that P is not NP","In this note, we present an elegant argument that P is not NP by demonstrating that the Meet-in-the-Middle algorithm must have the fastest running-time of all deterministic and exact algorithms which solve the SUBSET-SUM problem on a classical computer.",2006-07-19T19:01:09Z,http://arxiv.org/pdf/cs/0607093v21,2024-04-28,
cs/0607118v2,"A new function algebra of EXPTIME functions by safe nested recursion","Bellantoni and Cook have given a function-algebra characterization of the polynomial-time computable functions via an unbounded recursion scheme which is called safe recursion. Inspired by their work, we characterize the exponential-time computable functions with the use of a safe variant of nested recursion.",2006-07-27T06:34:53Z,http://arxiv.org/pdf/cs/0607118v2,2024-04-28,
cs/0608020v1,"Quasi-friendly sup-interpretations","In a previous paper, the sup-interpretation method was proposed as a new tool to control memory resources of first order functional programs with pattern matching by static analysis. Basically, a sup-interpretation provides an upper bound on the size of function outputs. In this former work, a criterion, which can be applied to terminating as well as non-terminating programs, was developed in order to bound polynomially the stack frame size. In this paper, we suggest a new criterion which captures more algorithms computing values polynomially bounded in the size of the inputs. Since this work is related to quasi-interpretations, we compare the two notions obtaining two main features. The first one is that, given a program, we have heuristics for finding a sup-interpretation when we consider polynomials of bounded degree. The other one consists in the characterizations of the set of function computable in polynomial time and in polynomial space.",2006-08-03T13:05:32Z,http://arxiv.org/pdf/cs/0608020v1,2024-04-28,
cs/0608036v1,"Reversal Complexity Revisited","We study a generalized version of reversal bounded Turing machines where, apart from several tapes on which the number of head reversals is bounded by r(n), there are several further tapes on which head reversals remain unrestricted, but size is bounded by s(n). Recently, such machines were introduced as a formalization of a computation model that restricts random access to external memory and internal memory space. Here, each of the tapes with a restriction on the head reversals corresponds to an external memory device, and the tapes of restricted size model internal memory. We use ST(r(n),s(n),O(1)) to denote the class of all problems that can be solved by deterministic Turing machines that comply to the above resource bounds. Similarly, NST and RST, respectively, are used for the corresponding nondeterministic and randomized classes.   While previous papers focused on lower bounds for particular problems, including sorting, the set equality problem, and several query evaluation problems, the present paper addresses the relations between the (R,N)ST-classes and classical complexity classes and investigates the structural complexity of the (R,N)ST-classes. Our main results are (1) a trade-off between internal memory space and external memory head reversals, (2) correspondences between the (R,N)ST-classes and ``classical'' time-bounded, space-bounded, reversal-bounded, and circuit complexity classes, and (3) hierarchies of (R)ST-classes in terms of increasing numbers of head reversals on external memory tapes.",2006-08-07T11:37:11Z,http://arxiv.org/pdf/cs/0608036v1,2024-04-28,
cs/0608067v1,"On Polynomial Time Computable Numbers","It will be shown that the polynomial time computable numbers form a field, and especially an algebraically closed field.",2006-08-16T15:26:08Z,http://arxiv.org/pdf/cs/0608067v1,2024-04-28,
cs/0608074v4,"From Invariants to Canonization in Parallel","A function $f$ of a graph is called a complete graph invariant if the isomorphism of graphs $G$ and $H$ is equivalent to the equality $f(G)=f(H)$. If, in addition, $f(G)$ is a graph isomorphic to $G$, then $f$ is called a canonical form for graphs. Gurevich proves that graphs have a polynomial-time computable canonical form exactly when they have a polynomial-time computable complete invariant. We extend this equivalence to the polylogarithmic-time model of parallel computation for classes of graphs with bounded rigidity index and for classes of graphs with small separators. In particular, our results apply to three representative classes of graphs embeddable into a fixed surface, namely, to 5-connected graphs, to 3-connected graphs admitting a polyhedral embedding, and 3-connected graphs admitting a large-edge-width embedding. Another application covers graphs with bounded treewidth. Since in the latter case an NC complete-invariant algorithm is known, we conclude that graphs of bounded treewidth have a canonical form (and even a canonical labeling) computable in NC.",2006-08-18T09:46:09Z,http://arxiv.org/pdf/cs/0608074v4,2024-04-28,
cs/0608106v3,"Lp Computable Functions and Fourier Series","This paper studies how well computable functions can be approximated by their Fourier series. To this end, we equip the space of Lp-computable functions (computable Lebesgue integrable functions) with a size notion, by introducing Lp-computable Baire categories.   We show that Lp-computable Baire categories satisfy the following three basic properties. Singleton sets {f} (where f is Lp-computable) are meager, suitable infinite unions of meager sets are meager, and the whole space of Lp-computable functions is not meager. We give an alternative characterization of meager sets via Banach Mazur games.   We study the convergence of Fourier series for Lp-computable functions and show that whereas for every p>1, the Fourier series of every Lp-computable function f converges to f in the Lp norm, the set of L1-computable functions whose Fourier series does not diverge almost everywhere is meager.",2006-08-28T12:50:36Z,http://arxiv.org/pdf/cs/0608106v3,2024-04-28,
cs/0609012v1,"Baire Categories on Small Complexity Classes and Meager-Comeager Laws","We introduce two resource-bounded Baire category notions on small complexity classes such as P, SUBEXP, and PSPACE and on probabilistic classes such as BPP, which differ on how the corresponding finite extension strategies are computed. We give an alternative characterization of small sets via resource-bounded Banach-Mazur games.   As an application of the first notion, we show that for almost every language A (i.e. all except a meager class) computable in subexponential time, P(A)=BPP(A). We also show that almost all languages in PSPACE do not have small nonuniform complexity.   We then switch to the second Baire category notion (called locally-computable), and show that the class SPARSE is meager in P. We show that in contrast to the resource-bounded measure case, meager-comeager laws can be obtained for many standard complexity classes, relative to locally-computable Baire category on BPP and PSPACE.   Another topic where locally-computable Baire categories differ from resource-bounded measure is regarding weak-completeness: we show that there is no weak-completeness notion in P based on locally-computable Baire categories, i.e. every P-weakly-complete set is complete for P. We also prove that the class of complete sets for P under Turing-logspace reductions is meager in P, if P is not equal to DSPACE(log n), and that the same holds unconditionally for quasi-poly time.   Finally we observe that locally-computable Baire categories are incomparable with all existing resource-bounded measure notions on small complexity classes, which might explain why those two settings seem to differ so fundamentally.",2006-09-05T09:02:11Z,http://arxiv.org/pdf/cs/0609012v1,2024-04-28,
cs/0609072v2,"The Connectivity of Boolean Satisfiability: Computational and Structural Dichotomies","Boolean satisfiability problems are an important benchmark for questions about complexity, algorithms, heuristics and threshold phenomena. Recent work on heuristics, and the satisfiability threshold has centered around the structure and connectivity of the solution space. Motivated by this work, we study structural and connectivity-related properties of the space of solutions of Boolean satisfiability problems and establish various dichotomies in Schaefer's framework.   On the structural side, we obtain dichotomies for the kinds of subgraphs of the hypercube that can be induced by the solutions of Boolean formulas, as well as for the diameter of the connected components of the solution space. On the computational side, we establish dichotomy theorems for the complexity of the connectivity and st-connectivity questions for the graph of solutions of Boolean formulas. Our results assert that the intractable side of the computational dichotomies is PSPACE-complete, while the tractable side - which includes but is not limited to all problems with polynomial time algorithms for satisfiability - is in P for the st-connectivity question, and in coNP for the connectivity question. The diameter of components can be exponential for the PSPACE-complete cases, whereas in all other cases it is linear; thus, small diameter and tractability of the connectivity problems are remarkably aligned. The crux of our results is an expressibility theorem showing that in the tractable cases, the subgraphs induced by the solution space possess certain good structural properties, whereas in the intractable cases, the subgraphs can be arbitrary.",2006-09-13T07:25:59Z,http://arxiv.org/pdf/cs/0609072v2,2024-04-28,
cs/0610009v2,"VPSPACE and a Transfer Theorem over the Reals","We introduce a new class VPSPACE of families of polynomials. Roughly speaking, a family of polynomials is in VPSPACE if its coefficients can be computed in polynomial space. Our main theorem is that if (uniform, constant-free) VPSPACE families can be evaluated efficiently then the class PAR of decision problems that can be solved in parallel polynomial time over the real numbers collapses to P. As a result, one must first be able to show that there are VPSPACE families which are hard to evaluate in order to separate over the reals P from NP, or even from PAR.",2006-10-03T13:48:44Z,http://arxiv.org/pdf/cs/0610009v2,2024-04-28,
cs/0611082v6,"The Computational Complexity of the Traveling Salesman Problem","In this note, we show that the Traveling Salesman Problem cannot be solved in polynomial-time on a classical computer.",2006-11-17T18:03:58Z,http://arxiv.org/pdf/cs/0611082v6,2024-04-28,
cs/0611147v9,"P is not equal to NP","This submission has been withdrawn at the request of the author.",2006-11-29T08:27:00Z,http://arxiv.org/pdf/cs/0611147v9,2024-04-28,
cs/0701004v4,"An algebraic approach to complexity of data stream computations","We consider a basic problem in the general data streaming model, namely, to estimate a vector $f \in \Z^n$ that is arbitrarily updated (i.e., incremented or decremented) coordinate-wise. The estimate $\hat{f} \in \Z^n$ must satisfy $\norm{\hat{f}-f}_{\infty}\le \epsilon\norm{f}_1 $, that is, $\forall i ~(\abs{\hat{f}_i - f_i} \le \epsilon \norm{f}_1)$. It is known to have $\tilde{O}(\epsilon^{-1})$ randomized space upper bound \cite{cm:jalgo}, $\Omega(\epsilon^{-1} \log (\epsilon n))$ space lower bound \cite{bkmt:sirocco03} and deterministic space upper bound of $\tilde{\Omega}(\epsilon^{-2})$ bits.\footnote{The $\tilde{O}$ and $\tilde{\Omega}$ notations suppress poly-logarithmic factors in $n, \log \epsilon^{-1}, \norm{f}_{\infty}$ and $\log \delta^{-1}$, where, $\delta$ is the error probability (for randomized algorithm).} We show that any deterministic algorithm for this problem requires space $\Omega(\epsilon^{-2} (\log \norm{f}_1))$ bits.",2007-01-02T18:12:52Z,http://arxiv.org/pdf/cs/0701004v4,2024-04-28,
cs/0701008v1,"On the Computational Complexity of Defining Sets","Suppose we have a family ${\cal F}$ of sets. For every $S \in {\cal F}$, a set $D \subseteq S$ is a {\sf defining set} for $({\cal F},S)$ if $S$ is the only element of $\cal{F}$ that contains $D$ as a subset. This concept has been studied in numerous cases, such as vertex colorings, perfect matchings, dominating sets, block designs, geodetics, orientations, and Latin squares.   In this paper, first, we propose the concept of a defining set of a logical formula, and we prove that the computational complexity of such a problem is $\Sigma_2$-complete.   We also show that the computational complexity of the following problem about the defining set of vertex colorings of graphs is $\Sigma_2$-complete:   {\sc Instance:} A graph $G$ with a vertex coloring $c$ and an integer $k$.   {\sc Question:} If ${\cal C}(G)$ be the set of all $\chi(G)$-colorings of $G$, then does $({\cal C}(G),c)$ have a defining set of size at most $k$?   Moreover, we study the computational complexity of some other variants of this problem.",2006-12-31T05:47:37Z,http://arxiv.org/pdf/cs/0701008v1,2024-04-28,
cs/0701014v2,"A Reply to Hofman On: ""Why LP cannot solve large instances of NP-complete problems in polynomial time""","Using an approach that seems to be patterned after that of Yannakakis, Hofman argues that an NP-complete problem cannot be formulated as a polynomial bounded-sized linear programming problem. He then goes on to propose a ""construct"" that he claims to be a counter-example to recently published linear programming formulations of the Traveling Salesman Problem (TSP) and the Quadratic Assignment Problems (QAP), respectively. In this paper, we show that Hofman's construct is flawed, and provide further proof that his ""counter-example"" is invalid.",2007-01-03T11:26:26Z,http://arxiv.org/pdf/cs/0701014v2,2024-04-28,
cs/0701033v1,"A Counterexample to a Proposed Proof of P=NP by S. Gubin","In a recent paper by S. Gubin [cs/0701023v1], a polynomial-time solution to the 3SAT problem was presented as proof that P=NP. The proposed algorithm cannot be made to work, which I shall demonstrate.",2007-01-05T19:16:46Z,http://arxiv.org/pdf/cs/0701033v1,2024-04-28,
cs/0701049v2,"On the Complexity of a Derivative Chess Problem","We introduce QUEENS, a derivative chess problem based on the classical n-queens problem. We prove that QUEENS is NP-complete, with respect to polynomial-time reductions.",2007-01-08T16:45:55Z,http://arxiv.org/pdf/cs/0701049v2,2024-04-28,
cs/0701128v1,"Interference Automata","We propose a computing model, the Two-Way Optical Interference Automata (2OIA), that makes use of the phenomenon of optical interference. We introduce this model to investigate the increase in power, in terms of language recognition, of a classical Deterministic Finite Automaton (DFA) when endowed with the facility of optical interference. The question is in the spirit of Two-Way Finite Automata With Quantum and Classical States (2QCFA) [A. Ambainis and J. Watrous, Two-way Finite Automata With Quantum and Classical States, Theoretical Computer Science, 287 (1), 299-311, (2002)] wherein the classical DFA is augmented with a quantum component of constant size. We test the power of 2OIA against the languages mentioned in the above paper. We give efficient 2OIA algorithms to recognize languages for which 2QCFA machines have been shown to exist, as well as languages whose status vis-a-vis 2QCFA has been posed as open questions. Finally we show the existence of a language that cannot be recognized by a 2OIA but can be recognized by an $O(n^3)$ space Turing machine.",2007-01-22T09:14:11Z,http://arxiv.org/pdf/cs/0701128v1,2024-04-28,
cs/0702047v1,"Hierarchical Unambiguity","We develop techniques to investigate relativized hierarchical unambiguous computation. We apply our techniques to generalize known constructs involving relativized unambiguity based complexity classes (UP and \mathcal{UP}) to new constructs involving arbitrary higher levels of the relativized unambiguous polynomial hierarchy (UPH). Our techniques are developed on constraints imposed by hierarchical arrangement of unambiguous nondeterministic polynomial-time Turing machines, and so they differ substantially, in applicability and in nature, from standard methods (such as the switching lemma [Hastad, Computational Limitations of Small-Depth Circuits, MIT Press, 1987]), which play roles in carrying out similar generalizations.   Aside from achieving these generalizations, we resolve a question posed by Cai, Hemachandra, and Vyskoc [J. Cai, L. Hemachandra, and J. Vyskoc, Promises and fault-tolerant database access, In K. Ambos-Spies, S. Homer, and U. Schoening, editors, Complexity Theory, pages 101-146. Cambridge University Press, 1993] on an issue related to nonadaptive Turing access to UP and adaptive smart Turing access to \mathcal{UP}.",2007-02-08T16:00:25Z,http://arxiv.org/pdf/cs/0702047v1,2024-04-28,
cs/0702053v1,"The DFAs of Finitely Different Languages","Two languages are ""finitely different"" if their symmetric difference is finite. We consider the DFAs of finitely different regular languages and find major structural similarities. We proceed to consider the smallest DFAs that recognize a language finitely different from some given DFA. Such ""f-minimal"" DFAs are not unique, and this non-uniqueness is characterized. Finally, we offer a solution to the minimization problem of finding such f-minimal DFAs.",2007-02-09T03:42:51Z,http://arxiv.org/pdf/cs/0702053v1,2024-04-28,
cs/0702058v1,"Exploring k-Colorability","An introductory paper to the graph k-colorability problem.",2007-02-09T23:07:30Z,http://arxiv.org/pdf/cs/0702058v1,2024-04-28,
cs/0702133v1,"Fast Exact Method for Solving the Travelling Salesman Problem","This paper describes TSP exact solution of polynomial complexity. It is considered properties of proposed method. Effectiveness of proposed solution is illustrated by outcomes of computer modeling.",2007-02-23T02:39:05Z,http://arxiv.org/pdf/cs/0702133v1,2024-04-28,
cs/0702160v1,"A Quantifier-Free String Theory for ALOGTIME Reasoning","The main contribution of this work is the definition of a quantifier-free string theory T_1 suitable for formalizing ALOGTIME reasoning. After describing L_1 -- a new, simple, algebraic characterization of the complexity class ALOGTIME based on strings instead of numbers -- the theory T_1 is defined (based on L_1), and a detailed formal development of T_1 is given.   Then, theorems of T_1 are shown to translate into families of propositional tautologies that have uniform polysize Frege proofs, T_1 is shown to prove the soundness of a particular Frege system F, and F is shown to provably p-simulate any proof system whose soundness can be proved in T_1. Finally, T_1 is compared with other theories for ALOGTIME reasoning in the literature.   To our knowledge, this is the first formal theory for ALOGTIME reasoning whose basic objects are strings instead of numbers, and the first quantifier-free theory formalizing ALOGTIME reasoning in which a direct proof of the soundness of some Frege system has been given (in the case of first-order theories, such a proof was first given by Arai for his theory AID). Also, the polysize Frege proofs we give for the propositional translations of theorems of T_1 are considerably simpler than those for other theories, and so is our proof of the soundness of a particular F-system in T_1. Together with the simplicity of T_1's recursion schemes, axioms, and rules these facts suggest that T_1 is one of the most natural theories available for ALOGTIME reasoning.",2007-02-28T02:59:36Z,http://arxiv.org/pdf/cs/0702160v1,2024-04-28,
cs/0703085v1,"Dimension and Relative Frequencies","We show how to calculate the finite-state dimension (equivalently, the finite-state compressibility) of a saturated sets $X$ consisting of {\em all} infinite sequences $S$ over a finite alphabet $\Sigma_m$ satisfying some given condition $P$ on the asymptotic frequencies with which various symbols from $\Sigma_m$ appear in $S$. When the condition $P$ completely specifies an empirical probability distribution $\pi$ over $\Sigma_m$, i.e., a limiting frequency of occurrence for {\em every} symbol in $\Sigma_m$, it has been known since 1949 that the Hausdorff dimension of $X$ is precisely $\CH(\pi)$, the Shannon entropy of $\pi$, and the finite-state dimension was proven to have this same value in 2001.   The saturated sets were studied by Volkmann and Cajar decades ago. It got attention again only with the recent developments in multifractal analysis by Barreira, Saussol, Schmeling, and separately Olsen. However, the powerful methods they used -- ergodic theory and multifractal analysis -- do not yield a value for the finite-state (or even computable) dimension in an obvious manner.   We give a pointwise characterization of finite-state dimensions of saturated sets. Simultaneously, we also show that their finite-state dimension and strong dimension coincide with their Hausdorff and packing dimension respectively, though the techniques we use are completely elementary. Our results automatically extend to less restrictive effective settings (e.g., constructive, computable, and polynomial-time dimensions).",2007-03-16T02:42:53Z,http://arxiv.org/pdf/cs/0703085v1,2024-04-28,
cs/0703110v4,"Geometric Complexity Theory IV: nonstandard quantum group for the Kronecker problem","The Kronecker coefficient g_{\lambda \mu \nu} is the multiplicity of the GL(V)\times GL(W)-irreducible V_\lambda \otimes W_\mu in the restriction of the GL(X)-irreducible X_\nu via the natural map GL(V)\times GL(W) \to GL(V \otimes W), where V, W are \mathbb{C}-vector spaces and X = V \otimes W. A fundamental open problem in algebraic combinatorics is to find a positive combinatorial formula for these coefficients.   We construct two quantum objects for this problem, which we call the nonstandard quantum group and nonstandard Hecke algebra. We show that the nonstandard quantum group has a compact real form and its representations are completely reducible, that the nonstandard Hecke algebra is semisimple, and that they satisfy an analog of quantum Schur-Weyl duality.   Using these nonstandard objects as a guide, we follow the approach of Adsul, Sohoni, and Subrahmanyam to construct, in the case dim(V) = dim(W) =2, a representation \check{X}_\nu of the nonstandard quantum group that specializes to Res_{GL(V) \times GL(W)} X_\nu at q=1. We then define a global crystal basis +HNSTC(\nu) of \check{X}_\nu that solves the two-row Kronecker problem: the number of highest weight elements of +HNSTC(\nu) of weight (\lambda,\mu) is the Kronecker coefficient g_{\lambda \mu \nu}. We go on to develop the beginnings of a graphical calculus for this basis, along the lines of the U_q(\sl_2) graphical calculus, and use this to organize the crystal components of +HNSTC(\nu) into eight families. This yields a fairly simple, explicit and positive formula for two-row Kronecker coefficients, generalizing a formula of Brown, van Willigenburg, and Zabrocki. As a byproduct of the approach, we also obtain a rule for the decomposition of Res_{GL_2 \times GL_2 \rtimes \S_2} X_\nu into irreducibles.",2007-03-22T15:35:29Z,http://arxiv.org/pdf/cs/0703110v4,2024-04-28,
0704.0108v1,"Reducing SAT to 2-SAT","Description of a polynomial time reduction of SAT to 2-SAT of polynomial size.",2007-04-01T23:16:27Z,http://arxiv.org/pdf/0704.0108v1,2024-04-28,
0704.0213v2,"Geometric Complexity Theory V: On deciding nonvanishing of a generalized Littlewood-Richardson coefficient","This article has been withdrawn because it has been merged with the earlier article GCT3 (arXiv: CS/0501076 [cs.CC]) in the series. The merged article is now available as:   Geometric Complexity Theory III: on deciding nonvanishing of a Littlewood-Richardson Coefficient, Journal of Algebraic Combinatorics, vol. 36, issue 1, 2012, pp. 103-110. (Authors: Ketan Mulmuley, Hari Narayanan and Milind Sohoni)   The new article in this GCT5 slot in the series is:   Geometric Complexity Theory V: Equivalence between blackbox derandomization of polynomial identity testing and derandomization of Noether's Normalization Lemma, in the Proceedings of FOCS 2012 (abstract), arXiv:1209.5993 [cs.CC] (full version) (Author: Ketan Mulmuley)",2007-04-02T15:13:27Z,http://arxiv.org/pdf/0704.0213v2,2024-04-28,
0704.0229v4,"Geometric Complexity Theory VI: the flip via saturated and positive integer programming in representation theory and algebraic geometry","This article belongs to a series on geometric complexity theory (GCT), an approach to the P vs. NP and related problems through algebraic geometry and representation theory. The basic principle behind this approach is called the flip. In essence, it reduces the negative hypothesis in complexity theory (the lower bound problems), such as the P vs. NP problem in characteristic zero, to the positive hypothesis in complexity theory (the upper bound problems): specifically, to showing that the problems of deciding nonvanishing of the fundamental structural constants in representation theory and algebraic geometry, such as the well known plethysm constants--or rather certain relaxed forms of these decision probelms--belong to the complexity class P. In this article, we suggest a plan for implementing the flip, i.e., for showing that these relaxed decision problems belong to P. This is based on the reduction of the preceding complexity-theoretic positive hypotheses to mathematical positivity hypotheses: specifically, to showing that there exist positive formulae--i.e. formulae with nonnegative coefficients--for the structural constants under consideration and certain functions associated with them. These turn out be intimately related to the similar positivity properties of the Kazhdan-Lusztig polynomials and the multiplicative structural constants of the canonical (global crystal) bases in the theory of Drinfeld-Jimbo quantum groups. The known proofs of these positivity properties depend on the Riemann hypothesis over finite fields and the related results. Thus the reduction here, in conjunction with the flip, in essence, says that the validity of the P vs. NP conjecture in characteristic zero is intimately linked to the Riemann hypothesis over finite fields and related problems.",2007-04-02T16:41:38Z,http://arxiv.org/pdf/0704.0229v4,2024-04-28,
0704.0301v1,"Differential Recursion and Differentially Algebraic Functions","Moore introduced a class of real-valued ""recursive"" functions by analogy with Kleene's formulation of the standard recursive functions. While his concise definition inspired a new line of research on analog computation, it contains some technical inaccuracies. Focusing on his ""primitive recursive"" functions, we pin down what is problematic and discuss possible attempts to remove the ambiguity regarding the behavior of the differential recursion operator on partial functions. It turns out that in any case the purported relation to differentially algebraic functions, and hence to Shannon's model of analog computation, fails.",2007-04-03T19:50:14Z,http://arxiv.org/pdf/0704.0301v1,2024-04-28,
0705.0915v2,"Satisfiability Parsimoniously Reduces to the Tantrix(TM) Rotation Puzzle Problem","Holzer and Holzer (Discrete Applied Mathematics 144(3):345--358, 2004) proved that the Tantrix(TM) rotation puzzle problem is NP-complete. They also showed that for infinite rotation puzzles, this problem becomes undecidable. We study the counting version and the unique version of this problem. We prove that the satisfiability problem parsimoniously reduces to the Tantrix(TM) rotation puzzle problem. In particular, this reduction preserves the uniqueness of the solution, which implies that the unique Tantrix(TM) rotation puzzle problem is as hard as the unique satisfiability problem, and so is DP-complete under polynomial-time randomized reductions, where DP is the second level of the boolean hierarchy over NP.",2007-05-07T14:23:20Z,http://arxiv.org/pdf/0705.0915v2,2024-04-28,
0705.1442v2,"Does P=NP?","This paper has been withdrawn Abstract: This paper has been withdrawn by the author due to the publication.",2007-05-10T11:41:26Z,http://arxiv.org/pdf/0705.1442v2,2024-04-28,
0706.1477v1,"VPSPACE and a transfer theorem over the complex field","We extend the transfer theorem of [KP2007] to the complex field. That is, we investigate the links between the class VPSPACE of families of polynomials and the Blum-Shub-Smale model of computation over C. Roughly speaking, a family of polynomials is in VPSPACE if its coefficients can be computed in polynomial space. Our main result is that if (uniform, constant-free) VPSPACE families can be evaluated efficiently then the class PAR of decision problems that can be solved in parallel polynomial time over the complex field collapses to P. As a result, one must first be able to show that there are VPSPACE families which are hard to evaluate in order to separate P from NP over C, or even from PAR.",2007-06-11T13:59:31Z,http://arxiv.org/pdf/0706.1477v1,2024-04-28,
0706.2035v1,"Critique of Feinstein's Proof that P is not Equal to NP","We examine a proof by Craig Alan Feinstein that P is not equal to NP. We present counterexamples to claims made in his paper and expose a flaw in the methodology he uses to make his assertions. The fault in his argument is the incorrect use of reduction. Feinstein makes incorrect assumptions about the complexity of a problem based on the fact that there is a more complex problem that can be used to solve it. His paper introduces the terminology ""imaginary processor"" to describe how it is possible to beat the brute force reduction he offers to solve the Subset-Sum problem. The claims made in the paper would not be validly established even were imaginary processors to exist.",2007-06-14T13:15:39Z,http://arxiv.org/pdf/0706.2035v1,2024-04-28,
0706.3412v1,"On Canonical Forms of Complete Problems via First-order Projections","The class of problems complete for NP via first-order reductions is known to be characterized by existential second-order sentences of a fixed form. All such sentences are built around the so-called generalized IS-form of the sentence that defines Independent-Set. This result can also be understood as that every sentence that defines a NP-complete problem P can be decomposed in two disjuncts such that the first one characterizes a fragment of P as hard as Independent-Set and the second the rest of P. That is, a decomposition that divides every such sentence into a quotient and residue modulo Independent-Set.   In this paper, we show that this result can be generalized over a wide collection of complexity classes, including the so-called nice classes. Moreover, we show that such decomposition can be done for any complete problem with respect to the given class, and that two such decompositions are non-equivalent in general. Interestingly, our results are based on simple and well-known properties of first-order reductions.ow that this result can be generalized over a wide collection of complexity classes, including the so-called nice classes. Moreover, we show that such decomposition can be done for any complete problem with respect to the given class, and that two such decompositions are non-equivalent in general. Interestingly, our results are based on simple and well-known properties of first-order reductions.",2007-06-22T21:27:06Z,http://arxiv.org/pdf/0706.3412v1,2024-04-28,
0707.0430v1,"Assisted Problem Solving and Decompositions of Finite Automata","A study of assisted problem solving formalized via decompositions of deterministic finite automata is initiated. The landscape of new types of decompositions of finite automata this study uncovered is presented. Languages with various degrees of decomposability between undecomposable and perfectly decomposable are shown to exist.",2007-07-03T14:54:53Z,http://arxiv.org/pdf/0707.0430v1,2024-04-28,
0707.1364v1,"Report on Generic Case Complexity","This article is a short introduction to generic case complexity, which is a recently developed way of measuring the difficulty of a computational problem while ignoring atypical behavior on a small set of inputs. Generic case complexity applies to both recursively solvable and recursively unsolvable problems.",2007-07-10T18:57:08Z,http://arxiv.org/pdf/0707.1364v1,2024-04-28,
0707.4489v1,"Small weakly universal Turing machines","We give small universal Turing machines with state-symbol pairs of (6, 2), (3, 3) and (2, 4). These machines are weakly universal, which means that they have an infinitely repeated word to the left of their input and another to the right. They simulate Rule 110 and are currently the smallest known weakly universal Turing machines.",2007-07-30T21:40:55Z,http://arxiv.org/pdf/0707.4489v1,2024-04-28,
0708.2105v1,"Attribute Estimation and Testing Quasi-Symmetry","A Boolean function is symmetric if it is invariant under all permutations of its arguments; it is quasi-symmetric if it is symmetric with respect to the arguments on which it actually depends. We present a test that accepts every quasi-symmetric function and, except with an error probability at most delta>0, rejects every function that differs from every quasi-symmetric function on at least a fraction epsilon>0 of the inputs. For a function of n arguments, the test probes the function at O((n/epsilon)\log(n/delta)) inputs. Our quasi-symmetry test acquires information concerning the arguments on which the function actually depends. To do this, it employs a generalization of the property testing paradigm that we call attribute estimation. Like property testing, attribute estimation uses random sampling to obtain results that have only ""one-sided'' errors and that are close to accurate with high probability.",2007-08-15T20:56:22Z,http://arxiv.org/pdf/0708.2105v1,2024-04-28,
0708.3568v1,"A Polynomial-time Algorithm for Computing the Permanent in GF(3^q)","A polynomial-time algorithm for computing the permanent in any field of characteristic 3 is presented in this article. The principal objects utilized for that purpose are the Cauchy and Vandermonde matrices, the discriminant function and their generalizations of various types. Classical theorems on the permanent such as the Binet-Minc identity and Borchadt's formula are widely applied, while a special new technique involving the notion of limit re-defined for fields of finite characteristics and corresponding computational methods was developed in order to deal with a number of polynomial-time reductions. All the constructions preserve a strictly algebraic nature ignoring the structure of the basic field, while applying its infinite extensions for calculating limits.   A natural corollary of the polynomial-time computability of the permanent in a field of a characteristic different from 2 is the non-uniform equality between the complexity classes P and NP what is equivalent to RP=NP (Ref. [1]).",2007-08-27T15:47:49Z,http://arxiv.org/pdf/0708.3568v1,2024-04-28,
0708.4075v1,"Graph Isomorphism is PSPACE-complete","Combining the the results of A.R. Meyer and L.J. Stockmeyer ""The Equivalence Problem for Regular Expressions with Squaring Requires Exponential Space"", and K.S. Booth ""Isomorphism testing for graphs, semigroups, and finite automata are polynomiamlly equivalent problems"" shows that graph isomorphism is PSPACE-complete.",2007-08-30T05:06:39Z,http://arxiv.org/pdf/0708.4075v1,2024-04-28,
0709.0367v2,"Relationship between clustering and algorithmic phase transitions in the random k-XORSAT model and its NP-complete extensions","We study the performances of stochastic heuristic search algorithms on Uniquely Extendible Constraint Satisfaction Problems with random inputs. We show that, for any heuristic preserving the Poissonian nature of the underlying instance, the (heuristic-dependent) largest ratio $\alpha_a$ of constraints per variables for which a search algorithm is likely to find solutions is smaller than the critical ratio $\alpha_d$ above which solutions are clustered and highly correlated. In addition we show that the clustering ratio can be reached when the number k of variables per constraints goes to infinity by the so-called Generalized Unit Clause heuristic.",2007-09-04T08:56:27Z,http://arxiv.org/pdf/0709.0367v2,2024-04-28,
0709.0746v1,"Geometric Complexity Theory: Introduction","These are lectures notes for the introductory graduate courses on geometric complexity theory (GCT) in the computer science department, the university of Chicago. Part I consists of the lecture notes for the course given by the first author in the spring quarter, 2007. It gives introduction to the basic structure of GCT. Part II consists of the lecture notes for the course given by the second author in the spring quarter, 2003. It gives introduction to invariant theory with a view towards GCT. No background in algebraic geometry or representation theory is assumed. These lecture notes in conjunction with the article \cite{GCTflip1}, which describes in detail the basic plan of GCT based on the principle called the flip, should provide a high level picture of GCT assuming familiarity with only basic notions of algebra, such as groups, rings, fields etc.",2007-09-05T21:54:52Z,http://arxiv.org/pdf/0709.0746v1,2024-04-28,
0709.0748v1,"On P vs. NP, Geometric Complexity Theory, and the Flip I: a high level view","Geometric complexity theory (GCT) is an approach to the $P$ vs. $NP$ and related problems through algebraic geometry and representation theory. This article gives a high-level exposition of the basic plan of GCT based on the principle, called the flip, without assuming any background in algebraic geometry or representation theory.",2007-09-05T22:10:31Z,http://arxiv.org/pdf/0709.0748v1,2024-04-28,
0709.0749v2,"Geometric Complexity Theory VII: Nonstandard quantum group for the plethysm problem","This article describes a {\em nonstandard} quantum group that may be used to derive a positive formula for the plethysm problem, just as the standard (Drinfeld-Jimbo) quantum group can be used to derive the positive Littlewood-Richardson rule for arbitrary complex semisimple Lie groups. The sequel \cite{GCT8} gives conjecturally correct algorithms to construct canonical bases of the coordinate rings of these nonstandard quantum groups and canonical bases of the dually paired nonstandard deformations of the symmetric group algebra. A positive $#P$-formula for the plethysm constant follows from the conjectural properties of these canonical bases and the duality and reciprocity conjectures herein.",2007-09-05T22:23:15Z,http://arxiv.org/pdf/0709.0749v2,2024-04-28,
0709.0751v2,"Geometric Complexity Theory VIII: On canonical bases for the nonstandard quantum groups","This article gives conjecturally correct algorithms to construct canonical bases of the irreducible polynomial representations and the matrix coordinate rings of the nonstandard quantum groups in GCT4 and GCT7, and canonical bases of the dually paired nonstandard deformations of the symmetric group algebra therein. These are generalizations of the canonical bases of the irreducible polynomial representations and the matrix coordinate ring of the standard quantum group, as constructed by Kashiwara and Lusztig, and the Kazhdan-Lusztig basis of the Hecke algebra. A positive ($#P$-) formula for the well-known plethysm constants follows from their conjectural properties and the duality and reciprocity conjectures in \cite{GCT7}.",2007-09-05T22:30:50Z,http://arxiv.org/pdf/0709.0751v2,2024-04-28,
0709.1207v3,"The P versus NP Brief","This paper discusses why P and NP are likely to be different. It analyses the essence of the concepts and points out that P and NP might be diverse by sheer definition. It also speculates that P and NP may be unequal due to natural laws.",2007-09-08T12:45:51Z,http://arxiv.org/pdf/0709.1207v3,2024-04-28,
0709.4117v1,"Deciding Unambiguity and Sequentiality starting from a Finitely Ambiguous Max-Plus Automaton","Finite automata with weights in the max-plus semiring are considered. The main result is: it is decidable in an effective way whether a series that is recognized by a finitely ambiguous max-plus automaton is unambiguous, or is sequential. A collection of examples is given to illustrate the hierarchy of max-plus series with respect to ambiguity.",2007-09-26T09:51:28Z,http://arxiv.org/pdf/0709.4117v1,2024-04-28,
0710.0360v1,"Interpolation in Valiant's theory","We investigate the following question: if a polynomial can be evaluated at rational points by a polynomial-time boolean algorithm, does it have a polynomial-size arithmetic circuit? We argue that this question is certainly difficult. Answering it negatively would indeed imply that the constant-free versions of the algebraic complexity classes VP and VNP defined by Valiant are different. Answering this question positively would imply a transfer theorem from boolean to algebraic complexity. Our proof method relies on Lagrange interpolation and on recent results connecting the (boolean) counting hierarchy to algebraic complexity classes. As a byproduct we obtain two additional results: (i) The constant-free, degree-unbounded version of Valiant's hypothesis that VP and VNP differ implies the degree-bounded version. This result was previously known to hold for fields of positive characteristic only. (ii) If exponential sums of easy to compute polynomials can be computed efficiently, then the same is true of exponential products. We point out an application of this result to the P=NP problem in the Blum-Shub-Smale model of computation over the field of complex numbers.",2007-10-01T18:58:19Z,http://arxiv.org/pdf/0710.0360v1,2024-04-28,
0710.0805v3,"On the Satisfiability Threshold and Clustering of Solutions of Random 3-SAT Formulas","We study the structure of satisfying assignments of a random 3-SAT formula. In particular, we show that a random formula of density 4.453 or higher almost surely has no non-trivial ""core"" assignments. Core assignments are certain partial assignments that can be extended to satisfying assignments, and have been studied recently in connection with the Survey Propagation heuristic for random SAT. Their existence implies the presence of clusters of solutions, and they have been shown to exist with high probability below the satisfiability threshold for k-SAT with k>8, by Achlioptas and Ricci-Tersenghi, STOC 2006. Our result implies that either this does not hold for 3-SAT or the threshold density for satisfiability in 3-SAT lies below 4.453.   The main technical tool that we use is a novel simple application of the first moment method.",2007-10-03T19:04:44Z,http://arxiv.org/pdf/0710.0805v3,2024-04-28,
0710.2732v1,"Probabilistic communication complexity over the reals","Deterministic and probabilistic communication protocols are introduced in which parties can exchange the values of polynomials (rather than bits in the usual setting). It is established a sharp lower bound $2n$ on the communication complexity of recognizing the $2n$-dimensional orthant, on the other hand the probabilistic communication complexity of its recognizing does not exceed 4. A polyhedron and a union of hyperplanes are constructed in $\RR^{2n}$ for which a lower bound $n/2$ on the probabilistic communication complexity of recognizing each is proved. As a consequence this bound holds also for the EMPTINESS and the KNAPSACK problems.",2007-10-15T08:03:38Z,http://arxiv.org/pdf/0710.2732v1,2024-04-28,
0710.3519v1,"P-matrix recognition is co-NP-complete","This is a summary of the proof by G.E. Coxson that P-matrix recognition is co-NP-complete. The result follows by a reduction from the MAX CUT problem using results of S. Poljak and J. Rohn.",2007-10-18T14:14:26Z,http://arxiv.org/pdf/0710.3519v1,2024-04-28,
0710.3961v1,"On a New Type of Information Processing for Efficient Management of Complex Systems","It is a challenge to manage complex systems efficiently without confronting NP-hard problems. To address the situation we suggest to use self-organization processes of prime integer relations for information processing. Self-organization processes of prime integer relations define correlation structures of a complex system and can be equivalently represented by transformations of two-dimensional geometrical patterns determining the dynamics of the system and revealing its structural complexity. Computational experiments raise the possibility of an optimality condition of complex systems presenting the structural complexity of a system as a key to its optimization.   From this perspective the optimization of a system could be all about the control of the structural complexity of the system to make it consistent with the structural complexity of the problem. The experiments also indicate that the performance of a complex system may behave as a concave function of the structural complexity. Therefore, once the structural complexity could be controlled as a single entity, the optimization of a complex system would be potentially reduced to a one-dimensional concave optimization irrespective of the number of variables involved its description. This might open a way to a new type of information processing for efficient management of complex systems.",2007-10-22T01:10:02Z,http://arxiv.org/pdf/0710.3961v1,2024-04-28,
0710.4272v2,"An approximation trichotomy for Boolean #CSP","We give a trichotomy theorem for the complexity of approximately counting the number of satisfying assignments of a Boolean CSP instance. Such problems are parameterised by a constraint language specifying the relations that may be used in constraints. If every relation in the constraint language is affine then the number of satisfying assignments can be exactly counted in polynomial time. Otherwise, if every relation in the constraint language is in the co-clone IM_2 from Post's lattice, then the problem of counting satisfying assignments is complete with respect to approximation-preserving reductions in the complexity class #RH\Pi_1. This means that the problem of approximately counting satisfying assignments of such a CSP instance is equivalent in complexity to several other known counting problems, including the problem of approximately counting the number of independent sets in a bipartite graph. For every other fixed constraint language, the problem is complete for #P with respect to approximation-preserving reductions, meaning that there is no fully polynomial randomised approximation scheme for counting satisfying assignments unless NP=RP.",2007-10-23T14:35:05Z,http://arxiv.org/pdf/0710.4272v2,2024-04-28,
0711.1827v3,"The Three-Color and Two-Color Tantrix(TM) Rotation Puzzle Problems are NP-Complete via Parsimonious Reductions","Holzer and Holzer (Discrete Applied Mathematics 144(3):345--358, 2004) proved that the Tantrix(TM) rotation puzzle problem with four colors is NP-complete, and they showed that the infinite variant of this problem is undecidable. In this paper, we study the three-color and two-color Tantrix(TM) rotation puzzle problems (3-TRP and 2-TRP) and their variants. Restricting the number of allowed colors to three (respectively, to two) reduces the set of available Tantrix(TM) tiles from 56 to 14 (respectively, to 8). We prove that 3-TRP and 2-TRP are NP-complete, which answers a question raised by Holzer and Holzer in the affirmative. Since our reductions are parsimonious, it follows that the problems Unique-3-TRP and Unique-2-TRP are DP-complete under randomized reductions. We also show that the another-solution problems associated with 4-TRP, 3-TRP, and 2-TRP are NP-complete. Finally, we prove that the infinite variants of 3-TRP and 2-TRP are undecidable.",2007-11-12T17:44:45Z,http://arxiv.org/pdf/0711.1827v3,2024-04-28,
0711.2010v5,"A Polynomial Time Algorithm for Graph Isomorphism","We claimed that there is a polynomial algorithm to test if two graphs are isomorphic. But the algorithm is wrong. It only tests if the adjacency matrices of two graphs have the same eigenvalues. There is a counterexample of two non-isomorphic graphs with the same eigenvalues.",2007-11-13T15:51:33Z,http://arxiv.org/pdf/0711.2010v5,2024-04-28,
0712.1532v1,"Hard constraint satisfaction problems have hard gaps at location 1","An instance of Max CSP is a finite collection of constraints on a set of variables, and the goal is to assign values to the variables that maximises the number of satisfied constraints. Max CSP captures many well-known problems (such as Max k-SAT and Max Cut) and is consequently NP-hard. Thus, it is natural to study how restrictions on the allowed constraint types (or constraint languages) affect the complexity and approximability of Max CSP. The PCP theorem is equivalent to the existence of a constraint language for which Max CSP has a hard gap at location 1, i.e. it is NP-hard to distinguish between satisfiable instances and instances where at most some constant fraction of the constraints are satisfiable. All constraint languages, for which the CSP problem (i.e., the problem of deciding whether all constraints can be satisfied) is currently known to be NP-hard, have a certain algebraic property. We prove that any constraint language with this algebraic property makes Max CSP have a hard gap at location 1 which, in particular, implies that such problems cannot have a PTAS unless P = NP. We then apply this result to Max CSP restricted to a single constraint type; this class of problems contains, for instance, Max Cut and Max DiCut. Assuming P $\neq$ NP, we show that such problems do not admit PTAS except in some trivial cases. Our results hold even if the number of occurrences of each variable is bounded by a constant. We use these results to partially answer open questions and strengthen results by Engebretsen et al. [Theor. Comput. Sci., 312 (2004), pp. 17--45], Feder et al. [Discrete Math., 307 (2007), pp. 386--392], Krokhin and Larose [Proc. Principles and Practice of Constraint Programming (2005), pp. 388--402], and Jonsson and Krokhin [J. Comput. System Sci., 73 (2007), pp. 691--702]",2007-12-10T16:42:18Z,http://arxiv.org/pdf/0712.1532v1,2024-04-28,
0712.3348v2,"On Exponential Time Lower Bound of Knapsack under Backtracking","M.Aleknovich et al. have recently proposed a model of algorithms, called BT model, which generalizes both the priority model of Borodin, Nielson and Rackoff, as well as a simple dynamic programming model by Woeginger. BT model can be further divided into three kinds of fixed, adaptive and fully adaptive ones. They have proved exponential time lower bounds of exact and approximation algorithms under adaptive BT model for Knapsack problem. Their exact lower bound is $\Omega(2^{0.5n}/\sqrt{n})$, in this paper, we slightly improve the exact lower bound to about $\Omega(2^{0.69n}/\sqrt{n})$, by the same technique, with related parameters optimized.",2007-12-20T09:15:17Z,http://arxiv.org/pdf/0712.3348v2,2024-04-28,
0712.4279v2,"Disjointness is hard in the multi-party number on the forehead model","We show that disjointness requires randomized communication Omega(n^{1/(k+1)}/2^{2^k}) in the general k-party number-on-the-forehead model of complexity. The previous best lower bound for k >= 3 was log(n)/(k-1). Our results give a separation between nondeterministic and randomized multiparty number-on-the-forehead communication complexity for up to k=log log n - O(log log log n) many players. Also by a reduction of Beame, Pitassi, and Segerlind, these results imply subexponential lower bounds on the size of proofs needed to refute certain unsatisfiable CNFs in a broad class of proof systems, including tree-like Lovasz-Schrijver proofs.",2007-12-27T20:45:53Z,http://arxiv.org/pdf/0712.4279v2,2024-04-28,
0801.0474v1,"Analysis and Counterexamples Regarding Yatsenko's Polynomial-Time Algorithm for Solving the Traveling Salesman Problem","Yatsenko gives a polynomial-time algorithm for solving the traveling salesman problem. We examine the correctness of the algorithm and its construction. We also comment on Yatsenko's evaluation of the algorithm.",2008-01-03T04:46:16Z,http://arxiv.org/pdf/0801.0474v1,2024-04-28,
0801.0514v1,"New results on Noncommutative and Commutative Polynomial Identity Testing","Using ideas from automata theory we design a new efficient (deterministic) identity test for the \emph{noncommutative} polynomial identity testing problem (first introduced and studied in \cite{RS05,BW05}). We also apply this idea to the reconstruction of black-box noncommuting algebraic branching programs. Assuming the black-box model allows us to query the ABP for the output at any given gate, we can reconstruct an (equivalent) ABP in deterministic polynomial time. Finally, we explore commutative identity testing when the coefficients of the input polynomial come from an arbitrary finite commutative ring with unity.",2008-01-03T12:32:41Z,http://arxiv.org/pdf/0801.0514v1,2024-04-28,
0801.3624v3,"Multiparty Communication Complexity of Disjointness","We obtain a lower bound of n^Omega(1) on the k-party randomized communication complexity of the Disjointness function in the `Number on the Forehead' model of multiparty communication when k is a constant. For k=o(loglog n), the bounds remain super-polylogarithmic i.e. (log n)^omega(1). The previous best lower bound for three players until recently was Omega(log n).   Our bound separates the communication complexity classes NP^{CC}_k and BPP^{CC}_k for k=o(loglog n). Furthermore, by the results of Beame, Pitassi and Segerlind \cite{BPS07}, our bound implies proof size lower bounds for tree-like, degree k-1 threshold systems and superpolynomial size lower bounds for Lovasz-Schrijver proofs.   Sherstov \cite{She07b} recently developed a novel technique to obtain lower bounds on two-party communication using the approximate polynomial degree of boolean functions. We obtain our results by extending his technique to the multi-party setting using ideas from Chattopadhyay \cite{Cha07}.   A similar bound for Disjointness has been recently and independently obtained by Lee and Shraibman.",2008-01-23T16:39:31Z,http://arxiv.org/pdf/0801.3624v3,2024-04-28,
0801.3669v4,"Merkle's Key Agreement Protocol is Optimal: An $O(n^2)$ Attack on any Key Agreement from Random Oracles","We prove that every key agreement protocol in the random oracle model in which the honest users make at most $n$ queries to the oracle can be broken by an adversary who makes $O(n^2)$ queries to the oracle. This improves on the previous $\widetilde{\Omega}(n^6)$ query attack given by Impagliazzo and Rudich (STOC '89) and resolves an open question posed by them.   Our bound is optimal up to a constant factor since Merkle proposed a key agreement protocol in 1974 that can be easily implemented with $n$ queries to a random oracle and cannot be broken by any adversary who asks $o(n^2)$ queries.",2008-01-23T21:01:37Z,http://arxiv.org/pdf/0801.3669v4,2024-04-28,
0801.4777v1,"Non-Deterministic Communication Complexity of Regular Languages","In this thesis, we study the place of regular languages within the communication complexity setting. In particular, we are interested in the non-deterministic communication complexity of regular languages.   We show that a regular language has either O(1) or Omega(log n) non-deterministic complexity. We obtain several linear lower bound results which cover a wide range of regular languages having linear non-deterministic complexity. These lower bound results also imply a result in semigroup theory: we obtain sufficient conditions for not being in the positive variety Pol(Com).   To obtain our results, we use algebraic techniques. In the study of regular languages, the algebraic point of view pioneered by Eilenberg (\cite{Eil74}) has led to many interesting results. Viewing a semigroup as a computational device that recognizes languages has proven to be prolific from both semigroup theory and formal languages perspectives. In this thesis, we provide further instances of such mutualism.",2008-01-30T21:55:13Z,http://arxiv.org/pdf/0801.4777v1,2024-04-28,
0801.4911v1,"On the Double Coset Membership Problem for Permutation Groups","We show that the Double Coset Membership problem for permutation groups possesses perfect zero-knowledge proofs.",2008-01-31T16:19:20Z,http://arxiv.org/pdf/0801.4911v1,2024-04-28,
0801.4917v1,"Zero-Knowledge Proofs of the Conjugacy for Permutation Groups","We design a perfect zero-knowledge proof system for recognition if two permutation groups are conjugate.",2008-01-31T16:33:15Z,http://arxiv.org/pdf/0801.4917v1,2024-04-28,
0802.0423v1,"Approximability Distance in the Space of H-Colourability Problems","A graph homomorphism is a vertex map which carries edges from a source graph to edges in a target graph. We study the approximability properties of the Weighted Maximum H-Colourable Subgraph problem (MAX H-COL). The instances of this problem are edge-weighted graphs G and the objective is to find a subgraph of G that has maximal total edge weight, under the condition that the subgraph has a homomorphism to H; note that for H=K_k this problem is equivalent to MAX k-CUT. To this end, we introduce a metric structure on the space of graphs which allows us to extend previously known approximability results to larger classes of graphs. Specifically, the approximation algorithms for MAX CUT by Goemans and Williamson and MAX k-CUT by Frieze and Jerrum can be used to yield non-trivial approximation results for MAX H-COL. For a variety of graphs, we show near-optimality results under the Unique Games Conjecture. We also use our method for comparing the performance of Frieze & Jerrum's algorithm with Hastad's approximation algorithm for general MAX 2-CSP. This comparison is, in most cases, favourable to Frieze & Jerrum.",2008-02-04T14:32:45Z,http://arxiv.org/pdf/0802.0423v1,2024-04-28,
0802.1465v2,"3-Way Composition of Weighted Finite-State Transducers","Composition of weighted transducers is a fundamental algorithm used in many applications, including for computing complex edit-distances between automata, or string kernels in machine learning, or to combine different components of a speech recognition, speech synthesis, or information extraction system. We present a generalization of the composition of weighted transducers, 3-way composition, which is dramatically faster in practice than the standard composition algorithm when combining more than two transducers. The worst-case complexity of our algorithm for composing three transducers $T_1$, $T_2$, and $T_3$ resulting in $T$, \ignore{depending on the strategy used, is $O(|T|_Q d(T_1) d(T_3) + |T|_E)$ or $(|T|_Q d(T_2) + |T|_E)$,} is $O(|T|_Q \min(d(T_1) d(T_3), d(T_2)) + |T|_E)$, where $|\cdot|_Q$ denotes the number of states, $|\cdot|_E$ the number of transitions, and $d(\cdot)$ the maximum out-degree. As in regular composition, the use of perfect hashing requires a pre-processing step with linear-time expected complexity in the size of the input transducers. In many cases, this approach significantly improves on the complexity of standard composition. Our algorithm also leads to a dramatically faster composition in practice. Furthermore, standard composition can be obtained as a special case of our algorithm. We report the results of several experiments demonstrating this improvement. These theoretical and empirical improvements significantly enhance performance in the applications already mentioned.",2008-02-11T16:18:40Z,http://arxiv.org/pdf/0802.1465v2,2024-04-28,
0802.1699v1,"Longest paths in Planar DAGs in Unambiguous Logspace","We show via two different algorithms that finding the length of the longest path in planar directed acyclic graph (DAG) is in unambiguous logspace UL, and also in the complement class co-UL. The result extends to toroidal DAGs as well.",2008-02-12T20:08:39Z,http://arxiv.org/pdf/0802.1699v1,2024-04-28,
0802.1790v1,"SAT Has No Wizards","An (encoded) decision problem is a pair (E, F) where E=words that encode instances of the problem, F=words to be accepted. We use ""strings"" in a technical sense. With an NP problem (E, F) we associate the ""logogram"" of F relative to E, which conveys structural information on E, F, and how F is embedded in E. The kernel Ker(P) of a program P that solves (E, F) consists of those strings in the logogram that are used by P. There are relations between Ker(P) and the complexity of P. We develop an application to SAT that relies upon a property of internal independence of SAT. We show that SAT cannot have in its logogram strings serving as collective certificates. As consequence, all programs that solve SAT have same kernel.",2008-02-13T14:43:40Z,http://arxiv.org/pdf/0802.1790v1,2024-04-28,
0802.1829v1,"A review of the Statistical Mechanics approach to Random Optimization Problems","We review the connection between statistical mechanics and the analysis of random optimization problems, with particular emphasis on the random k-SAT problem. We discuss and characterize the different phase transitions that are met in these problems, starting from basic concepts. We also discuss how statistical mechanics methods can be used to investigate the behavior of local search and decimation based algorithms.",2008-02-13T13:45:16Z,http://arxiv.org/pdf/0802.1829v1,2024-04-28,
0802.2300v1,"Approximation Resistant Predicates From Pairwise Independence","We study the approximability of predicates on $k$ variables from a domain $[q]$, and give a new sufficient condition for such predicates to be approximation resistant under the Unique Games Conjecture. Specifically, we show that a predicate $P$ is approximation resistant if there exists a balanced pairwise independent distribution over $[q]^k$ whose support is contained in the set of satisfying assignments to $P$.",2008-02-15T23:21:05Z,http://arxiv.org/pdf/0802.2300v1,2024-04-28,
0802.2833v1,"Limit complexities revisited","The main goal of this paper is to put some known results in a common perspective and to simplify their proofs. We start with a simple proof of a result from (Vereshchagin, 2002) saying that $\limsup_n\KS(x|n)$ (here $\KS(x|n)$ is conditional (plain) Kolmogorov complexity of $x$ when $n$ is known) equals $\KS^{\mathbf{0'}(x)$, the plain Kolmogorov complexity with $\mathbf{0'$-oracle. Then we use the same argument to prove similar results for prefix complexity (and also improve results of (Muchnik, 1987) about limit frequencies), a priori probability on binary tree and measure of effectively open sets. As a by-product, we get a criterion of $\mathbf{0'}$ Martin-L\""of randomness (called also 2-randomness) proved in (Miller, 2004): a sequence $\omega$ is 2-random if and only if there exists $c$ such that any prefix $x$ of $\omega$ is a prefix of some string $y$ such that $\KS(y)\ge |y|-c$. (In the 1960ies this property was suggested in (Kolmogorov, 1968) as one of possible randomness definitions; its equivalence to 2-randomness was shown in (Miller, 2004) while proving another 2-randomness criterion (see also (Nies et al. 2005)): $\omega$ is 2-random if and only if $\KS(x)\ge |x|-c$ for some $c$ and infinitely many prefixes $x$ of $\omega$. Finally, we show that the low-basis theorem can be used to get alternative proofs for these results and to improve the result about effectively open sets; this stronger version implies the 2-randomness criterion mentioned in the previous sentence.",2008-02-20T14:13:31Z,http://arxiv.org/pdf/0802.2833v1,2024-04-28,
0802.2860v1,"A Theory for Valiant's Matchcircuits (Extended Abstract)","The computational function of a matchgate is represented by its character matrix. In this article, we show that all nonsingular character matrices are closed under matrix inverse operation, so that for every $k$, the nonsingular character matrices of $k$-bit matchgates form a group, extending the recent work of Cai and Choudhary (2006) of the same result for the case of $k=2$, and that the single and the two-bit matchgates are universal for matchcircuits, answering a question of Valiant (2002).",2008-02-20T14:31:38Z,http://arxiv.org/pdf/0802.2860v1,2024-04-28,
0802.2868v1,"Efficient Algorithms for Membership in Boolean Hierarchies of Regular Languages","The purpose of this paper is to provide efficient algorithms that decide membership for classes of several Boolean hierarchies for which efficiency (or even decidability) were previously not known. We develop new forbidden-chain characterizations for the single levels of these hierarchies and obtain the following results: - The classes of the Boolean hierarchy over level $\Sigma_1$ of the dot-depth hierarchy are decidable in $NL$ (previously only the decidability was known). The same remains true if predicates mod $d$ for fixed $d$ are allowed. - If modular predicates for arbitrary $d$ are allowed, then the classes of the Boolean hierarchy over level $\Sigma_1$ are decidable. - For the restricted case of a two-letter alphabet, the classes of the Boolean hierarchy over level $\Sigma_2$ of the Straubing-Th\'erien hierarchy are decidable in $NL$. This is the first decidability result for this hierarchy. - The membership problems for all mentioned Boolean-hierarchy classes are logspace many-one hard for $NL$. - The membership problems for quasi-aperiodic languages and for $d$-quasi-aperiodic languages are logspace many-one complete for $PSPACE$.",2008-02-20T14:40:02Z,http://arxiv.org/pdf/0802.2868v1,2024-04-28,
0802.2869v1,"Succinctness of the Complement and Intersection of Regular Expressions","We study the succinctness of the complement and intersection of regular expressions. In particular, we show that when constructing a regular expression defining the complement of a given regular expression, a double exponential size increase cannot be avoided. Similarly, when constructing a regular expression defining the intersection of a fixed and an arbitrary number of regular expressions, an exponential and double exponential size increase, respectively, can in worst-case not be avoided. All mentioned lower bounds improve the existing ones by one exponential and are tight in the sense that the target expression can be constructed in the corresponding time class, i.e., exponential or double exponential time. As a by-product, we generalize a theorem by Ehrenfeucht and Zeiger stating that there is a class of DFAs which are exponentially more succinct than regular expressions, to a fixed four-letter alphabet. When the given regular expressions are one-unambiguous, as for instance required by the XML Schema specification, the complement can be computed in polynomial time whereas the bounds concerning intersection continue to hold. For the subclass of single-occurrence regular expressions, we prove a tight exponential lower bound for intersection.",2008-02-20T14:40:53Z,http://arxiv.org/pdf/0802.2869v1,2024-04-28,
0802.3254v1,"General Algorithms for Testing the Ambiguity of Finite Automata","This paper presents efficient algorithms for testing the finite, polynomial, and exponential ambiguity of finite automata with $\epsilon$-transitions. It gives an algorithm for testing the exponential ambiguity of an automaton $A$ in time $O(|A|_E^2)$, and finite or polynomial ambiguity in time $O(|A|_E^3)$. These complexities significantly improve over the previous best complexities given for the same problem. Furthermore, the algorithms presented are simple and are based on a general algorithm for the composition or intersection of automata. We also give an algorithm to determine the degree of polynomial ambiguity of a finite automaton $A$ that is polynomially ambiguous in time $O(|A|_E^3)$. Finally, we present an application of our algorithms to an approximate computation of the entropy of a probabilistic automaton.",2008-02-22T05:20:08Z,http://arxiv.org/pdf/0802.3254v1,2024-04-28,
0802.3860v1,"Separating NOF communication complexity classes RP and NP","We provide a non-explicit separation of the number-on-forehead communication complexity classes RP and NP when the number of players is up to \delta log(n) for any \delta<1. Recent lower bounds on Set-Disjointness [LS08,CA08] provide an explicit separation between these classes when the number of players is only up to o(loglog(n)).",2008-02-26T19:58:26Z,http://arxiv.org/pdf/0802.3860v1,2024-04-28,
0802.4312v2,"Curves That Must Be Retraced","We exhibit a polynomial time computable plane curve GAMMA that has finite length, does not intersect itself, and is smooth except at one endpoint, but has the following property. For every computable parametrization f of GAMMA and every positive integer n, there is some positive-length subcurve of GAMMA that f retraces at least n times. In contrast, every computable curve of finite length that does not intersect itself has a constant-speed (hence non-retracing) parametrization that is computable relative to the halting problem.",2008-02-29T00:58:26Z,http://arxiv.org/pdf/0802.4312v2,2024-04-28,
0803.0055v1,"A compact topology for sand automata","In this paper, we exhibit a strong relation between the sand automata configuration space and the cellular automata configuration space. This relation induces a compact topology for sand automata, and a new context in which sand automata are homeomorphic to cellular automata acting on a specific subshift. We show that the existing topological results for sand automata, including the Hedlund-like representation theorem, still hold. In this context, we give a characterization of the cellular automata which are sand automata, and study some dynamical behaviors such as equicontinuity. Furthermore, we deal with the nilpotency. We show that the classical definition is not meaningful for sand automata. Then, we introduce a suitable new notion of nilpotency for sand automata. Finally, we prove that this simple dynamical behavior is undecidable.",2008-03-01T08:36:39Z,http://arxiv.org/pdf/0803.0055v1,2024-04-28,
0803.1030v3,"Robust Stochastic Chemical Reaction Networks and Bounded Tau-Leaping","The behavior of some stochastic chemical reaction networks is largely unaffected by slight inaccuracies in reaction rates. We formalize the robustness of state probabilities to reaction rate deviations, and describe a formal connection between robustness and efficiency of simulation. Without robustness guarantees, stochastic simulation seems to require computational time proportional to the total number of reaction events. Even if the concentration (molecular count per volume) stays bounded, the number of reaction events can be linear in the duration of simulated time and total molecular count. We show that the behavior of robust systems can be predicted such that the computational work scales linearly with the duration of simulated time and concentration, and only polylogarithmically in the total molecular count. Thus our asymptotic analysis captures the dramatic speedup when molecular counts are large, and shows that for bounded concentrations the computation time is essentially invariant with molecular count. Finally, by noticing that even robust stochastic chemical reaction networks are capable of embedding complex computational problems, we argue that the linear dependence on simulated time and concentration is likely optimal.",2008-03-07T17:36:54Z,http://arxiv.org/pdf/0803.1030v3,2024-04-28,
0803.4206v2,"Product theorems via semidefinite programming","The tendency of semidefinite programs to compose perfectly under product has been exploited many times in complexity theory: for example, by Lovasz to determine the Shannon capacity of the pentagon; to show a direct sum theorem for non-deterministic communication complexity and direct product theorems for discrepancy; and in interactive proof systems to show parallel repetition theorems for restricted classes of games.   Despite all these examples of product theorems--some going back nearly thirty years--it was only recently that Mittal and Szegedy began to develop a general theory to explain when and why semidefinite programs behave perfectly under product. This theory captured many examples in the literature, but there were also some notable exceptions which it could not explain--namely, an early parallel repetition result of Feige and Lovasz, and a direct product theorem for the discrepancy method of communication complexity by Lee, Shraibman, and Spalek.   We extend the theory of Mittal and Szegedy to explain these cases as well. Indeed, to the best of our knowledge, our theory captures all examples of semidefinite product theorems in the literature.",2008-03-28T20:27:47Z,http://arxiv.org/pdf/0803.4206v2,2024-04-28,
0803.4261v1,"Common Permutation Problem","In this paper we show that the following problem is NP-complete: Given an alphabet $\Sigma$ and two strings over $\Sigma$, the question is whether there exists a permutation of $\Sigma$ which is a subsequence of both of the given strings.",2008-03-29T11:32:29Z,http://arxiv.org/pdf/0803.4261v1,2024-04-28,
0803.4516v1,"A Dual Polynomial for OR","We reprove that the approximate degree of the OR function on n bits is Omega(sqrt(n)). We consider a linear program which is feasible if and only if there is an approximate polynomial for a given function, and apply the duality theory. The duality theory says that the primal program has no solution if and only if its dual has a solution. Therefore one can prove the nonexistence of an approximate polynomial by exhibiting a dual solution, coined the dual polynomial. We construct such a polynomial.",2008-03-31T18:20:53Z,http://arxiv.org/pdf/0803.4516v1,2024-04-28,
0804.0957v2,"Derandomizing the Isolation Lemma and Lower Bounds for Circuit Size","The isolation lemma of Mulmuley et al \cite{MVV87} is an important tool in the design of randomized algorithms and has played an important role in several nontrivial complexity upper bounds. On the other hand, polynomial identity testing is a well-studied algorithmic problem with efficient randomized algorithms and the problem of obtaining efficient \emph{deterministic} identity tests has received a lot of attention recently. The goal of this note is to compare the isolation lemma with polynomial identity testing: 1. We show that derandomizing reasonably restricted versions of the isolation lemma implies circuit size lower bounds. We derive the circuit lower bounds by examining the connection between the isolation lemma and polynomial identity testing. We give a randomized polynomial-time identity test for noncommutative circuits of polynomial degree based on the isolation lemma. Using this result, we show that derandomizing the isolation lemma implies noncommutative circuit size lower bounds. The restricted versions of the isolation lemma we consider are natural and would suffice for the standard applications of the isolation lemma. 2. From the result of Klivans-Spielman \cite{KS01} we observe that there is a randomized polynomial-time identity test for commutative circuits of polynomial degree, also based on a more general isolation lemma for linear forms. Consequently, derandomization of (a suitable version of) this isolation lemma also implies circuit size lower bounds in the commutative setting.",2008-04-07T04:04:21Z,http://arxiv.org/pdf/0804.0957v2,2024-04-28,
0804.1079v12,"P is a proper subset of NP","The purpose of this article is to examine and limit the conditions in which the P complexity class could be equivalent to the NP complexity class. Proof is provided by demonstrating that as the number of clauses in a NP-complete problem approaches infinity, the number of input sets processed per computation performed also approaches infinity when solved by a polynomial time solution. It is then possible to determine that the only deterministic optimization of a NP-complete problem that could prove P = NP would be one that examines no more than a polynomial number of input sets for a given problem.   It is then shown that subdividing the set of all possible input sets into a representative polynomial search partition is a problem in the FEXP complexity class. The findings of this article are combined with the findings of other articles in this series of 4 articles. The final conclusion will be demonstrated that P =/= NP.",2008-04-07T16:58:59Z,http://arxiv.org/pdf/0804.1079v12,2024-04-28,
0805.0517v5,"Analysis of the Deterministic Polynomial Time Solvability of the 0-1-Knapsack Problem","Previously the author has demonstrated that a representative polynomial search partition is required to solve a NP-complete problem in deterministic polynomial time. It has also been demonstrated that finding such a partition can only be done in deterministic polynomial time if the form of the problem provides a simple method for producing the partition. It is the purpose of this article to demonstrate that no deterministic polynomial time method exists to produce a representative polynomial search partition for the Knapsack problem.",2008-05-05T12:34:27Z,http://arxiv.org/pdf/0805.0517v5,2024-04-28,
0805.1385v3,"Almost-natural proofs","Razborov and Rudich have shown that so-called ""natural proofs"" are not useful for separating P from NP unless hard pseudorandom number generators do not exist. This famous result is widely regarded as a serious barrier to proving strong lower bounds in circuit complexity theory.   By definition, a natural combinatorial property satisfies two conditions, constructivity and largeness. Our main result is that if the largeness condition is weakened slightly, then not only does the Razborov-Rudich proof break down, but such ""almost-natural"" (and useful) properties provably exist. Specifically, under the same pseudorandomness assumption that Razborov and Rudich make, a simple, explicit property that we call ""discrimination"" suffices to separate P/poly from NP; discrimination is nearly linear-time computable and almost large, having density 2^{-q(n)} where q is a quasi-polynomial function. For those who hope to separate P from NP using random function properties in some sense, discrimination is interesting, because it is constructive, yet may be thought of as a minor alteration of a property of a random function.   The proof relies heavily on the self-defeating character of natural proofs. Our proof technique also yields an unconditional result, namely that there exist almost-large and useful properties that are constructive, if we are allowed to call non-uniform low-complexity classes ""constructive."" We note, though, that this unconditional result can also be proved by a more conventional counting argument.   Finally, we give an alternative proof, communicated to us by Salil Vadhan at FOCS 2008, of one of our theorems, and we make some speculative remarks on the future prospects for proving strong circuit lower bounds.",2008-05-09T18:14:43Z,http://arxiv.org/pdf/0805.1385v3,2024-04-28,
0805.1765v1,"Efficiently Testing Sparse GF(2) Polynomials","We give the first algorithm that is both query-efficient and time-efficient for testing whether an unknown function $f: \{0,1\}^n \to \{0,1\}$ is an $s$-sparse GF(2) polynomial versus $\eps$-far from every such polynomial. Our algorithm makes $\poly(s,1/\eps)$ black-box queries to $f$ and runs in time $n \cdot \poly(s,1/\eps)$. The only previous algorithm for this testing problem \cite{DLM+:07} used poly$(s,1/\eps)$ queries, but had running time exponential in $s$ and super-polynomial in $1/\eps$.   Our approach significantly extends the ``testing by implicit learning'' methodology of \cite{DLM+:07}. The learning component of that earlier work was a brute-force exhaustive search over a concept class to find a hypothesis consistent with a sample of random examples. In this work, the learning component is a sophisticated exact learning algorithm for sparse GF(2) polynomials due to Schapire and Sellie \cite{SchapireSellie:96}. A crucial element of this work, which enables us to simulate the membership queries required by \cite{SchapireSellie:96}, is an analysis establishing new properties of how sparse GF(2) polynomials simplify under certain restrictions of ``low-influence'' sets of variables.",2008-05-13T00:51:30Z,http://arxiv.org/pdf/0805.1765v1,2024-04-28,
0805.2135v1,"Communication Lower Bounds Using Dual Polynomials","Representations of Boolean functions by real polynomials play an important role in complexity theory. Typically, one is interested in the least degree of a polynomial p(x_1,...,x_n) that approximates or sign-represents a given Boolean function f(x_1,...,x_n). This article surveys a new and growing body of work in communication complexity that centers around the dual objects, i.e., polynomials that certify the difficulty of approximating or sign-representing a given function. We provide a unified guide to the following results, complete with all the key proofs:   (1) Sherstov's Degree/Discrepancy Theorem, which translates lower bounds on the threshold degree of a Boolean function into upper bounds on the discrepancy of a related function;   (2) Two different methods for proving lower bounds on bounded-error communication based on the approximate degree: Sherstov's pattern matrix method and Shi and Zhu's block composition method;   (3) Extension of the pattern matrix method to the multiparty model, obtained by Lee and Shraibman and by Chattopadhyay and Ada, and the resulting improved lower bounds for DISJOINTNESS;   (4) David and Pitassi's separation of NP and BPP in multiparty communication complexity for k=(1-eps)log n players.",2008-05-14T18:52:06Z,http://arxiv.org/pdf/0805.2135v1,2024-04-28,
0805.2170v6,"Independence of P vs. NP in regards to oracle relativizations","This is the third article in a series of four articles dealing with the P vs. NP question. The purpose of this work is to demonstrate that the methods used in the first two articles of this series are not affected by oracle relativizations. Furthermore, the solution to the P vs. NP problem is actually independent of oracle relativizations.",2008-05-14T21:10:55Z,http://arxiv.org/pdf/0805.2170v6,2024-04-28,
0805.3058v1,"A New Structural Property of SAT","We review a minimum set of notions from our previous paper on structural properties of SAT at arXiv:0802.1790 that will allow us to define and discuss the ""complete internal independence"" of a decision problem. This property is strictly stronger than the independence property that was called ""strong internal independence"" in cited paper. We show that SAT exhibits this property. We argue that this form of independence of a decision problem is the strongest possible for a problem. By relying upon this maximally strong form of internal independence, we reformulate in more strict terms the informal remarks on possible exponentiality of SAT that concluded our previous paper. The net result of that reformulation is a hint for a proof for SAT being exponential. We conjecture that a complete proof of that proposition can be obtained by strictly following the line of given hint of proof.",2008-05-20T12:07:51Z,http://arxiv.org/pdf/0805.3058v1,2024-04-28,
0806.1041v1,"3-connected Planar Graph Isomorphism is in Log-space","We show that the isomorphism of 3-connected planar graphs can be decided in deterministic log-space. This improves the previously known bound UL$\cap$coUL of Thierauf and Wagner.",2008-06-05T19:33:29Z,http://arxiv.org/pdf/0806.1041v1,2024-04-28,
0807.1412v1,"Quantum Query Complexity of Multilinear Identity Testing","Motivated by the quantum algorithm in \cite{MN05} for testing commutativity of black-box groups, we study the following problem: Given a black-box finite ring $R=\angle{r_1,...,r_k}$ where $\{r_1,r_2,...,r_k\}$ is an additive generating set for $R$ and a multilinear polynomial $f(x_1,...,x_m)$ over $R$ also accessed as a black-box function $f:R^m\to R$ (where we allow the indeterminates $x_1,...,x_m$ to be commuting or noncommuting), we study the problem of testing if $f$ is an \emph{identity} for the ring $R$. More precisely, the problem is to test if $f(a_1,a_2,...,a_m)=0$ for all $a_i\in R$.   We give a quantum algorithm with query complexity $O(m(1+\alpha)^{m/2} k^{\frac{m}{m+1}})$ assuming $k\geq (1+1/\alpha)^{m+1}$. Towards a lower bound, we also discuss a reduction from a version of $m$-collision to this problem.   We also observe a randomized test with query complexity $4^mmk$ and constant success probability and a deterministic test with $k^m$ query complexity.",2008-07-09T10:39:19Z,http://arxiv.org/pdf/0807.1412v1,2024-04-28,
0808.2662v3,"Multitask Efficiencies in the Decision Tree Model","In Direct Sum problems [KRW], one tries to show that for a given computational model, the complexity of computing a collection of finite functions on independent inputs is approximately the sum of their individual complexities. In this paper, by contrast, we study the diversity of ways in which the joint computational complexity can behave when all the functions are evaluated on a common input. We focus on the deterministic decision tree model, with depth as the complexity measure; in this model we prove a result to the effect that the 'obvious' constraints on joint computational complexity are essentially the only ones.   The proof uses an intriguing new type of cryptographic data structure called a `mystery bin' which we construct using a small polynomial separation between deterministic and unambiguous query complexity shown by Savicky. We also pose a variant of the Direct Sum Conjecture of [KRW] which, if proved for a single family of functions, could yield an analogous result for models such as the communication model.",2008-08-20T11:15:33Z,http://arxiv.org/pdf/0808.2662v3,2024-04-28,
0808.3222v5,"Analysis of the postulates produced by Karp's Theorem","This is the final article in a series of four articles. Richard Karp has proven that a deterministic polynomial time solution to K-SAT will result in a deterministic polynomial time solution to all NP-Complete problems. However, it is demonstrated that a deterministic polynomial time solution to any NP-Complete problem does not necessarily produce a deterministic polynomial time solution to all NP-Complete problems.",2008-08-24T02:59:29Z,http://arxiv.org/pdf/0808.3222v5,2024-04-28,
0809.0257v4,"Linear Kernelizations for Restricted 3-Hitting Set Problems","The 3-\textsc{Hitting Set} problem is also called the \textsc{Vertex Cover} problem on 3-uniform hypergraphs. In this paper, we address kernelizations of the \textsc{Vertex Cover} problem on 3-uniform hypergraphs. We show that this problem admits a linear kernel in three classes of 3-uniform hypergraphs. We also obtain lower and upper bounds on the kernel size for them by the parametric duality.",2008-09-01T14:52:30Z,http://arxiv.org/pdf/0809.0257v4,2024-04-28,
0809.0352v3,"Instruction sequences and non-uniform complexity theory","We develop theory concerning non-uniform complexity in a setting in which the notion of single-pass instruction sequence considered in program algebra is the central notion. We define counterparts of the complexity classes P/poly and NP/poly and formulate a counterpart of the complexity theoretic conjecture that NP is not included in P/poly. In addition, we define a notion of completeness for the counterpart of NP/poly using a non-uniform reducibility relation and formulate complexity hypotheses which concern restrictions on the instruction sequences used for computation. We think that the theory developed opens up an additional way of investigating issues concerning non-uniform complexity.",2008-09-02T05:45:48Z,http://arxiv.org/pdf/0809.0352v3,2024-04-28,
0809.1836v1,"The complexity of counting solutions to Generalised Satisfiability Problems modulo k","Generalised Satisfiability Problems (or Boolean Constraint Satisfaction Problems), introduced by Schaefer in 1978, are a general class of problem which allow the systematic study of the complexity of satisfiability problems with different types of constraints. In 1979, Valiant introduced the complexity class parity P, the problem of counting the number of solutions to NP problems modulo two. Others have since considered the question of counting modulo other integers.   We give a dichotomy theorem for the complexity of counting the number of solutions to Generalised Satisfiability Problems modulo integers. This follows from an earlier result of Creignou and Hermann which gave a counting dichotomy for these types of problem, and the dichotomy itself is almost identical. Specifically, counting the number of solutions to a Generalised Satisfiability Problem can be done in polynomial time if all the relations are affine. Otherwise, except for one special case with k = 2, it is #_kP-complete.",2008-09-10T16:23:53Z,http://arxiv.org/pdf/0809.1836v1,2024-04-28,
0809.2093v2,"An approximation algorithm for approximation rank","One of the strongest techniques available for showing lower bounds on quantum communication complexity is the logarithm of the approximation rank of the communication matrix--the minimum rank of a matrix which is entrywise close to the communication matrix. This technique has two main drawbacks: it is difficult to compute, and it is not known to lower bound quantum communication complexity with entanglement.   Linial and Shraibman recently introduced a norm, called gamma_2^{alpha}, to quantum communication complexity, showing that it can be used to lower bound communication with entanglement. Here the parameter alpha is a measure of approximation which is related to the allowable error probability of the protocol. This bound can be written as a semidefinite program and gives bounds at least as large as many techniques in the literature, although it is smaller than the corresponding alpha-approximation rank, rk_alpha. We show that in fact log gamma_2^{alpha}(A)$ and log rk_{alpha}(A)$ agree up to small factors. As corollaries we obtain a constant factor polynomial time approximation algorithm to the logarithm of approximate rank, and that the logarithm of approximation rank is a lower bound for quantum communication complexity with entanglement.",2008-09-11T20:06:56Z,http://arxiv.org/pdf/0809.2093v2,2024-04-28,
0809.2319v2,"A Log-space Algorithm for Canonization of Planar Graphs","Graph Isomorphism is the prime example of a computational problem with a wide difference between the best known lower and upper bounds on its complexity. We bridge this gap for a natural and important special case, planar graph isomorphism, by presenting an upper bound that matches the known logspace hardness [Lindell'92]. In fact, we show the formally stronger result that planar graph canonization is in logspace. This improves the previously known upper bound of AC1 [MillerReif'91].   Our algorithm first constructs the biconnected component tree of a connected planar graph and then refines each biconnected component into a triconnected component tree. The next step is to logspace reduce the biconnected planar graph isomorphism and canonization problems to those for 3-connected planar graphs, which are known to be in logspace by [DattaLimayeNimbhorkar'08]. This is achieved by using the above decomposition, and by making significant modifications to Lindell's algorithm for tree canonization, along with changes in the space complexity analysis.   The reduction from the connected case to the biconnected case requires further new ideas, including a non-trivial case analysis and a group theoretic lemma to bound the number of automorphisms of a colored 3-connected planar graph. This lemma is crucial for the reduction to work in logspace.",2008-09-15T06:22:39Z,http://arxiv.org/pdf/0809.2319v2,2024-04-28,
0809.3614v1,"Improved Monotone Circuit Depth Upper Bound for Directed Graph Reachability","We prove that the directed graph reachability problem (transitive closure) can be solved by monotone fan-in 2 boolean circuits of depth (1/2+o(1))(log n)^2, where n is the number of nodes. This improves the previous known upper bound (1+o(1))(log n)^2. The proof is non-constructive, but we give a constructive proof of the upper bound (7/8+o(1))(log n)^2.",2008-09-22T15:14:06Z,http://arxiv.org/pdf/0809.3614v1,2024-04-28,
0810.1018v1,"A simple constant-probability RP reduction from NP to Parity P","The proof of Toda's celebrated theorem that the polynomial hierarchy is contained in $\P^{# P}$ relies on the fact that, under mild technical conditions on the complexity class $C$, we have $\exists C \subset BP \cdot \oplus C$. More concretely, there is a randomized reduction which transforms nonempty sets and the empty set, respectively, into sets of odd or even size. The customary method is to invoke Valiant's and Vazirani's randomized reduction from NP to UP, followed by amplification of the resulting success probability from $1/\poly(n)$ to a constant by combining the parities of $\poly(n)$ trials. Here we give a direct algebraic reduction which achieves constant success probability without the need for amplification. Our reduction is very simple, and its analysis relies on well-known properties of the Legendre symbol in finite fields.",2008-10-06T17:23:06Z,http://arxiv.org/pdf/0810.1018v1,2024-04-28,
0810.4249v1,"Ogden's Lemma for Regular Tree Languages","We motivate and prove a strong pumping lemma for regular tree languages. The new lemma can be seen as the natural correspondent of Ogden's lemma for context-free string languages.",2008-10-23T19:43:26Z,http://arxiv.org/pdf/0810.4249v1,2024-04-28,
0812.0852v3,"Hierarchy and equivalence of multi-letter quantum finite automata","Multi-letter {\it quantum finite automata} (QFAs) were a new one-way QFA model proposed recently by Belovs, Rosmanis, and Smotrovs (LNCS, Vol. 4588, Springer, Berlin, 2007, pp. 60-71), and they showed that multi-letter QFAs can accept with no error some regular languages ($(a+b)^{*}b$) that are unacceptable by the one-way QFAs. In this paper, we continue to study multi-letter QFAs. We mainly focus on two issues: (1) we show that $(k+1)$-letter QFAs are computationally more powerful than $k$-letter QFAs, that is, $(k+1)$-letter QFAs can accept some regular languages that are unacceptable by any $k$-letter QFA. A comparison with the one-way QFAs is made by some examples; (2) we prove that a $k_{1}$-letter QFA ${\cal A}_1$ and another $k_{2}$-letter QFA ${\cal A}_2$ are equivalent if and only if they are $(n_{1}+n_{2})^{4}+k-1$-equivalent, and the time complexity of determining the equivalence of two multi-letter QFAs using this method is $O(n^{12}+k^{2}n^{4}+kn^{8})$, where $n_{1}$ and $n_{2}$ are the numbers of states of ${\cal A}_{1}$ and ${\cal A}_{2}$, respectively, and $k=\max(k_{1},k_{2})$. Some other issues are addressed for further consideration.",2008-12-04T03:10:29Z,http://arxiv.org/pdf/0812.0852v3,2024-04-28,
0812.1601v4,"Scarf is Ppad-Complete","Scarf's lemma is one of the fundamental results in combinatorics, originally introduced to study the core of an N-person game. Over the last four decades, the usefulness of Scarf's lemma has been demonstrated in several important combinatorial problems seeking ""stable"" solutions. However, the complexity of the computational version of Scarf's lemma (SCARF) remained open. In this paper, we prove that SCARF is complete for the complexity class PPAD. This proves that SCARF is as hard as the computational versions of Brouwer's fixed point theorem and Sperner's lemma. Hence, there is no polynomial-time algorithm for SCARF unless PPAD \subseteq P. We also show that fractional stable paths problem and finding strong fractional kernels in digraphs are PPAD-hard.",2008-12-09T01:07:25Z,http://arxiv.org/pdf/0812.1601v4,2024-04-28,
0812.3214v2,"Two conjectures such that the proof of any one of them will lead to the proof that P = NP","In this paper we define a construct called a time-graph. A complete time-graph of order n is the cartesian product of a complete graph with n vertices and a linear graph with n vertices. A time-graph of order n is given by a subset of the set of edges E(n) of such a graph. The notion of a hamiltonian time-graph is defined in a natural way and we define the Hamiltonian time-graph problem (HAMTG) as : Given a time-graph is it hamiltonian ? We show that the Hamiltonian path problem (HAMP) can be transformed to HAMTG in polynomial time. We then define certain vector spaces of functions from E(n) and E(n)xE(n) to B = {0,1}, the field of two elements and derive certain properties of these spaces. We give two conjectures about these spaces and prove that if any one of these conjectures is true, we get a polynomial time algorithm for the Hamiltonian path problem. Since the Hamiltonian path problem is NP-complete we obtain the proof of P = NP provided any one of the two conjectures is true.",2008-12-17T07:03:53Z,http://arxiv.org/pdf/0812.3214v2,2024-04-28,
0812.4009v21,"Graph Field Automata","The Graph Automata have been the paradigm in the expression of utilizing Graphs as a language. Matrix Graph grammars \cite{Pedro} are an algebratization of graph rewriting systems. Here we present the dual of this formalizm which some extensions which we term Graph Field Automata The advantage to this approach is a framework for expressing machines that can use Matrix Graph Grammars.",2008-12-22T00:26:29Z,http://arxiv.org/pdf/0812.4009v21,2024-04-28,
0901.2703v2,"Language recognition by generalized quantum finite automata with unbounded error (abstract & poster)","In this note, we generalize the results of arXiv:0901.2703v1 We show that all one-way quantum finite automaton (QFA) models that are at least as general as Kondacs-Watrous QFA's are equivalent in power to classical probabilistic finite automata in this setting. Unlike their probabilistic counterparts, allowing the tape head to stay put for some steps during its traversal of the input does enlarge the class of languages recognized by such QFA's with unbounded error. (Note that, the proof of Theorem 1 in the abstract was presented in the previous version (arXiv:0901.2703v1).)",2009-01-18T13:33:21Z,http://arxiv.org/pdf/0901.2703v2,2024-04-28,
0901.2906v1,"Measuring communication complexity using instance complexity with oracles","We establish a connection between non-deterministic communication complexity and instance complexity, a measure of information based on algorithmic entropy. Let $\overline{x}$, $\overline{y}$ and $Y_1(\overline{x})$ be respectively the input known by Alice, the input known by Bob, and the set of all values of $y$ such that $f(\overline{x},y)=1$; a string is a witness of the non-deterministic communication protocol iff it is a program $p$ that ""corresponds exactly"" to the instance complexity $\ic^{f,t}(\overline{y}:Y_1(\overline{x}))$.",2009-01-19T18:00:37Z,http://arxiv.org/pdf/0901.2906v1,2024-04-28,
0902.0047v1,"Bounds on the Size of Small Depth Circuits for Approximating Majority","In this paper, we show that for every constant $0 < \epsilon < 1/2$ and for every constant $d \geq 2$, the minimum size of a depth $d$ Boolean circuit that $\epsilon$-approximates Majority function on $n$ variables is exp$(\Theta(n^{1/(2d-2)}))$. The lower bound for every $d \geq 2$ and the upper bound for $d=2$ have been previously shown by O'Donnell and Wimmer [ICALP'07], and the contribution of this paper is to give a matching upper bound for $d \geq 3$.",2009-01-31T04:22:52Z,http://arxiv.org/pdf/0902.0047v1,2024-04-28,
0902.1609v1,"Asymptotically Optimal Lower Bounds on the NIH-Multi-Party Information","Here we prove an asymptotically optimal lower bound on the information complexity of the k-party disjointness function with the unique intersection promise, an important special case of the well known disjointness problem, and the ANDk-function in the number in the hand model. Our (n/k) bound for disjointness improves on an earlier (n/(k log k)) bound by Chakrabarti et al. (2003), who obtained an asymptotically tight lower bound for one-way protocols, but failed to do so for the general case. Our result eliminates both the gap between the upper and the lower bound for unrestricted protocols and the gap between the lower bounds for one-way protocols and unrestricted protocols.",2009-02-10T09:13:26Z,http://arxiv.org/pdf/0902.1609v1,2024-04-28,
0902.1835v2,"Polynomial Kernelizations for MIN F^+Pi_1 and MAX NP","It has been observed in many places that constant-factor approximable problems often admit polynomial or even linear problem kernels for their decision versions, e.g., Vertex Cover, Feedback Vertex Set, and Triangle Packing. While there exist examples like Bin Packing, which does not admit any kernel unless P = NP, there apparently is a strong relation between these two polynomial-time techniques. We add to this picture by showing that the natural decision versions of all problems in two prominent classes of constant-factor approximable problems, namely MIN F^+\Pi_1 and MAX NP, admit polynomial problem kernels. Problems in MAX SNP, a subclass of MAX NP, are shown to admit kernels with a linear base set, e.g., the set of vertices of a graph. This extends results of Cai and Chen (JCSS 1997), stating that the standard parameterizations of problems in MAX SNP and MIN F^+\Pi_1 are fixed-parameter tractable, and complements recent research on problems that do not admit polynomial kernelizations (Bodlaender et al. JCSS 2009).",2009-02-11T10:19:38Z,http://arxiv.org/pdf/0902.1835v2,2024-04-28,
0902.1866v1,"A Superpolynomial Lower Bound on the Size of Uniform Non-constant-depth Threshold Circuits for the Permanent","We show that the permanent cannot be computed by DLOGTIME-uniform threshold or arithmetic circuits of depth o(log log n) and polynomial size.",2009-02-11T12:39:25Z,http://arxiv.org/pdf/0902.1866v1,2024-04-28,
0902.2081v2,"Languages recognized by nondeterministic quantum finite automata","The nondeterministic quantum finite automaton (NQFA) is the only known case where a one-way quantum finite automaton (QFA) model has been shown to be strictly superior in terms of language recognition power to its probabilistic counterpart. We give a characterization of the class of languages recognized by NQFA's, demonstrating that it is equal to the class of exclusive stochastic languages. We also characterize the class of languages that are recognized necessarily by two-sided error by QFA's. It is shown that these classes remain the same when the QFA's used in their definitions are replaced by several different model variants that have appeared in the literature. We prove several closure properties of the related classes. The ramifications of these results about classical and quantum sublogarithmic space complexity classes are examined.",2009-02-12T10:59:09Z,http://arxiv.org/pdf/0902.2081v2,2024-04-28,
0902.2146v1,"A Stronger LP Bound for Formula Size Lower Bounds via Clique Constraints","We introduce a new technique proving formula size lower bounds based on the linear programming bound originally introduced by Karchmer, Kushilevitz and Nisan [11] and the theory of stable set polytope. We apply it to majority functions and prove their formula size lower bounds improved from the classical result of Khrapchenko [13]. Moreover, we introduce a notion of unbalanced recursive ternary majority functions motivated by a decomposition theory of monotone self-dual functions and give integrally matching upper and lower bounds of their formula size. We also show monotone formula size lower bounds of balanced recursive ternary majority functions improved from the quantum adversary bound of Laplante, Lee and Szegedy [15].",2009-02-12T16:01:09Z,http://arxiv.org/pdf/0902.2146v1,2024-04-28,
0902.2300v2,"A Dichotomy Theorem for Polynomial Evaluation","A dichotomy theorem for counting problems due to Creignou and Hermann states that or any nite set S of logical relations, the counting problem #SAT(S) is either in FP, or #P-complete. In the present paper we show a dichotomy theorem for polynomial evaluation. That is, we show that for a given set S, either there exists a VNP-complete family of polynomials associated to S, or the associated families of polynomials are all in VP. We give a concise characterization of the sets S that give rise to ""easy"" and ""hard"" polynomials. We also prove that several problems which were known to be #P-complete under Turing reductions only are in fact #P-complete under many-one reductions.",2009-02-13T12:34:27Z,http://arxiv.org/pdf/0902.2300v2,2024-04-28,
0902.2674v4,"Inseparability and Strong Hypotheses for Disjoint NP Pairs","This paper investigates the existence of inseparable disjoint pairs of NP languages and related strong hypotheses in computational complexity. Our main theorem says that, if NP does not have measure 0 in EXP, then there exist disjoint pairs of NP languages that are P-inseparable, in fact TIME(2^(n^k))-inseparable. We also relate these conditions to strong hypotheses concerning randomness and genericity of disjoint pairs.",2009-02-16T12:27:54Z,http://arxiv.org/pdf/0902.2674v4,2024-04-28,
0902.3757v1,"Bounded Independence Fools Halfspaces","We show that any distribution on {-1,1}^n that is k-wise independent fools any halfspace h with error \eps for k = O(\log^2(1/\eps) /\eps^2). Up to logarithmic factors, our result matches a lower bound by Benjamini, Gurel-Gurevich, and Peled (2007) showing that k = \Omega(1/(\eps^2 \cdot \log(1/\eps))). Using standard constructions of k-wise independent distributions, we obtain the first explicit pseudorandom generators G: {-1,1}^s --> {-1,1}^n that fool halfspaces. Specifically, we fool halfspaces with error eps and seed length s = k \log n = O(\log n \cdot \log^2(1/\eps) /\eps^2).   Our approach combines classical tools from real approximation theory with structural results on halfspaces by Servedio (Computational Complexity 2007).",2009-02-21T21:01:21Z,http://arxiv.org/pdf/0902.3757v1,2024-04-28,
0903.0050v2,"Succinctness of two-way probabilistic and quantum finite automata","We prove that two-way probabilistic and quantum finite automata (2PFA's and 2QFA's) can be considerably more concise than both their one-way versions (1PFA's and 1QFA's), and two-way nondeterministic finite automata (2NFA's). For this purpose, we demonstrate several infinite families of regular languages which can be recognized with some fixed probability greater than $ {1/2} $ by just tuning the transition amplitudes of a 2QFA (and, in one case, a 2PFA) with a constant number of states, whereas the sizes of the corresponding 1PFA's, 1QFA's and 2NFA's grow without bound. We also show that 2QFA's with mixed states can support highly efficient probability amplification. The weakest known model of computation where quantum computers recognize more languages with bounded error than their classical counterparts is introduced.",2009-02-28T07:33:50Z,http://arxiv.org/pdf/0903.0050v2,2024-04-28,
0903.4728v2,"Graph Homomorphisms with Complex Values: A Dichotomy Theorem","Graph homomorphism has been studied intensively. Given an m x m symmetric matrix A, the graph homomorphism function is defined as \[Z_A (G) = \sum_{f:V->[m]} \prod_{(u,v)\in E} A_{f(u),f(v)}, \] where G = (V,E) is any undirected graph. The function Z_A can encode many interesting graph properties, including counting vertex covers and k-colorings. We study the computational complexity of Z_A for arbitrary symmetric matrices A with algebraic complex values. Building on work by Dyer and Greenhill, Bulatov and Grohe, and especially the recent beautiful work by Goldberg, Grohe, Jerrum and Thurley, we prove a complete dichotomy theorem for this problem. We show that Z_A is either computable in polynomial-time or #P-hard, depending explicitly on the matrix A. We further prove that the tractability criterion on A is polynomial-time decidable.",2009-03-27T02:57:32Z,http://arxiv.org/pdf/0903.4728v2,2024-04-28,
0904.0698v3,"About the impossibility to prove P=NP and the pseudo-randomness in NP","The relationship between the complexity classes P and NP is an unsolved question in the field of theoretical computer science. In this paper, we look at the link between the P - NP question and the ""Deterministic"" versus ""Non Deterministic"" nature of a problem, and more specifically at the temporal nature of the complexity within the NP class of problems. Let us remind that the NP class is called the class of ""Non Deterministic Polynomial"" languages. Using the meta argument that results in Mathematics should be ""time independent"" as they are reproducible, the paper shows that the P!=NP assertion is impossible to prove in the a-temporal framework of Mathematics. In a previous version of the report, we use a similar argument based on randomness to show that the P = NP assertion was also impossible to prove, but this part of the paper was shown to be incorrect. So, this version deletes it. In fact, this paper highlights the time dependence of the complexity for any NP problem, linked to some pseudo-randomness in its heart.",2009-04-04T09:16:04Z,http://arxiv.org/pdf/0904.0698v3,2024-04-28,
0904.3116v4,"Variations on Muchnik's Conditional Complexity Theorem","Muchnik's theorem about simple conditional descriptions states that for all strings $a$ and $b$ there exists a short program $p$ transforming $a$ to $b$ that has the least possible length and is simple conditional on $b$. In this paper we present two new proofs of this theorem. The first one is based on the on-line matching algorithm for bipartite graphs. The second one, based on extractors, can be generalized to prove a version of Muchnik's theorem for space-bounded Kolmogorov complexity.",2009-04-20T21:05:09Z,http://arxiv.org/pdf/0904.3116v4,2024-04-28,
0904.3912v2,"Refutation of Aslam's Proof that NP = P","Aslam presents an algorithm he claims will count the number of perfect matchings in any incomplete bipartite graph with an algorithm in the function-computing version of NC, which is itself a subset of FP. Counting perfect matchings is known to be #P-complete; therefore if Aslam's algorithm is correct, then NP=P. However, we show that Aslam's algorithm does not correctly count the number of perfect matchings and offer an incomplete bipartite graph as a concrete counter-example.",2009-04-24T18:01:54Z,http://arxiv.org/pdf/0904.3912v2,2024-04-28,
0904.3927v1,"A Critique of ""Solving the P/NP Problem Under Intrinsic Uncertainty"", arXiv:0811.0463","Although whether P equals NP is an important, open problem in computer science, and although Jaeger's 2008 paper, ""Solving the P/NP Problem Under Intrinsic Uncertainty"" (arXiv:0811.0463) presents an attempt at tackling the problem by discussing the possibility that all computation is uncertain to some degree, there are a number of logical oversights present in that paper which preclude it from serious consideration toward having resolved P-versus-NP. There are several differences between the model of computation presented in Jaeger's paper and the standard model, as well as several bold assumptions that are not well supported in Jaeger's paper or in the literature. In addition, we find several omissions of rigorous proof that ultimately weaken this paper to a point where it cannot be considered a candidate solution to the P-versus-NP problem.",2009-04-24T19:32:10Z,http://arxiv.org/pdf/0904.3927v1,2024-04-28,
0904.3941v1,"Representating groups on graphs","In this paper we formulate and study the problem of representing groups on graphs. We show that with respect to polynomial time turing reducibility, both abelian and solvable group representability are all equivalent to graph isomorphism, even when the group is presented as a permutation group via generators. On the other hand, the representability problem for general groups on trees is equivalent to checking, given a group $G$ and $n$, whether a nontrivial homomorphism from $G$ to $S_n$ exists. There does not seem to be a polynomial time algorithm for this problem, in spite of the fact that tree isomorphism has polynomial time algorithm.",2009-04-24T20:39:59Z,http://arxiv.org/pdf/0904.3941v1,2024-04-28,
0906.1084v1,"Physical portrayal of computational complexity","Computational complexity is examined using the principle of increasing entropy. To consider computation as a physical process from an initial instance to the final acceptance is motivated because many natural processes have been recognized to complete in non-polynomial time (NP). The irreversible process with three or more degrees of freedom is found intractable because, in terms of physics, flows of energy are inseparable from their driving forces. In computational terms, when solving problems in the class NP, decisions will affect subsequently available sets of decisions. The state space of a non-deterministic finite automaton is evolving due to the computation itself hence it cannot be efficiently contracted using a deterministic finite automaton that will arrive at a solution in super-polynomial time. The solution of the NP problem itself is verifiable in polynomial time (P) because the corresponding state is stationary. Likewise the class P set of states does not depend on computational history hence it can be efficiently contracted to the accepting state by a deterministic sequence of dissipative transformations. Thus it is concluded that the class P set of states is inherently smaller than the set of class NP. Since the computational time to contract a given set is proportional to dissipation, the computational complexity class P is a subset of NP.",2009-06-05T10:56:00Z,http://arxiv.org/pdf/0906.1084v1,2024-04-28,
0906.3119v1,"Computational Power of P Systems with Small Size Insertion and Deletion Rules","Recent investigations show insertion-deletion systems of small size that are not complete and cannot generate all recursively enumerable languages. However, if additional computational distribution mechanisms like P systems are added, then the computational completeness is achieved in some cases. In this article we take two insertion-deletion systems that are not computationally complete, consider them in the framework of P systems and show that the computational power is strictly increased by proving that any recursively enumerable language can be generated. At the end some open problems are presented.",2009-06-17T14:44:01Z,http://arxiv.org/pdf/0906.3119v1,2024-04-28,
0906.3162v1,"Are stable instances easy?","We introduce the notion of a stable instance for a discrete optimization problem, and argue that in many practical situations only sufficiently stable instances are of interest. The question then arises whether stable instances of NP--hard problems are easier to solve. In particular, whether there exist algorithms that solve correctly and in polynomial time all sufficiently stable instances of some NP--hard problem. The paper focuses on the Max--Cut problem, for which we show that this is indeed the case.",2009-06-17T12:43:41Z,http://arxiv.org/pdf/0906.3162v1,2024-04-28,
0906.3186v1,"A General Notion of Useful Information","In this paper we introduce a general framework for defining the depth of a sequence with respect to a class of observers. We show that our general framework captures all depth notions introduced in complexity theory so far. We review most such notions, show how they are particular cases of our general depth framework, and review some classical results about the different depth notions.",2009-06-17T14:09:03Z,http://arxiv.org/pdf/0906.3186v1,2024-04-28,
0906.3231v1,"New Choice for Small Universal Devices: Symport/Antiport P Systems","Symport/antiport P systems provide a very simple machinery inspired by corresponding operations in the living cell. It turns out that systems of small descriptional complexity are needed to achieve the universality by these systems. This makes them a good candidate for small universal devices replacing register machines for different simulations, especially when a simulating parallel machinery is involved. This article contains survey of these systems and presents different trade-offs between parameters.",2009-06-17T16:14:54Z,http://arxiv.org/pdf/0906.3231v1,2024-04-28,
0906.3251v1,"Limitations of Self-Assembly at Temperature One (extended abstract)","We prove that if a subset X of the integer Cartesian plane weakly self-assembles at temperature 1 in a deterministic (Winfree) tile assembly system satisfying a natural condition known as *pumpability*, then X is a finite union of doubly periodic sets. This shows that only the most simple of infinite shapes and patterns can be constructed using pumpable temperature 1 tile assembly systems, and gives strong evidence for the thesis that temperature 2 or higher is required to carry out general-purpose computation in a tile assembly system. Finally, we show that general-purpose computation is possible at temperature 1 if negative glue strengths are allowed in the tile assembly model.",2009-06-17T17:06:51Z,http://arxiv.org/pdf/0906.3251v1,2024-04-28,
0906.3306v1,"Self-Assembly of Infinite Structures","We review some recent results related to the self-assembly of infinite structures in the Tile Assembly Model. These results include impossibility results, as well as novel tile assembly systems in which shapes and patterns that represent various notions of computation self-assemble. Several open questions are also presented and motivated.",2009-06-17T20:33:07Z,http://arxiv.org/pdf/0906.3306v1,2024-04-28,
0906.3765v3,"Speedup for Natural Problems and Noncomputability","A resource-bounded version of the statement ""no algorithm recognizes all non-halting Turing machines"" is equivalent to an infinitely often (i.o.) superpolynomial speedup for the time required to accept any coNP-complete language and also equivalent to a superpolynomial speedup in proof length in propositional proof systems for tautologies, each of which implies P!=NP. This suggests a correspondence between the properties 'has no algorithm at all' and 'has no best algorithm' which seems relevant to open problems in computational and proof complexity.",2009-06-20T02:58:26Z,http://arxiv.org/pdf/0906.3765v3,2024-04-28,
0906.4431v4,"The Complexity of Probabilistic Lobbying","We propose models for lobbying in a probabilistic environment, in which an actor (called ""The Lobby"") seeks to influence voters' preferences of voting for or against multiple issues when the voters' preferences are represented in terms of probabilities. In particular, we provide two evaluation criteria and two bribery methods to formally describe these models, and we consider the resulting forms of lobbying with and without issue weighting. We provide a formal analysis for these problems of lobbying in a stochastic environment, and determine their classical and parameterized complexity depending on the given bribery/evaluation criteria and on various natural parameterizations. Specifically, we show that some of these problems can be solved in polynomial time, some are NP-complete but fixed-parameter tractable, and some are W[2]-complete. Finally, we provide approximability and inapproximability results for these problems and several variants.",2009-06-24T10:14:13Z,http://arxiv.org/pdf/0906.4431v4,2024-04-28,
0906.5112v4,"Response to Refutation of Aslam's Proof that NP = P","This paper provides a further refinement to the previous response by introducing new structures and algorithms for counting VMPs of common \emph{Edge Requirement} (ER) and hence for counting the perfect matchings.",2009-06-28T16:07:06Z,http://arxiv.org/pdf/0906.5112v4,2024-04-28,
0907.0691v1,"On the complexity of deciding whether the distinguishing chromatic number of a graph is at most two","In an article [3] published recently in this journal, it was shown that when k >= 3, the problem of deciding whether the distinguishing chromatic number of a graph is at most k is NP-hard. We consider the problem when k = 2. In regards to the issue of solvability in polynomial time, we show that the problem is at least as hard as graph automorphism but no harder than graph isomorphism.",2009-07-03T18:34:38Z,http://arxiv.org/pdf/0907.0691v1,2024-04-28,
0907.1307v1,"Reducing Tile Complexity for the Self-Assembly of Scaled Shapes Through Temperature Programming","This paper concerns the self-assembly of scaled-up versions of arbitrary finite shapes. We work in the multiple temperature model that was introduced by Aggarwal, Cheng, Goldwasser, Kao, and Schweller (Complexities for Generalized Models of Self-Assembly, SODA 2004). The multiple temperature model is a natural generalization of Winfree's abstract tile assembly model, where the temperature of a tile system is allowed to be shifted up and down as self-assembly proceeds. We first exhibit two constant-size tile sets in which scaled-up versions of arbitrary shapes self-assemble. Our first tile set has the property that each scaled shape self-assembles via an asymptotically ""Kolmogorov-optimum"" temperature sequence but the scaling factor grows with the size of the shape being assembled. In contrast, our second tile set assembles each scaled shape via a temperature sequence whose length is proportional to the number of points in the shape but the scaling factor is a constant independent of the shape being assembled. We then show that there is no constant-size tile set that can uniquely assemble an arbitrary (non-scaled, connected) shape in the multiple temperature model, i.e., the scaling is necessary for self-assembly. This answers an open question of Kao and Schweller (Reducing Tile Complexity for Self-Assembly Through Temperature Programming, SODA 2006), who asked whether such a tile set existed.",2009-07-07T21:37:11Z,http://arxiv.org/pdf/0907.1307v1,2024-04-28,
0907.2324v1,"Separations of non-monotonic randomness notions","In the theory of algorithmic randomness, several notions of random sequence are defined via a game-theoretic approach, and the notions that received most attention are perhaps Martin-Loef randomness and computable randomness. The latter notion was introduced by Schnorr and is rather natural: an infinite binary sequence is computably random if no total computable strategy succeeds on it by betting on bits in order. However, computably random sequences can have properties that one may consider to be incompatible with being random, in particular, there are computably random sequences that are highly compressible. The concept of Martin-Loef randomness is much better behaved in this and other respects, on the other hand its definition in terms of martingales is considerably less natural. Muchnik, elaborating on ideas of Kolmogorov and Loveland, refined Schnorr's model by also allowing non-monotonic strategies, i.e. strategies that do not bet on bits in order. The subsequent ``non-monotonic'' notion of randomness, now called Kolmogorov-Loveland randomness, has been shown to be quite close to Martin-Loef randomness, but whether these two classes coincide remains a fundamental open question. As suggested by Miller and Nies, we study in this paper weak versions of Kolmogorov-Loveland randomness, where the betting strategies are non-adaptive (i.e., the positions of the bits to bet on should be decided before the game). We obtain a full classification of the different notions we consider.",2009-07-14T10:41:08Z,http://arxiv.org/pdf/0907.2324v1,2024-04-28,
0907.2621v1,"Homogeneous formulas and symmetric polynomials","We investigate the arithmetic formula complexity of the elementary symmetric polynomials S(k,n). We show that every multilinear homogeneous formula computing S(k,n) has size at least k^(Omega(log k))n, and that product-depth d multilinear homogeneous formulas for S(k,n) have size at least 2^(Omega(k^{1/d}))n. Since S(n,2n) has a multilinear formula of size O(n^2), we obtain a superpolynomial separation between multilinear and multilinear homogeneous formulas. We also show that S(k,n) can be computed by homogeneous formulas of size k^(O(log k))n, answering a question of Nisan and Wigderson. Finally, we present a superpolynomial separation between monotone and non-monotone formulas in the noncommutative setting, answering a question of Nisan.",2009-07-15T18:49:20Z,http://arxiv.org/pdf/0907.2621v1,2024-04-28,
0907.3117v1,"A Survey on Continuous Time Computations","We provide an overview of theories of continuous time computation. These theories allow us to understand both the hardness of questions related to continuous time dynamical systems and the computational power of continuous time analog models. We survey the existing models, summarizing results, and point to relevant references in the literature.",2009-07-17T17:17:20Z,http://arxiv.org/pdf/0907.3117v1,2024-04-28,
0907.3780v2,"On Lower Bounds for Constant Width Arithmetic Circuits","The motivation for this paper is to study the complexity of constant-width arithmetic circuits. Our main results are the following.   1. For every k > 1, we provide an explicit polynomial that can be computed by a linear-sized monotone circuit of width 2k but has no subexponential-sized monotone circuit of width k. It follows, from the definition of the polynomial, that the constant-width and the constant-depth hierarchies of monotone arithmetic circuits are infinite, both in the commutative and the noncommutative settings.   2. We prove hardness-randomness tradeoffs for identity testing constant-width commutative circuits analogous to [KI03,DSY08].",2009-07-22T06:21:30Z,http://arxiv.org/pdf/0907.3780v2,2024-04-28,
0907.3965v80,"P != NP Proof","This paper demonstrates that P \not= NP. The way was to generalize the traditional definitions of the classes P and NP, to construct an artificial problem (a generalization to SAT: The XG-SAT, much more difficult than the former) and then to demonstrate that it is in NP but not in P (where the classes P and NP are generalized and called too simply P and NP in this paper, and then it is explained why the traditional classes P and NP should be fixed and replaced by these generalized ones into Theory of Computer Science). The demonstration consists of: 1. Definition of Restricted Type X Program; 2. Definition of the General Extended Problem of Satisfiability of a Boolean Formula - XG-SAT; 3. Generalization to classes P and NP; 4. Demonstration that the XG-SAT is in NP; 5. Demonstration that the XG-SAT is not in P; 6. Demonstration that the Baker-Gill-Solovay Theorem does not refute the proof; 7. Demonstration that the Razborov-Rudich Theorem does not refute the proof; 8. Demonstration that the Aaronson-Wigderson Theorem does not refute the proof.",2009-07-23T00:30:29Z,http://arxiv.org/pdf/0907.3965v80,2024-04-28,
0907.4006v1,"Arithmetic Circuits and the Hadamard Product of Polynomials","Motivated by the Hadamard product of matrices we define the Hadamard product of multivariate polynomials and study its arithmetic circuit and branching program complexity. We also give applications and connections to polynomial identity testing. Our main results are the following. 1. We show that noncommutative polynomial identity testing for algebraic branching programs over rationals is complete for the logspace counting class $\ceql$, and over fields of characteristic $p$ the problem is in $\ModpL/\Poly$. 2.We show an exponential lower bound for expressing the Raz-Yehudayoff polynomial as the Hadamard product of two monotone multilinear polynomials. In contrast the Permanent can be expressed as the Hadamard product of two monotone multilinear formulas of quadratic size.",2009-07-23T08:52:50Z,http://arxiv.org/pdf/0907.4006v1,2024-04-28,
0907.4775v2,"Complexity Classes of Equivalence Problems Revisited","To determine if two lists of numbers are the same set, we sort both lists and see if we get the same result. The sorted list is a canonical form for the equivalence relation of set equality. Other canonical forms arise in graph isomorphism algorithms, and the equality of permutation groups given by generators. To determine if two graphs are cospectral (have the same eigenvalues), however, we compute their characteristic polynomials and see if they are the same; the characteristic polynomial is a complete invariant for the equivalence relation of cospectrality. This is weaker than a canonical form, and it is not known whether a polynomial-time canonical form for cospectrality exists. Note that it is a priori possible for an equivalence relation to be decidable in polynomial time without either a complete invariant or canonical form.   Blass and Gurevich (SIAM J. Comput., 1984) ask whether these conditions on equivalence relations -- having an FP canonical form, having an FP complete invariant, and simply being in P -- are in fact different. They showed that this question requires non-relativizing techniques to resolve. Here we extend their results, and give new connections to probabilistic and quantum computation.",2009-07-27T21:30:02Z,http://arxiv.org/pdf/0907.4775v2,2024-04-28,
0907.5575v2,"A hitting set construction, with application to arithmetic circuit lower bounds","A polynomial identity testing algorithm must determine whether a given input polynomial is identically equal to 0. We give a deterministic black-box identity testing algorithm for univariate polynomials of the form $\sum_{j=0}^t c_j X^{\alpha_j} (a + b X)^{\beta_j}$. From our algorithm we derive an exponential lower bound for representations of polynomials such as $\prod_{i=1}^{2^n} (X^i-1)$ under this form. It has been conjectured that these polynomials are hard to compute by general arithmetic circuits. Our result shows that the ""hardness from derandomization"" approach to lower bounds is feasible for a restricted class of arithmetic circuits. The proof is based on techniques from algebraic number theory, and more precisely on properties of the height function of algebraic numbers.",2009-07-31T16:51:06Z,http://arxiv.org/pdf/0907.5575v2,2024-04-28,
0908.1159v2,"On the Running Time of the Shortest Programs","The Kolmogorov complexity of the word w is equal to the length of the shortest concatenation of program Z and its input x with which the word w is computed by the universal turing machine U. The question introduced in this paper is the following: How long do the shortest programs run for?",2009-08-10T14:07:47Z,http://arxiv.org/pdf/0908.1159v2,2024-04-28,
0908.1397v3,"Matrix P-norms are NP-hard to approximate if p \neq 1,2,\infty","We show that for any rational p \in [1,\infty) except p = 1, 2, unless P = NP, there is no polynomial-time algorithm for approximating the matrix p-norm to arbitrary relative precision. We also show that for any rational p\in [1,\infty) including p = 1, 2, unless P = NP, there is no polynomial-time algorithm approximates the \infty, p mixed norm to some fixed relative precision.",2009-08-10T20:22:46Z,http://arxiv.org/pdf/0908.1397v3,2024-04-28,
0908.1932v2,"On P vs. NP, Geometric Complexity Theory, Explicit Proofs and the Complexity Barrier","Geometric complexity theory (GCT) is an approach to the P vs. NP and related problems. This article gives its complexity theoretic overview without assuming any background in algebraic geometry or representation theory.",2009-08-13T18:31:54Z,http://arxiv.org/pdf/0908.1932v2,2024-04-28,
0908.1936v2,"On P vs. NP, Geometric Complexity Theory, and the Riemann Hypothesis","Geometric complexity theory (GCT) is an approach to the $P$ vs. $NP$ and related problems. A high level overview of this research plan and the results obtained so far was presented in a series of three lectures in the Institute of Advanced study, Princeton, Feb 9-11, 2009. This article contains the material covered in those lectures after some revision, and gives a mathematical overview of GCT. No background in algebraic geometry, representation theory or quantum groups is assumed.",2009-08-13T18:05:07Z,http://arxiv.org/pdf/0908.1936v2,2024-04-28,
0908.2122v1,"Approximate Counting and Quantum Computation","Motivated by the result that an `approximate' evaluation of the Jones polynomial of a braid at a $5^{th}$ root of unity can be used to simulate the quantum part of any algorithm in the quantum complexity class BQP, and results relating BQP to the counting class GapP, we introduce a form of additive approximation which can be used to simulate a function in BQP. We show that all functions in the classes #P and GapP have such an approximation scheme under certain natural normalisations. However we are unable to determine whether the particular functions we are motivated by, such as the above evaluation of the Jones polynomial, can be approximated in this way. We close with some open problems motivated by this work.",2009-08-14T19:29:07Z,http://arxiv.org/pdf/0908.2122v1,2024-04-28,
0908.2476v1,"Concurrent Knowledge-Extraction in the Public-Key Model","Knowledge extraction is a fundamental notion, modelling machine possession of values (witnesses) in a computational complexity sense. The notion provides an essential tool for cryptographic protocol design and analysis, enabling one to argue about the internal state of protocol players without ever looking at this supposedly secret state. However, when transactions are concurrent (e.g., over the Internet) with players possessing public-keys (as is common in cryptography), assuring that entities ``know'' what they claim to know, where adversaries may be well coordinated across different transactions, turns out to be much more subtle and in need of re-examination. Here, we investigate how to formally treat knowledge possession by parties (with registered public-keys) interacting over the Internet. Stated more technically, we look into the relative power of the notion of ``concurrent knowledge-extraction'' (CKE) in the concurrent zero-knowledge (CZK) bare public-key (BPK) model.",2009-08-18T01:43:21Z,http://arxiv.org/pdf/0908.2476v1,2024-04-28,
0908.2940v3,"A Strong Direct Product Theorem for Disjointness","A strong direct product theorem states that if we want to compute $k$ independent instances of a function, using less than $k$ times the resources needed for one instance, then the overall success probability will be exponentially small in $k$. We establish such a theorem for the randomized communication complexity of the Disjointness problem, i.e., with communication $const\cdot kn$ the success probability of solving $k$ instances of size $n$ can only be exponentially small in $k$. We show that this bound even holds for $AM$ communication protocols with limited ambiguity. This also implies a new lower bound for Disjointness in a restricted 3-player NOF protocol, and optimal communication-space tradeoffs for Boolean matrix product. Our main result follows from a solution to the dual of a linear programming problem, whose feasibility comes from a so-called Intersection Sampling Lemma that generalizes a result by Razborov.",2009-08-20T14:59:24Z,http://arxiv.org/pdf/0908.2940v3,2024-04-28,
0908.4013v3,"Recombinations of Busy Beaver Machines","Many programmers belive that Turing-based machines cannot think. We also believe in this, however it is interesting to note that the most sophisticated machines are not programmed by human beings. We have only discovered them. In this paper, using well-known Busy Beaver and Placid Platypus machines, we generate further very similar, but not exactly the same machines. We have found a recombinated BB_5 machine which can make 70.740.809 steps before halting.",2009-08-27T13:59:27Z,http://arxiv.org/pdf/0908.4013v3,2024-04-28,
0909.3466v3,"Résolution du ""partition problem"" par une approche arithmétique","This article has been withdrawn",2009-09-18T15:25:32Z,http://arxiv.org/pdf/0909.3466v3,2024-04-28,
0909.3868v2,"Method of resolution of 3SAT in polynomial time","Presentation of a Method for determining whether a problem 3Sat has solution, and if yes to find one, in time max O(n^15). Is thus proved that the problem 3Sat is fully resolved in polynomial time and therefore that it is in P, by the work of Cook and Levin, and can transform a SAT problem in a 3Sat in polynomial time (ref. Karp), it follows that P = NP. Open Source program is available at http://www.visainformatica.it/3sat",2009-09-21T21:45:10Z,http://arxiv.org/pdf/0909.3868v2,2024-04-28,
0909.4607v1,"A note on the sign degree of formulas","Recent breakthroughs in quantum query complexity have shown that any formula of size n can be evaluated with O(sqrt(n)log(n)/log log(n)) many quantum queries in the bounded-error setting [FGG08, ACRSZ07, RS08b, Rei09]. In particular, this gives an upper bound on the approximate polynomial degree of formulas of the same magnitude, as approximate polynomial degree is a lower bound on quantum query complexity [BBCMW01].   These results essentially answer in the affirmative a conjecture of O'Donnell and Servedio [O'DS03] that the sign degree--the minimal degree of a polynomial that agrees in sign with a function on the Boolean cube--of every formula of size n is O(sqrt(n)).   In this note, we show that sign degree is super-multiplicative under function composition. Combining this result with the above mentioned upper bounds on the quantum query complexity of formulas allows the removal of logarithmic factors to show that the sign degree of every size n formula is at most sqrt(n).",2009-09-25T05:26:07Z,http://arxiv.org/pdf/0909.4607v1,2024-04-28,
0909.5479v1,"Proceedings Fourth Athens Colloquium on Algorithms and Complexity","ACAC 2009 is organized by the Athens University of Economics and Business (AUEB) and it is the fourth in a series of meetings that aim to bring together researchers working on all areas of the theory of algorithms and computational complexity. These meetings are expected to serve as a lively forum for presenting results that are in a preliminary stage or have been recently presented in some major conference. For the first time this year all submitted papers were reviewed and ACAC also offered to the authors the choice of publishing their contribution (provided it has not been published anywhere else before) with the post-proceedings of EPTCS (Electronic Proceedings in Theoretical Computer Science).",2009-09-30T02:05:28Z,http://arxiv.org/pdf/0909.5479v1,2024-04-28,
0909.5684v1,"Partition Arguments in Multiparty Communication Complexity","Consider the ""Number in Hand"" multiparty communication complexity model, where k players holding inputs x_1,...,x_k in {0,1}^n communicate to compute the value f(x_1,...,x_k) of a function f known to all of them. The main lower bound technique for the communication complexity of such problems is that of partition arguments: partition the k players into two disjoint sets of players and find a lower bound for the induced two-party communication complexity problem.   In this paper, we study the power of partition arguments. Our two main results are very different in nature: (i) For randomized communication complexity, we show that partition arguments may yield bounds that are exponentially far from the true communication complexity. Specifically, we prove that there exists a 3-argument function f whose communication complexity is Omega(n), while partition arguments can only yield an Omega(log n) lower bound. The same holds for nondeterministic communication complexity. (ii) For deterministic communication complexity, we prove that finding significant gaps between the true communication complexity and the best lower bound that can be obtained via partition arguments, would imply progress on a generalized version of the ""log-rank conjecture"" in communication complexity.   We conclude with two results on the multiparty ""fooling set technique"", another method for obtaining communication complexity lower bounds.",2009-09-30T18:18:04Z,http://arxiv.org/pdf/0909.5684v1,2024-04-28,
0910.1427v1,"Balancing Bounded Treewidth Circuits","Algorithmic tools for graphs of small treewidth are used to address questions in complexity theory. For both arithmetic and Boolean circuits, it is shown that any circuit of size $n^{O(1)}$ and treewidth $O(\log^i n)$ can be simulated by a circuit of width $O(\log^{i+1} n)$ and size $n^c$, where $c = O(1)$, if $i=0$, and $c=O(\log \log n)$ otherwise. For our main construction, we prove that multiplicatively disjoint arithmetic circuits of size $n^{O(1)}$ and treewidth $k$ can be simulated by bounded fan-in arithmetic formulas of depth $O(k^2\log n)$. From this we derive the analogous statement for syntactically multilinear arithmetic circuits, which strengthens a theorem of Mahajan and Rao. As another application, we derive that constant width arithmetic circuits of size $n^{O(1)}$ can be balanced to depth $O(\log n)$, provided certain restrictions are made on the use of iterated multiplication. Also from our main construction, we derive that Boolean bounded fan-in circuits of size $n^{O(1)}$ and treewidth $k$ can be simulated by bounded fan-in formulas of depth $O(k^2\log n)$. This strengthens in the non-uniform setting the known inclusion that $SC^0 \subseteq NC^1$. Finally, we apply our construction to show that {\sc reachability} for directed graphs of bounded treewidth is in $LogDCFL$.",2009-10-08T06:56:50Z,http://arxiv.org/pdf/0910.1427v1,2024-04-28,
0910.1443v1,"Weakening Assumptions for Deterministic Subexponential Time Non-Singular Matrix Completion","In (Kabanets, Impagliazzo, 2004) it is shown how to decide the circuit polynomial identity testing problem (CPIT) in deterministic subexponential time, assuming hardness of some explicit multilinear polynomial family for arithmetical circuits. In this paper, a special case of CPIT is considered, namely low-degree non-singular matrix completion (NSMC). For this subclass of problems it is shown how to obtain the same deterministic time bound, using a weaker assumption in terms of determinantal complexity.   Hardness-randomness tradeoffs will also be shown in the converse direction, in an effort to make progress on Valiant's VP versus VNP problem. To separate VP and VNP, it is known to be sufficient to prove that the determinantal complexity of the m-by-m permanent is $m^{\omega(\log m)}$. In this paper it is shown, for an appropriate notion of explicitness, that the existence of an explicit multilinear polynomial family with determinantal complexity m^{\omega(\log m)}$ is equivalent to the existence of an efficiently computable generator $G_n$ for multilinear NSMC with seed length $O(n^{1/\sqrt{\log n}})$. The latter is a combinatorial object that provides an efficient deterministic black-box algorithm for NSMC. ``Multilinear NSMC'' indicates that $G_n$ only has to work for matrices $M(x)$ of $poly(n)$ size in $n$ variables, for which $det(M(x))$ is a multilinear polynomial.",2009-10-08T08:44:29Z,http://arxiv.org/pdf/0910.1443v1,2024-04-28,
0910.1862v1,"The intersection of two halfspaces has high threshold degree","The threshold degree of a Boolean function f:{0,1}^n->{-1,+1} is the least degree of a real polynomial p such that f(x)=sgn p(x). We construct two halfspaces on {0,1}^n whose intersection has threshold degree Theta(sqrt n), an exponential improvement on previous lower bounds. This solves an open problem due to Klivans (2002) and rules out the use of perceptron-based techniques for PAC learning the intersection of two halfspaces, a central unresolved challenge in computational learning. We also prove that the intersection of two majority functions has threshold degree Omega(log n), which is tight and settles a conjecture of O'Donnell and Servedio (2003).   Our proof consists of two parts. First, we show that for any nonconstant Boolean functions f and g, the intersection f(x)^g(y) has threshold degree O(d) if and only if ||f-F||_infty + ||g-G||_infty < 1 for some rational functions F, G of degree O(d). Second, we settle the least degree required for approximating a halfspace and a majority function to any given accuracy by rational functions.   Our technique further allows us to make progress on Aaronson's challenge (2008) and contribute strong direct product theorems for polynomial representations of composed Boolean functions of the form F(f_1,...,f_n). In particular, we give an improved lower bound on the approximate degree of the AND-OR tree.",2009-10-12T16:14:48Z,http://arxiv.org/pdf/0910.1862v1,2024-04-28,
0910.2271v3,"Improved Inapproximability Results for Maximum k-Colorable Subgraph","We study the maximization version of the fundamental graph coloring problem. Here the goal is to color the vertices of a k-colorable graph with k colors so that a maximum fraction of edges are properly colored (i.e. their endpoints receive different colors). A random k-coloring properly colors an expected fraction 1-1/k of edges. We prove that given a graph promised to be k-colorable, it is NP-hard to find a k-coloring that properly colors more than a fraction ~1-O(1/k} of edges. Previously, only a hardness factor of 1-O(1/k^2) was known. Our result pins down the correct asymptotic dependence of the approximation factor on k. Along the way, we prove that approximating the Maximum 3-colorable subgraph problem within a factor greater than 32/33 is NP-hard. Using semidefinite programming, it is known that one can do better than a random coloring and properly color a fraction 1-1/k +2 ln k/k^2 of edges in polynomial time. We show that, assuming the 2-to-1 conjecture, it is hard to properly color (using k colors) more than a fraction 1-1/k + O(ln k/ k^2) of edges of a k-colorable graph.",2009-10-12T23:49:08Z,http://arxiv.org/pdf/0910.2271v3,2024-04-28,
0910.2649v1,"Polynomially Correlated Knapsack is NP-complete","0-1 Knapsack is a fundamental NP-complete problem. In this article we prove that it remains NP-complete even when the weights of the objects in the packing constraints and their values in the objective function satisfy specific stringent conditions: the values are integral powers of the weights of the objects.",2009-10-14T15:43:40Z,http://arxiv.org/pdf/0910.2649v1,2024-04-28,
0910.3282v1,"Adaptive Concurrent Non-Malleability with Bare Public-Keys","Concurrent non-malleability (CNM) is central for cryptographic protocols running concurrently in environments such as the Internet. In this work, we formulate CNM in the bare public-key (BPK) model, and show that round-efficient concurrent non-malleable cryptography with full adaptive input selection can be established, in general, with bare public-keys (where, in particular, no trusted assumption is made). Along the way, we clarify the various subtleties of adaptive concurrent non-malleability in the bare public-key model.",2009-10-17T07:28:50Z,http://arxiv.org/pdf/0910.3282v1,2024-04-28,
0910.3719v1,"Improved Approximation of Linear Threshold Functions","We prove two main results on how arbitrary linear threshold functions $f(x) = \sign(w\cdot x - \theta)$ over the $n$-dimensional Boolean hypercube can be approximated by simple threshold functions.   Our first result shows that every $n$-variable threshold function $f$ is $\eps$-close to a threshold function depending only on $\Inf(f)^2 \cdot \poly(1/\eps)$ many variables, where $\Inf(f)$ denotes the total influence or average sensitivity of $f.$ This is an exponential sharpening of Friedgut's well-known theorem \cite{Friedgut:98}, which states that every Boolean function $f$ is $\eps$-close to a function depending only on $2^{O(\Inf(f)/\eps)}$ many variables, for the case of threshold functions. We complement this upper bound by showing that $\Omega(\Inf(f)^2 + 1/\epsilon^2)$ many variables are required for $\epsilon$-approximating threshold functions.   Our second result is a proof that every $n$-variable threshold function is $\eps$-close to a threshold function with integer weights at most $\poly(n) \cdot 2^{\tilde{O}(1/\eps^{2/3})}.$ This is a significant improvement, in the dependence on the error parameter $\eps$, on an earlier result of \cite{Servedio:07cc} which gave a $\poly(n) \cdot 2^{\tilde{O}(1/\eps^{2})}$ bound. Our improvement is obtained via a new proof technique that uses strong anti-concentration bounds from probability theory. The new technique also gives a simple and modular proof of the original \cite{Servedio:07cc} result, and extends to give low-weight approximators for threshold functions under a range of probability distributions beyond just the uniform distribution.",2009-10-19T23:11:46Z,http://arxiv.org/pdf/0910.3719v1,2024-04-28,
0910.4122v5,"Pseudorandom Generators for Polynomial Threshold Functions","We study the natural question of constructing pseudorandom generators (PRGs) for low-degree polynomial threshold functions (PTFs). We give a PRG with seed-length log n/eps^{O(d)} fooling degree d PTFs with error at most eps. Previously, no nontrivial constructions were known even for quadratic threshold functions and constant error eps. For the class of degree 1 threshold functions or halfspaces, we construct PRGs with much better dependence on the error parameter eps and obtain a PRG with seed-length O(log n + log^2(1/eps)). Previously, only PRGs with seed length O(log n log^2(1/eps)/eps^2) were known for halfspaces. We also obtain PRGs with similar seed lengths for fooling halfspaces over the n-dimensional unit sphere.   The main theme of our constructions and analysis is the use of invariance principles to construct pseudorandom generators. We also introduce the notion of monotone read-once branching programs, which is key to improving the dependence on the error rate eps for halfspaces. These techniques may be of independent interest.",2009-10-21T15:48:00Z,http://arxiv.org/pdf/0910.4122v5,2024-04-28,
0910.4224v2,"Optimal bounds for sign-representing the intersection of two halfspaces by polynomials","The threshold degree of a function f:{0,1}^n->{-1,+1} is the least degree of a real polynomial p with f(x)=sgn p(x). We prove that the intersection of two halfspaces on {0,1}^n has threshold degree Omega(n), which matches the trivial upper bound and completely answers a question due to Klivans (2002). The best previous lower bound was Omega(sqrt n). Our result shows that the intersection of two halfspaces on {0,1}^n only admits a trivial 2^{Theta(n)}-time learning algorithm based on sign-representation by polynomials, unlike the advances achieved in PAC learning DNF formulas and read-once Boolean formulas. The proof introduces a new technique of independent interest, based on Fourier analysis and matrix theory.",2009-10-22T04:05:31Z,http://arxiv.org/pdf/0910.4224v2,2024-04-28,
0910.4266v2,"The Partition Bound for Classical Communication Complexity and Query Complexity","We describe new lower bounds for randomized communication complexity and query complexity which we call the partition bounds. They are expressed as the optimum value of linear programs. For communication complexity we show that the partition bound is stronger than both the rectangle/corruption bound and the \gamma_2/generalized discrepancy bounds. In the model of query complexity we show that the partition bound is stronger than the approximate polynomial degree and classical adversary bounds. We also exhibit an example where the partition bound is quadratically larger than polynomial degree and classical adversary bounds.",2009-10-22T09:40:58Z,http://arxiv.org/pdf/0910.4266v2,2024-04-28,
0910.4518v1,"Preprocessing of Min Ones Problems: A Dichotomy","A parameterized problem consists of a classical problem and an additional component, the so-called parameter. This point of view allows a formal definition of preprocessing: Given a parameterized instance (I,k), a polynomial kernelization computes an equivalent instance (I',k') of size and parameter bounded by a polynomial in k. We give a complete classification of Min Ones Constraint Satisfaction problems, i.e., Min Ones SAT(\Gamma), with respect to admitting or not admitting a polynomial kernelization (unless NP \subseteq coNP/poly). For this we introduce the notion of mergeability. If all relations of the constraint language \Gamma are mergeable, then a new variant of sunflower kernelization applies, based on non-zero-closed cores. We obtain a kernel with O(k^{d+1}) variables and polynomial total size, where d is the maximum arity of a constraint in \Gamma, comparing nicely with the bound of O(k^{d-1}) vertices for the less general and arguably simpler d-Hitting Set problem. Otherwise, any relation in \Gamma that is not mergeable permits us to construct a log-cost selection formula, i.e., an n-ary selection formula with O(log n) true local variables. From this we can construct our lower bound using recent results by Bodlaender et al. as well as Fortnow and Santhanam, proving that there is no polynomial kernelization, unless NP \subseteq coNP/poly and the polynomial hierarchy collapses to the third level.",2009-10-23T14:14:37Z,http://arxiv.org/pdf/0910.4518v1,2024-04-28,
0911.0664v7,"Bounds on monotone switching networks for directed connectivity","We separate monotone analogues of L and NL by proving that any monotone switching network solving directed connectivity on $n$ vertices must have size at least $n^(\Omega(\lg(n)))$.",2009-11-03T20:10:05Z,http://arxiv.org/pdf/0911.0664v7,2024-04-28,
0911.2325v1,"Characterizing Polynomial Time Computability of Rational and Real Functions","Recursive analysis was introduced by A. Turing [1936], A. Grzegorczyk [1955], and D. Lacombe [1955]. It is based on a discrete mechanical framework that can be used to model computation over the real numbers. In this context the computational complexity of real functions defined over compact domains has been extensively studied. However, much less have been done for other kinds of real functions. This article is divided into two main parts. The first part investigates polynomial time computability of rational functions and the role of continuity in such computation. On the one hand this is interesting for its own sake. On the other hand it provides insights into polynomial time computability of real functions for the latter, in the sense of recursive analysis, is modeled as approximations of rational computations. The main conclusion of this part is that continuity does not play any role in the efficiency of computing rational functions. The second part defines polynomial time computability of arbitrary real functions, characterizes it, and compares it with the corresponding notion over rational functions. Assuming continuity, the main conclusion is that there is a conceptual difference between polynomial time computation over the rationals and the reals manifested by the fact that there are polynomial time computable rational functions whose extensions to the reals are not polynomial time computable and vice versa.",2009-11-12T08:50:28Z,http://arxiv.org/pdf/0911.2325v1,2024-04-28,
0911.3389v2,"Bounded Independence Fools Degree-2 Threshold Functions","Let x be a random vector coming from any k-wise independent distribution over {-1,1}^n. For an n-variate degree-2 polynomial p, we prove that E[sgn(p(x))] is determined up to an additive epsilon for k = poly(1/epsilon). This answers an open question of Diakonikolas et al. (FOCS 2009). Using standard constructions of k-wise independent distributions, we obtain a broad class of explicit generators that epsilon-fool the class of degree-2 threshold functions with seed length log(n)*poly(1/epsilon).   Our approach is quite robust: it easily extends to yield that the intersection of any constant number of degree-2 threshold functions is epsilon-fooled by poly(1/epsilon)-wise independence. Our results also hold if the entries of x are k-wise independent standard normals, implying for example that bounded independence derandomizes the Goemans-Williamson hyperplane rounding scheme.   To achieve our results, we introduce a technique we dub multivariate FT-mollification, a generalization of the univariate form introduced by Kane et al. (SODA 2010) in the context of streaming algorithms. Along the way we prove a generalized hypercontractive inequality for quadratic forms which takes the operator norm of the associated matrix into account. These techniques may be of independent interest.",2009-11-17T20:24:27Z,http://arxiv.org/pdf/0911.3389v2,2024-04-28,
0911.3473v3,"Polynomial Threshold Functions: Structure, Approximation and Pseudorandomness","We study the computational power of polynomial threshold functions, that is, threshold functions of real polynomials over the boolean cube. We provide two new results bounding the computational power of this model.   Our first result shows that low-degree polynomial threshold functions cannot approximate any function with many influential variables. We provide a couple of examples where this technique yields tight approximation bounds.   Our second result relates to constructing pseudorandom generators fooling low-degree polynomial threshold functions. This problem has received attention recently, where Diakonikolas et al proved that $k$-wise independence suffices to fool linear threshold functions. We prove that any low-degree polynomial threshold function, which can be represented as a function of a small number of linear threshold functions, can also be fooled by $k$-wise independence. We view this as an important step towards fooling general polynomial threshold functions, and we discuss a plausible approach achieving this goal based on our techniques.   Our results combine tools from real approximation theory, hyper-contractive inequalities and probabilistic methods. In particular, we develop several new tools in approximation theory which may be of independent interest.",2009-11-18T07:28:08Z,http://arxiv.org/pdf/0911.3473v3,2024-04-28,
0911.3492v2,"Towards a Dichotomy for the Possible Winner Problem in Elections Based on Scoring Rules","To make a joint decision, agents (or voters) are often required to provide their preferences as linear orders. To determine a winner, the given linear orders can be aggregated according to a voting protocol. However, in realistic settings, the voters may often only provide partial orders. This directly leads to the Possible Winner problem that asks, given a set of partial votes, whether a distinguished candidate can still become a winner. In this work, we consider the computational complexity of Possible Winner for the broad class of voting protocols defined by scoring rules. A scoring rule provides a score value for every position which a candidate can have in a linear order. Prominent examples include plurality, k-approval, and Borda. Generalizing previous NP-hardness results for some special cases, we settle the computational complexity for all but one scoring rule. More precisely, for an unbounded number of candidates and unweighted voters, we show that Possible Winner is NP-complete for all pure scoring rules except plurality, veto, and the scoring rule defined by the scoring vector (2,1,...,1,0), while it is solvable in polynomial time for plurality and veto.",2009-11-18T10:02:39Z,http://arxiv.org/pdf/0911.3492v2,2024-04-28,
0911.4337v1,"Circuit Lower Bounds, Help Functions, and the Remote Point Problem","We investigate the power of Algebraic Branching Programs (ABPs) augmented with help polynomials, and constant-depth Boolean circuits augmented with help functions. We relate the problem of proving explicit lower bounds in both these models to the Remote Point Problem (introduced by Alon, Panigrahy, and Yekhanin (RANDOM '09)). More precisely, proving lower bounds for ABPs with help polynomials is related to the Remote Point Problem w.r.t. the rank metric, and for constant-depth circuits with help functions it is related to the Remote Point Problem w.r.t. the Hamming metric. For algebraic branching programs with help polynomials with some degree restrictions we show exponential size lower bounds for explicit polynomials.",2009-11-23T08:25:40Z,http://arxiv.org/pdf/0911.4337v1,2024-04-28,
0912.0309v2,"Hardness Results for the Gapped Consecutive-Ones Property","Motivated by problems of comparative genomics and paleogenomics, in [Chauve et al., 2009], the authors introduced the Gapped Consecutive-Ones Property Problem (k,delta)-C1P: given a binary matrix M and two integers k and delta, can the columns of M be permuted such that each row contains at most k blocks of ones and no two consecutive blocks of ones are separated by a gap of more than delta zeros. The classical C1P problem, which is known to be polynomial is equivalent to the (1,0)-C1P problem. They showed that the (2,delta)-C1P Problem is NP-complete for all delta >= 2 and that the (3,1)-C1P problem is NP-complete. They also conjectured that the (k,delta)-C1P Problem is NP-complete for k >= 2, delta >= 1 and (k,delta) =/= (2,1). Here, we prove that this conjecture is true. The only remaining case is the (2,1)-C1P Problem, which could be polynomial-time solvable.",2009-12-02T00:26:17Z,http://arxiv.org/pdf/0912.0309v2,2024-04-28,
0912.0568v1,"Hardness Amplification in Proof Complexity","We present a general method for converting any family of unsatisfiable CNF formulas that is hard for one of the simplest proof systems, tree resolution, into formulas that require large rank in any proof system that manipulates polynomials or polynomial threshold functions of degree at most k (known as Th(k) proofs). Such systems include Lovasz-Schrijver and Cutting Planes proof systems as well as their high degree analogues.   These are based on analyzing two new proof systems, denoted by T^cc(k) and R^cc(k). The proof lines of T^cc(k) are arbitrary Boolean functions, each of which can be evaluated by an efficient k-party randomized communication protocol. They include Th{k-1} proofs as a special case. R^cc(k) proofs are stronger and only require that each inference be locally checkable by an efficient k-party randomized communication protocol.   Our main results are the following:   (1) When k is O(loglogn), for any unsatisfiable CNF formula F requiring resolution rank r, there is a related CNF formula G=Lift_k(F) requiring refutation rank r^Omega(1/k) log^O(1) n in all R^cc(k) systems.   (2) There are strict hierarchies for T^cc(k) and R^cc(k) systems with respect to k when k is O(loglogn in that there are unsatisfiable CNF formulas requiring large rank R^cc(k) refutations but having log^O(1) n rank Th(k) refutations.   (3) When k is O(loglogn) there are 2^(n^Omega(1/k)) lower bounds on the size of tree-like T^cc(k) refutations for large classes of lifted CNF formulas.   (4) A general method for producing integrality gaps for low rank R^cc(2) inference (and hence Cutting Planes and Th(1) inference) based on related gaps for low rank resolution. These gaps are optimal for MAX-2t-SAT.",2009-12-03T02:09:22Z,http://arxiv.org/pdf/0912.0568v1,2024-04-28,
0912.0741v2,"A boundary between universality and non-universality in spiking neural P systems","In this work we offer a significant improvement on the previous smallest spiking neural P systems and solve the problem of finding the smallest possible extended spiking neural P system. Paun and Paun gave a universal spiking neural P system with 84 neurons and another that has extended rules with 49 neurons. Subsequently, Zhang et al. reduced the number of neurons used to give universality to 67 for spiking neural P systems and to 41 for the extended model. Here we give a small universal spiking neural P system that has only 17 neurons and another that has extended rules with 5 neurons. All of the above mentioned spiking neural P systems suffer from an exponential slow down when simulating Turing machines. Using a more relaxed encoding technique we get a universal spiking neural P system that has extended rules with only 4 neurons. This latter spiking neural P system simulates 2-counter machines in linear time and thus suffer from a double exponential time overhead when simulating Turing machines. We show that extended spiking neural P systems with 3 neurons are simulated by log-space bounded Turing machines, and so there exists no such universal system with 3 neurons. It immediately follows that our 4-neuron system is the smallest possible extended spiking neural P system that is universal. Finally, we show that if we generalise the output technique we can give a universal spiking neural P system with extended rules that has only 3 neurons. This system is also the smallest of its kind as a universal spiking neural P system with extended rules and generalised output is not possible with 2 neurons.",2009-12-04T20:36:55Z,http://arxiv.org/pdf/0912.0741v2,2024-04-28,
0912.1776v1,"On the Optimality of a Class of LP-based Algorithms","In this paper we will be concerned with a class of packing and covering problems which includes Vertex Cover and Independent Set. Typically, one can write an LP relaxation and then round the solution. In this paper, we explain why the simple LP-based rounding algorithm for the \\VC problem is optimal assuming the UGC. Complementing Raghavendra's result, our result generalizes to a class of strict, covering/packing type CSPs.",2009-12-09T15:49:04Z,http://arxiv.org/pdf/0912.1776v1,2024-04-28,
0912.2565v1,"Deterministic Identity Testing of Read-Once Algebraic Branching Programs","In this paper we study polynomial identity testing of sums of $k$ read-once algebraic branching programs ($\Sigma_k$-RO-ABPs), generalizing the work in (Shpilka and Volkovich 2008,2009), who considered sums of $k$ read-once formulas ($\Sigma_k$-RO-formulas). We show that $\Sigma_k$-RO-ABPs are strictly more powerful than $\Sigma_k$-RO-formulas, for any $k \leq \lfloor n/2\rfloor$, where $n$ is the number of variables. We obtain the following results:   1) Given free access to the RO-ABPs in the sum, we get a deterministic algorithm that runs in time $O(k^2n^7s) + n^{O(k)}$, where $s$ bounds the size of any largest RO-ABP given on the input. This implies we have a deterministic polynomial time algorithm for testing whether the sum of a constant number of RO-ABPs computes the zero polynomial.   2) Given black-box access to the RO-ABPs computing the individual polynomials in the sum, we get a deterministic algorithm that runs in time $k^2n^{O(\log n)} + n^{O(k)}$.   3) Finally, given only black-box access to the polynomial computed by the sum of the $k$ RO-ABPs, we obtain an $n^{O(k + \log n)}$ time deterministic algorithm.",2009-12-14T02:10:50Z,http://arxiv.org/pdf/0912.2565v1,2024-04-28,
0912.2607v3,"The Multivariate Resultant is NP-hard in any Characteristic","The multivariate resultant is a fundamental tool of computational algebraic geometry. It can in particular be used to decide whether a system of n homogeneous equations in n variables is satisfiable (the resultant is a polynomial in the system's coefficients which vanishes if and only if the system is satisfiable). In this paper we present several NP-hardness results for testing whether a multivariate resultant vanishes, or equivalently for deciding whether a square system of homogeneous equations is satisfiable. Our main result is that testing the resultant for zero is NP-hard under deterministic reductions in any characteristic, for systems of low-degree polynomials with coefficients in the ground field (rather than in an extension). We also observe that in characteristic zero, this problem is in the Arthur-Merlin class AM if the generalized Riemann hypothesis holds true. In positive characteristic, the best upper bound remains PSPACE.",2009-12-14T10:30:25Z,http://arxiv.org/pdf/0912.2607v3,2024-04-28,
0912.3162v1,"Derandomizing from Random Strings","In this paper we show that BPP is truth-table reducible to the set of Kolmogorov random strings R_K. It was previously known that PSPACE, and hence BPP is Turing-reducible to R_K. The earlier proof relied on the adaptivity of the Turing-reduction to find a Kolmogorov-random string of polynomial length using the set R_K as oracle. Our new non-adaptive result relies on a new fundamental fact about the set R_K, namely each initial segment of the characteristic sequence of R_K is not compressible by recursive means. As a partial converse to our claim we show that strings of high Kolmogorov-complexity when used as advice are not much more useful than randomly chosen strings.",2009-12-16T15:15:20Z,http://arxiv.org/pdf/0912.3162v1,2024-04-28,
0912.3627v1,"New Learning and Testing Problems for Read-Once Functions","In the paper, we consider several types of queries for classical and new problems of learning and testing read-once functions. In several cases, the border between polynomial and exponential complexities is obtained.",2009-12-18T09:58:55Z,http://arxiv.org/pdf/0912.3627v1,2024-04-28,
0912.3730v2,"On the circuit-size of inverses","We reprove a result of Boppana and Lagarias: If Pi_2^P is different from Sigma_2^P then there exists a partial function f that is computable by a polynomial-size family of circuits, but no inverse of f is computable by a polynomial-size family of circuits. We strengthen this result by showing that there exist length-preserving total functions that are one-way by circuit size and that are computable in uniform polynomial time. We also prove, if Pi_2^P is different from Sigma_2^P, that there exist polynomially balanced total surjective functions that are one-way by circuit size; here non-uniformity is used.",2009-12-18T16:34:15Z,http://arxiv.org/pdf/0912.3730v2,2024-04-28,
0912.3802v3,"The complexity of the list homomorphism problem for graphs","We completely classify the computational complexity of the list H-colouring problem for graphs (with possible loops) in combinatorial and algebraic terms: for every graph H the problem is either NP-complete, NL-complete, L-complete or is first-order definable; descriptive complexity equivalents are given as well via Datalog and its fragments. Our algebraic characterisations match important conjectures in the study of constraint satisfaction problems.",2009-12-18T21:14:52Z,http://arxiv.org/pdf/0912.3802v3,2024-04-28,
0912.4602v2,"Log-space Algorithms for Paths and Matchings in k-trees","Reachability and shortest path problems are NL-complete for general graphs. They are known to be in L for graphs of tree-width 2 [JT07]. However, for graphs of tree-width larger than 2, no bound better than NL is known. In this paper, we improve these bounds for k-trees, where k is a constant. In particular, the main results of our paper are log-space algorithms for reachability in directed k-trees, and for computation of shortest and longest paths in directed acyclic k-trees.   Besides the path problems mentioned above, we also consider the problem of deciding whether a k-tree has a perfect macthing (decision version), and if so, finding a perfect match- ing (search version), and prove that these two problems are L-complete. These problems are known to be in P and in RNC for general graphs, and in SPL for planar bipartite graphs [DKR08].   Our results settle the complexity of these problems for the class of k-trees. The results are also applicable for bounded tree-width graphs, when a tree-decomposition is given as input. The technique central to our algorithms is a careful implementation of divide-and-conquer approach in log-space, along with some ideas from [JT07] and [LMR07].",2009-12-23T10:58:08Z,http://arxiv.org/pdf/0912.4602v2,2024-04-28,
0912.4935v4,"Inapproximability of maximal strip recovery","In comparative genomic, the first step of sequence analysis is usually to decompose two or more genomes into syntenic blocks that are segments of homologous chromosomes. For the reliable recovery of syntenic blocks, noise and ambiguities in the genomic maps need to be removed first. Maximal Strip Recovery (MSR) is an optimization problem proposed by Zheng, Zhu, and Sankoff for reliably recovering syntenic blocks from genomic maps in the midst of noise and ambiguities. Given $d$ genomic maps as sequences of gene markers, the objective of \msr{d} is to find $d$ subsequences, one subsequence of each genomic map, such that the total length of syntenic blocks in these subsequences is maximized. For any constant $d \ge 2$, a polynomial-time 2d-approximation for \msr{d} was previously known. In this paper, we show that for any $d \ge 2$, \msr{d} is APX-hard, even for the most basic version of the problem in which all gene markers are distinct and appear in positive orientation in each genomic map. Moreover, we provide the first explicit lower bounds on approximating \msr{d} for all $d \ge 2$. In particular, we show that \msr{d} is NP-hard to approximate within $\Omega(d/\log d)$. From the other direction, we show that the previous 2d-approximation for \msr{d} can be optimized into a polynomial-time algorithm even if $d$ is not a constant but is part of the input. We then extend our inapproximability results to several related problems including \cmsr{d}, \gapmsr{\delta}{d}, and \gapcmsr{\delta}{d}.",2009-12-25T03:25:15Z,http://arxiv.org/pdf/0912.4935v4,2024-04-28,
0912.5276v1,"Better Gap-Hamming Lower Bounds via Better Round Elimination","Gap Hamming Distance is a well-studied problem in communication complexity, in which Alice and Bob have to decide whether the Hamming distance between their respective n-bit inputs is less than n/2-sqrt(n) or greater than n/2+sqrt(n). We show that every k-round bounded-error communication protocol for this problem sends a message of at least Omega(n/(k^2\log k)) bits. This lower bound has an exponentially better dependence on the number of rounds than the previous best bound, due to Brody and Chakrabarti. Our communication lower bound implies strong space lower bounds on algorithms for a number of data stream computations, such as approximating the number of distinct elements in a stream.   Subsequent to this result, the bound has been improved by some of us to the optimal Omega(n), independent of k, by using different techniques.",2009-12-30T14:12:32Z,http://arxiv.org/pdf/0912.5276v1,2024-04-28,
1001.0117v2,"Collapsing and Separating Completeness Notions under Average-Case and Worst-Case Hypotheses","This paper presents the following results on sets that are complete for NP.   1. If there is a problem in NP that requires exponential time at almost all lengths, then every many-one NP-complete set is complete under length-increasing reductions that are computed by polynomial-size circuits. 2. If there is a problem in coNP that cannot be solved by polynomial-size nondeterministic circuits, then every many-one complete set is complete under length-increasing reductions that are computed by polynomial-size circuits. 3. If there exist a one-way permutation that is secure against subexponential-size circuits and there is a hard tally language in NP intersect coNP, then there is a Turing complete language for NP that is not many-one complete. Our first two results use worst-case hardness hypotheses whereas earlier work that showed similar results relied on average-case or almost-everywhere hardness assumptions. The use of average-case and worst-case hypotheses in the last result is unique as previous results obtaining the same consequence relied on almost-everywhere hardness results.",2010-01-04T20:55:05Z,http://arxiv.org/pdf/1001.0117v2,2024-04-28,
1001.0208v2,"Intrinsic Universality in Self-Assembly","We show that the Tile Assembly Model exhibits a strong notion of universality where the goal is to give a single tile assembly system that simulates the behavior of any other tile assembly system. We give a tile assembly system that is capable of simulating a very wide class of tile systems, including itself. Specifically, we give a tile set that simulates the assembly of any tile assembly system in a class of systems that we call \emph{locally consistent}: each tile binds with exactly the strength needed to stay attached, and that there are no glue mismatches between tiles in any produced assembly.   Our construction is reminiscent of the studies of \emph{intrinsic universality} of cellular automata by Ollinger and others, in the sense that our simulation of a tile system $T$ by a tile system $U$ represents each tile in an assembly produced by $T$ by a $c \times c$ block of tiles in $U$, where $c$ is a constant depending on $T$ but not on the size of the assembly $T$ produces (which may in fact be infinite). Also, our construction improves on earlier simulations of tile assembly systems by other tile assembly systems (in particular, those of Soloveichik and Winfree, and of Demaine et al.) in that we simulate the actual process of self-assembly, not just the end result, as in Soloveichik and Winfree's construction, and we do not discriminate against infinite structures. Both previous results simulate only temperature 1 systems, whereas our construction simulates tile assembly systems operating at temperature 2.",2010-01-01T15:39:54Z,http://arxiv.org/pdf/1001.0208v2,2024-04-28,
1001.0383v2,"Restricted Space Algorithms for Isomorphism on Bounded Treewidth Graphs","The Graph Isomorphism problem restricted to graphs of bounded treewidth or bounded tree distance width are known to be solvable in polynomial time [Bod90],[YBFT99]. We give restricted space algorithms for these problems proving the following results: - Isomorphism for bounded tree distance width graphs is in L and thus complete for the class. We also show that for this kind of graphs a canon can be computed within logspace. - For bounded treewidth graphs, when both input graphs are given together with a tree decomposition, the problem of whether there is an isomorphism which respects the decompositions (i.e. considering only isomorphisms mapping bags in one decomposition blockwise onto bags in the other decomposition) is in L. - For bounded treewidth graphs, when one of the input graphs is given with a tree decomposition the isomorphism problem is in LogCFL. - As a corollary the isomorphism problem for bounded treewidth graphs is in LogCFL. This improves the known TC1 upper bound for the problem given by Grohe and Verbitsky [GroVer06].",2010-01-03T15:44:56Z,http://arxiv.org/pdf/1001.0383v2,2024-04-28,
1001.0464v3,"Holant Problems for Regular Graphs with Complex Edge Functions","We prove a complexity dichotomy theorem for Holant Problems on 3-regular graphs with an arbitrary complex-valued edge function. Three new techniques are introduced: (1) higher dimensional iterations in interpolation; (2) Eigenvalue Shifted Pairs, which allow us to prove that a pair of combinatorial gadgets in combination succeed in proving #P-hardness; and (3) algebraic symmetrization, which significantly lowers the symbolic complexity of the proof for computational complexity. With holographic reductions the classification theorem also applies to problems beyond the basic model.",2010-01-04T12:33:25Z,http://arxiv.org/pdf/1001.0464v3,2024-04-28,
1001.1593v1,"Fooling functions of halfspaces under product distributions","We construct pseudorandom generators that fool functions of halfspaces (threshold functions) under a very broad class of product distributions. This class includes not only familiar cases such as the uniform distribution on the discrete cube, the uniform distribution on the solid cube, and the multivariate Gaussian distribution, but also includes any product of discrete distributions with probabilities bounded away from 0.   Our first main result shows that a recent pseudorandom generator construction of Meka and Zuckerman [MZ09], when suitably modifed, can fool arbitrary functions of d halfspaces under product distributions where each coordinate has bounded fourth moment. To eps-fool any size-s, depth-d decision tree of halfspaces, our pseudorandom generator uses seed length O((d log(ds/eps)+log n) log(ds/eps)). For monotone functions of d halfspaces, the seed length can be improved to O((d log(d/eps)+log n) log(d/eps)). We get better bounds for larger eps; for example, to 1/polylog(n)-fool all monotone functions of (log n)= log log n halfspaces, our generator requires a seed of length just O(log n). Our second main result generalizes the work of Diakonikolas et al. [DGJ+09] to show that bounded independence suffices to fool functions of halfspaces under product distributions. Assuming each coordinate satisfies a certain stronger moment condition, we show that any function computable by a size-s, depth-d decision tree of halfspaces is eps-fooled by O(d^4s^2/eps^2)-wise independence.",2010-01-11T06:11:56Z,http://arxiv.org/pdf/1001.1593v1,2024-04-28,
1001.2034v1,"On the Power of Unambiguity in Logspace","We report progress on the \NL vs \UL problem. [-] We show unconditionally that the complexity class $\ReachFewL\subseteq\UL$. This improves on the earlier known upper bound $\ReachFewL \subseteq \FewL$. [-] We investigate the complexity of min-uniqueness - a central notion in studying the \NL vs \UL problem. We show that min-uniqueness is necessary and sufficient for showing $\NL =\UL$. We revisit the class $\OptL[\log n]$ and show that {\sc ShortestPathLength} - computing the length of the shortest path in a DAG, is complete for $\OptL[\log n]$. We introduce $\UOptL[\log n]$, an unambiguous version of $\OptL[\log n]$, and show that (a) $\NL =\UL$ if and only if $\OptL[\log n] = \UOptL[\log n]$, (b) $\LogFew \leq \UOptL[\log n] \leq \SPL$. [-] We show that the reachability problem over graphs embedded on 3 pages is complete for \NL. This contrasts with the reachability problem over graphs embedded on 2 pages which is logspace equivalent to the reachability problem in planar graphs and hence is in \UL.",2010-01-12T22:13:31Z,http://arxiv.org/pdf/1001.2034v1,2024-04-28,
1001.2052v1,"Block Sensitivity of Minterm-Transitive Functions","Boolean functions with symmetry properties are interesting from a complexity theory perspective; extensive research has shown that these functions, if nonconstant, must have high `complexity' according to various measures.   In recent work of this type, Sun gave bounds on the block sensitivity of nonconstant Boolean functions invariant under a transitive permutation group. Sun showed that all such functions satisfy bs(f) = Omega(N^{1/3}), and that there exists such a function for which bs(f) = O(N^{3/7}ln N). His example function belongs to a subclass of transitively invariant functions called the minterm-transitive functions (defined in earlier work by Chakraborty).   We extend these results in two ways. First, we show that nonconstant minterm-transitive functions satisfy bs(f) = Omega(N^{3/7}). Thus Sun's example function has nearly minimal block sensitivity for this subclass. Second, we give an improved example: a minterm-transitive function for which bs(f) = O(N^{3/7}ln^{1/7}N).",2010-01-13T20:29:28Z,http://arxiv.org/pdf/1001.2052v1,2024-04-28,
1001.3485v1,"Randomness Testing of Compressed Data","Random Number Generators play a critical role in a number of important applications. In practice, statistical testing is employed to gather evidence that a generator indeed produces numbers that appear to be random. In this paper, we reports on the studies that were conducted on the compressed data using 8 compression algorithms or compressors. The test results suggest that the output of compression algorithms or compressors has bad randomness, the compression algorithms or compressors are not suitable as random number generator. We also found that, for the same compression algorithm, there exists positive correlation relationship between compression ratio and randomness, increasing the compression ratio increases randomness of compressed data. As time permits, additional randomness testing efforts will be conducted.",2010-01-20T07:43:47Z,http://arxiv.org/pdf/1001.3485v1,2024-04-28,
1001.3816v2,"The P versus NP Problem","Removed by arXiv administration.   This article was plagiarized directly from Stephen Cook's description of the problem for the Clay Mathematics Institute. See http://gauss.claymath.org:8888/millennium/P_vs_NP/pvsnp.pdf for the original text.",2010-01-21T14:37:36Z,http://arxiv.org/pdf/1001.3816v2,2024-04-28,
1001.4649v1,"Is Space a Stronger Resource than Time? Positive Answer for the Nondeterministic at-Least-Quadratic Time Case","We show that all languages accepted in time f(n) >= n^2 can be accepted in space O(f(n)^{1/2})_and_ in time O(f(n)). The proof is carried out by simulation, based on the idea of guessing the sequences of internal states of the simulated TM when entering certain critical cells, whose location is also guessed. Our method cannot be generalised easily to many-tapes TMs, and in no case can it be relativised.",2010-01-26T12:22:33Z,http://arxiv.org/pdf/1001.4649v1,2024-04-28,
1001.4687v1,m-sophistication,"The m-sophistication of a finite binary string x is introduced as a generalization of some parameter in the proof that complexity of complexity is rare. A probabilistic near sufficient statistic of x is given which length is upper bounded by the m-sophistication of x within small additive terms. This shows that m-sophistication is lower bounded by coarse sophistication and upper bounded by sophistication within small additive terms. It is also shown that m-sophistication and coarse sophistication can not be approximated by an upper or lower semicomputable function, not even within very large error.",2010-01-26T13:53:08Z,http://arxiv.org/pdf/1001.4687v1,2024-04-28,
1001.4987v2,"The Complexity of Approximating Bounded-Degree Boolean #CSP (Extended Abstract)","The degree of a CSP instance is the maximum number of times that a variable may appear in the scope of constraints. We consider the approximate counting problem for Boolean CSPs with bounded-degree instances, for constraint languages containing the two unary constant relations {0} and {1}. When the maximum degree is at least 25 we obtain a complete classification of the complexity of this problem. It is exactly solvable in polynomial-time if every relation in the constraint language is affine. It is equivalent to the problem of approximately counting independent sets in bipartite graphs if every relation can be expressed as conjunctions of {0}, {1} and binary implication. Otherwise, there is no FPRAS unless NP=RP. For lower degree bounds, additional cases arise in which the complexity is related to the complexity of approximately counting independent sets in hypergraphs.",2010-01-27T17:07:40Z,http://arxiv.org/pdf/1001.4987v2,2024-04-28,
1002.1496v1,"Deterministic Black-Box Identity Testing $π$-Ordered Algebraic Branching Programs","In this paper we study algebraic branching programs (ABPs) with restrictions on the order and the number of reads of variables in the program. Given a permutation $\pi$ of $n$ variables, for a $\pi$-ordered ABP ($\pi$-OABP), for any directed path $p$ from source to sink, a variable can appear at most once on $p$, and the order in which variables appear on $p$ must respect $\pi$. An ABP $A$ is said to be of read $r$, if any variable appears at most $r$ times in $A$. Our main result pertains to the identity testing problem. Over any field $F$ and in the black-box model, i.e. given only query access to the polynomial, we have the following result: read $r$ $\pi$-OABP computable polynomials can be tested in $\DTIME[2^{O(r\log r \cdot \log^2 n \log\log n)}]$.   Our next set of results investigates the computational limitations of OABPs. It is shown that any OABP computing the determinant or permanent requires size $\Omega(2^n/n)$ and read $\Omega(2^n/n^2)$. We give a multilinear polynomial $p$ in $2n+1$ variables over some specifically selected field $G$, such that any OABP computing $p$ must read some variable at least $2^n$ times. We show that the elementary symmetric polynomial of degree $r$ in $n$ variables can be computed by a size $O(rn)$ read $r$ OABP, but not by a read $(r-1)$ OABP, for any $0 < 2r-1 \leq n$. Finally, we give an example of a polynomial $p$ and two variables orders $\pi \neq \pi'$, such that $p$ can be computed by a read-once $\pi$-OABP, but where any $\pi'$-OABP computing $p$ must read some variable at least $2^n$",2010-02-07T22:40:21Z,http://arxiv.org/pdf/1002.1496v1,2024-04-28,
1002.1606v4,"Derandomized Parallel Repetition via Structured PCPs","A PCP is a proof system for NP in which the proof can be checked by a probabilistic verifier. The verifier is only allowed to read a very small portion of the proof, and in return is allowed to err with some bounded probability. The probability that the verifier accepts a false proof is called the soundness error, and is an important parameter of a PCP system that one seeks to minimize. Constructing PCPs with sub-constant soundness error and, at the same time, a minimal number of queries into the proof (namely two) is especially important due to applications for inapproximability.   In this work we construct such PCP verifiers, i.e., PCPs that make only two queries and have sub-constant soundness error. Our construction can be viewed as a combinatorial alternative to the ""manifold vs. point"" construction, which is the only construction in the literature for this parameter range. The ""manifold vs. point"" PCP is based on a low degree test, while our construction is based on a direct product test. We also extend our construction to yield a decodable PCP (dPCP) with the same parameters. By plugging in this dPCP into the scheme of Dinur and Harsha (FOCS 2009) one gets an alternative construction of the result of Moshkovitz and Raz (FOCS 2008), namely: a construction of two-query PCPs with small soundness error and small alphabet size.   Our construction of a PCP is based on extending the derandomized direct product test of Impagliazzo, Kabanets and Wigderson (STOC 09) to a derandomized parallel repetition theorem. More accurately, our PCP construction is obtained in two steps. We first prove a derandomized parallel repetition theorem for specially structured PCPs. Then, we show that any PCP can be transformed into one that has the required structure, by embedding it on a de-Bruijn graph.",2010-02-08T13:59:28Z,http://arxiv.org/pdf/1002.1606v4,2024-04-28,
1002.1880v4,"Finding and counting vertex-colored subtrees","The problems studied in this article originate from the Graph Motif problem introduced by Lacroix et al. in the context of biological networks. The problem is to decide if a vertex-colored graph has a connected subgraph whose colors equal a given multiset of colors $M$. It is a graph pattern-matching problem variant, where the structure of the occurrence of the pattern is not of interest but the only requirement is the connectedness. Using an algebraic framework recently introduced by Koutis et al., we obtain new FPT algorithms for Graph Motif and variants, with improved running times. We also obtain results on the counting versions of this problem, proving that the counting problem is FPT if M is a set, but becomes W[1]-hard if M is a multiset with two colors. Finally, we present an experimental evaluation of this approach on real datasets, showing that its performance compares favorably with existing software.",2010-02-09T15:19:54Z,http://arxiv.org/pdf/1002.1880v4,2024-04-28,
1002.3453v1,"On the complexity of stratified logics","Our primary motivation is the comparison of two different traditions used in ICC to characterize the class FPTIME of the polynomial time computable functions. On one side, FPTIME can be captured by Intuitionistic Light Affine Logic (ILAL), a logic derived from Linear Logic, characterized by the structural invariant Stratification. On the other side, FPTIME can be captured by Safe Recursion on Notation (SRN), an algebra of functions based on Predicative Recursion, a restriction of the standard recursion schema used to defiine primitive recursive functions. Stratifiication and Predicative Recursion seem to share common underlying principles, whose study is the main subject of this work.",2010-02-18T08:41:01Z,http://arxiv.org/pdf/1002.3453v1,2024-04-28,
1002.3664v1,"A PCP Characterization of AM","We introduce a 2-round stochastic constraint-satisfaction problem, and show that its approximation version is complete for (the promise version of) the complexity class AM. This gives a `PCP characterization' of AM analogous to the PCP Theorem for NP. Similar characterizations have been given for higher levels of the Polynomial Hierarchy, and for PSPACE; however, we suggest that the result for AM might be of particular significance for attempts to derandomize this class.   To test this notion, we pose some `Randomized Optimization Hypotheses' related to our stochastic CSPs that (in light of our result) would imply collapse results for AM. Unfortunately, the hypotheses appear over-strong, and we present evidence against them. In the process we show that, if some language in NP is hard-on-average against circuits of size 2^{Omega(n)}, then there exist hard-on-average optimization problems of a particularly elegant form.   All our proofs use a powerful form of PCPs known as Probabilistically Checkable Proofs of Proximity, and demonstrate their versatility. We also use known results on randomness-efficient soundness- and hardness-amplification. In particular, we make essential use of the Impagliazzo-Wigderson generator; our analysis relies on a recent Chernoff-type theorem for expander walks.",2010-02-19T03:34:59Z,http://arxiv.org/pdf/1002.3664v1,2024-04-28,
1002.3769v2,"Polyominoes Simulating Arbitrary-Neighborhood Zippers and Tilings","This paper provides a bridge between the classical tiling theory and the complex neighborhood self-assembling situations that exist in practice. The neighborhood of a position in the plane is the set of coordinates which are considered adjacent to it. This includes classical neighborhoods of size four, as well as arbitrarily complex neighborhoods. A generalized tile system consists of a set of tiles, a neighborhood, and a relation which dictates which are the ""admissible"" neighboring tiles of a given tile. Thus, in correctly formed assemblies, tiles are assigned positions of the plane in accordance to this relation. We prove that any validly tiled path defined in a given but arbitrary neighborhood (a zipper) can be simulated by a simple ""ribbon"" of microtiles. A ribbon is a special kind of polyomino, consisting of a non-self-crossing sequence of tiles on the plane, in which successive tiles stick along their adjacent edge. Finally, we extend this construction to the case of traditional tilings, proving that we can simulate arbitrary-neighborhood tilings by simple-neighborhood tilings, while preserving some of their essential properties.",2010-02-19T16:19:53Z,http://arxiv.org/pdf/1002.3769v2,2024-04-28,
1002.4084v1,"Properties of Pseudo-Primitive Words and their Applications","A pseudo-primitive word with respect to an antimorphic involution \theta is a word which cannot be written as a catenation of occurrences of a strictly shorter word t and \theta(t). Properties of pseudo-primitive words are investigated in this paper. These properties link pseudo-primitive words with essential notions in combinatorics on words such as primitive words, (pseudo)-palindromes, and (pseudo)-commutativity. Their applications include an improved solution to the extended Lyndon-Sch\""utzenberger equation u_1 u_2 ... u_l = v_1 ... v_n w_1 ... w_m, where u_1, ..., u_l \in {u, \theta(u)}, v_1, ..., v_n \in {v, \theta(v)}, and w_1, ..., w_m \in {w, \theata(w)} for some words u, v, w, integers l, n, m \ge 2, and an antimorphic involution \theta. We prove that for l \ge 4, n,m \ge 3, this equation implies that u, v, w can be expressed in terms of a common word t and its image \theta(t). Moreover, several cases of this equation where l = 3 are examined.",2010-02-22T09:40:13Z,http://arxiv.org/pdf/1002.4084v1,2024-04-28,
1002.4676v1,"Pebbling and Branching Programs Solving the Tree Evaluation Problem","We study restricted computation models related to the Tree Evaluation Problem}. The TEP was introduced in earlier work as a simple candidate for the (*very*) long term goal of separating L and LogDCFL. The input to the problem is a rooted, balanced binary tree of height h, whose internal nodes are labeled with binary functions on [k] = {1,...,k} (each given simply as a list of k^2 elements of [k]), and whose leaves are labeled with elements of [k]. Each node obtains a value in [k] equal to its binary function applied to the values of its children, and the output is the value of the root. The first restricted computation model, called Fractional Pebbling, is a generalization of the black/white pebbling game on graphs, and arises in a natural way from the search for good upper bounds on the size of nondeterministic branching programs (BPs) solving the TEP - for any fixed h, if the binary tree of height h has fractional pebbling cost at most p, then there are nondeterministic BPs of size O(k^p) solving the height h TEP. We prove a lower bound on the fractional pebbling cost of d-ary trees that is tight to within an additive constant for each fixed d. The second restricted computation model we study is a semantic restriction on (non)deterministic BPs solving the TEP - Thrifty BPs. Deterministic (resp. nondeterministic) thrifty BPs suffice to implement the best known algorithms for the TEP, based on black (resp. fractional) pebbling. In earlier work, for each fixed h a lower bound on the size of deterministic thrifty BPs was proved that is tight for sufficiently large k. We give an alternative proof that achieves the same bound for all k. We show the same bound still holds in a less-restricted model, and also that gradually weaker lower bounds can be obtained for gradually weaker restrictions on the model.",2010-02-25T01:41:38Z,http://arxiv.org/pdf/1002.4676v1,2024-04-28,
1003.1164v1,"Repeating Patterns in Linear Programs that express NP-Complete Problems","One of my recent papers transforms an NP-Complete problem into the question of whether or not a feasible real solution exists to some Linear Program. The unique feature of this Linear Program is that though there is no explicit bound on the minimum required number of linear inequalities, which is most probably exponential to the size of the NP-Complete problem, the Linear Program can still be described efficiently. The reason for this efficient description is that coefficients keep repeating in some pattern, even as the number of inequalities is conveniently assumed to tend to Infinity. I discuss why this convenient assumption does not change the feasibility result of the Linear Program. I conclude with two Conjectures, which might help to make an efficient decision on the feasibility of this Linear Program.",2010-03-05T13:02:18Z,http://arxiv.org/pdf/1003.1164v1,2024-04-28,
1003.3704v2,"On a variant of Monotone NAE-3SAT and the Triangle-Free Cut problem","In this paper we define a restricted version of Monotone NAE-3SAT and show that it remains NP-Complete even under that restriction. We expect this result would be useful in proving NP-Completeness results for problems on $k$-colourable graphs ($k \ge 5$). We also prove the NP-Completeness of the Triangle-Free Cut problem.",2010-03-19T02:45:38Z,http://arxiv.org/pdf/1003.3704v2,2024-04-28,
cs/0011014v1,"Chip-level CMP Modeling and Smart Dummy for HDP and Conformal CVD Films","Chip-level CMP modeling is investigated to obtain the post-CMP film profile thickness across a die from its design layout file and a few film deposition and CMP parameters. The work covers both HDP and conformal CVD film. The experimental CMP results agree well with the modeled results. Different algorithms for filling of dummy structure are compared. A smart algorithm for dummy filling is presented, which achieves maximal pattern-density uniformity and CMP planarity.",2000-11-09T20:39:25Z,http://arxiv.org/pdf/cs/0011014v1,2024-04-28,
cs/0102003v1,"Fast Pricing of European Asian Options with Provable Accuracy: Single-stock and Basket Options","This paper develops three polynomial-time pricing techniques for European Asian options with provably small errors, where the stock prices follow binomial trees or trees of higher-degree. The first technique is the first known Monte Carlo algorithm with analytical error bounds suitable for pricing single-stock options with meaningful confidence and speed. The second technique is a general recursive bucketing-based scheme that can use the Aingworth-Motwani-Oldham aggregation algorithm, Monte-Carlo simulation and possibly others as the base-case subroutine. This scheme enables robust trade-offs between accuracy and time over subtrees of different sizes. For long-term options or high frequency price averaging, it can price single-stock options with smaller errors in less time than the base-case algorithms themselves. The third technique combines Fast Fourier Transform with bucketing-based schemes for pricing basket options. This technique takes polynomial time in the number of days and the number of stocks, and does not add any errors to those already incurred in the companion bucketing scheme. This technique assumes that the price of each underlying stock moves independently.",2001-02-02T20:52:36Z,http://arxiv.org/pdf/cs/0102003v1,2024-04-28,
cs/0104013v1,"Shooting Over or Under the Mark: Towards a Reliable and Flexible Anticipation in the Economy","The real monetary economy is grounded upon monetary flow equilibration or the activity of actualizing monetary flow continuity at each economic agent except for the central bank. Every update of monetary flow continuity at each agent constantly causes monetary flow equilibration at the neighborhood agents. Every monetary flow equilibration as the activity of shooting the mark identified as monetary flow continuity turns out to be off the mark, and constantly generate the similar activities in sequence. Monetary flow equilibration ceaselessly reverberating in the economy performs two functions. One is to seek an organization on its own, and the other is to perturb the ongoing organization. Monetary flow equilibration as the agency of seeking and perturbing its organization also serves as a means of predicting its behavior. The likely organizational behavior could be the one that remains most robust against monetary flow equilibration as an agency of applying perturbations.",2001-04-09T06:43:12Z,http://arxiv.org/pdf/cs/0104013v1,2024-04-28,
cs/0104014v1,"Tracing a Faint Fingerprint of the Invisible Hand?","Any economic agent constituting the monetary economy maintains the activity of monetary flow equilibration for fulfilling the condition of monetary flow continuity in the record, except at the central bank. At the same time, monetary flow equilibration at one economic agent constantly induces at other agents in the economy further flow disequilibrium to be eliminated subsequently. We propose the rate of monetary flow disequilibration as a figure measuring the progressive movement of the economy. The rate of disequilibration was read out of both the Japanese and the United States monetary economy recorded over the last fifty years.",2001-04-09T08:49:57Z,http://arxiv.org/pdf/cs/0104014v1,2024-04-28,
cs/0105004v1,"Parallel implementation of the TRANSIMS micro-simulation","This paper describes the parallel implementation of the TRANSIMS traffic micro-simulation. The parallelization method is domain decomposition, which means that each CPU of the parallel computer is responsible for a different geographical area of the simulated region. We describe how information between domains is exchanged, and how the transportation network graph is partitioned. An adaptive scheme is used to optimize load balancing. We then demonstrate how computing speeds of our parallel micro-simulations can be systematically predicted once the scenario and the computer architecture are known. This makes it possible, for example, to decide if a certain study is feasible with a certain computing budget, and how to invest that budget. The main ingredients of the prediction are knowledge about the parallel implementation of the micro-simulation, knowledge about the characteristics of the partitioning of the transportation network graph, and knowledge about the interaction of these quantities with the computer system. In particular, we investigate the differences between switched and non-switched topologies, and the effects of 10 Mbit, 100 Mbit, and Gbit Ethernet. keywords: Traffic simulation, parallel computing, transportation planning, TRANSIMS",2001-05-02T12:43:39Z,http://arxiv.org/pdf/cs/0105004v1,2024-04-28,
cs/0110067v1,"Analysis of Investment Policy in Belarus","The optimal planning trajectory is analyzed on the basis of the growth model with effectiveness. The saving per capital value has to be rather high initially with smooth decrement in the future years.",2001-10-31T20:18:35Z,http://arxiv.org/pdf/cs/0110067v1,2024-04-28,
cs/0201026v1,"An Empirical Model for Volatility of Returns and Option Pricing","In a seminal paper in 1973, Black and Scholes argued how expected distributions of stock prices can be used to price options. Their model assumed a directed random motion for the returns and consequently a lognormal distribution of asset prices after a finite time. We point out two problems with their formulation. First, we show that the option valuation is not uniquely determined; in particular, stratergies based on the delta-hedge and CAMP (Capital Asset Pricing Model) are shown to provide different valuations of an option. Second, asset returns are known not to be Gaussian distributed. Empirically, distributions of returns are seen to be much better approximated by an exponential distribution. This exponential distribution of asset prices can be used to develop a new pricing model for options that is shown to provide valuations that agree very well with those used by traders. We show how the Fokker-Planck formulation of fluctuations (i.e., the dynamics of the distribution) can be modified to provide an exponential distribution for returns. We also show how a singular volatility can be used to go smoothly from exponential to Gaussian returns and thereby illustrate why exponential returns cannot be reached perturbatively starting from Gaussian ones, and explain how the theory of 'stochastic volatility' can be obtained from our model by making a bad approximation. Finally, we show how to calculate put and call prices for a stretched exponential density.",2002-01-29T18:03:39Z,http://arxiv.org/pdf/cs/0201026v1,2024-04-28,
cs/0203023v1,"Agent trade servers in financial exchange systems","New services based on the best-effort paradigm could complement the current deterministic services of an electronic financial exchange. Four crucial aspects of such systems would benefit from a hybrid stance: proper use of processing resources, bandwidth management, fault tolerance, and exception handling. We argue that a more refined view on Quality-of-Service control for exchange systems, in which the principal ambition of upholding a fair and orderly marketplace is left uncompromised, would benefit all interested parties.",2002-03-19T10:05:58Z,http://arxiv.org/pdf/cs/0203023v1,2024-04-28,
cs/0204051v1,"Parrondo Strategies for Artificial Traders","On markets with receding prices, artificial noise traders may consider alternatives to buy-and-hold. By simulating variations of the Parrondo strategy, using real data from the Swedish stock market, we produce first indications of a buy-low-sell-random Parrondo variation outperforming buy-and-hold. Subject to our assumptions, buy-low-sell-random also outperforms the traditional value and trend investor strategies. We measure the success of the Parrondo variations not only through their performance compared to other kinds of strategies, but also relative to varying levels of perfect information, received through messages within a multi-agent system of artificial traders.",2002-04-26T12:20:08Z,http://arxiv.org/pdf/cs/0204051v1,2024-04-28,
cs/0204056v1,"Trading Agents for Roaming Users","Some roaming users need services to manipulate autonomous processes. Trading agents running on agent trade servers are used as a case in point. We present a solution that provides the agent owners with means to upkeeping their desktop environment, and maintaining their agent trade server processes, via a briefcase service.",2002-04-29T12:20:11Z,http://arxiv.org/pdf/cs/0204056v1,2024-04-28,
cs/0208022v1,"Symbolic Methodology in Numeric Data Mining: Relational Techniques for Financial Applications","Currently statistical and artificial neural network methods dominate in financial data mining. Alternative relational (symbolic) data mining methods have shown their effectiveness in robotics, drug design and other applications. Traditionally symbolic methods prevail in the areas with significant non-numeric (symbolic) knowledge, such as relative location in robot navigation. At first glance, stock market forecast looks as a pure numeric area irrelevant to symbolic methods. One of our major goals is to show that financial time series can benefit significantly from relational data mining based on symbolic methods. The paper overviews relational data mining methodology and develops this techniques for financial data mining.",2002-08-15T03:45:36Z,http://arxiv.org/pdf/cs/0208022v1,2024-04-28,
cs/0208040v1,"Using Hierarchical Data Mining to Characterize Performance of Wireless System Configurations","This paper presents a statistical framework for assessing wireless systems performance using hierarchical data mining techniques. We consider WCDMA (wideband code division multiple access) systems with two-branch STTD (space time transmit diversity) and 1/2 rate convolutional coding (forward error correction codes). Monte Carlo simulation estimates the bit error probability (BEP) of the system across a wide range of signal-to-noise ratios (SNRs). A performance database of simulation runs is collected over a targeted space of system configurations. This database is then mined to obtain regions of the configuration space that exhibit acceptable average performance. The shape of the mined regions illustrates the joint influence of configuration parameters on system performance. The role of data mining in this application is to provide explainable and statistically valid design conclusions. The research issue is to define statistically meaningful aggregation of data in a manner that permits efficient and effective data mining algorithms. We achieve a good compromise between these goals and help establish the applicability of data mining for characterizing wireless systems performance.",2002-08-25T16:28:33Z,http://arxiv.org/pdf/cs/0208040v1,2024-04-28,
cs/0210005v1,"Positive time fractional derivative","In mathematical modeling of the non-squared frequency-dependent diffusions, also known as the anomalous diffusions, it is desirable to have a positive real Fourier transform for the time derivative of arbitrary fractional or odd integer order. The Fourier transform of the fractional time derivative in the Riemann-Liouville and Caputo senses, however, involves a complex power function of the fractional order. In this study, a positive time derivative of fractional or odd integer order is introduced to respect the positivity in modeling the anomalous diffusions.",2002-10-07T19:28:50Z,http://arxiv.org/pdf/cs/0210005v1,2024-04-28,
cs/0302034v2,"Interest Rate Model Calibration Using Semidefinite Programming","We show that, for the purpose of pricing Swaptions, the Swap rate and the corresponding Forward rates can be considered lognormal under a single martingale measure. Swaptions can then be priced as options on a basket of lognormal assets and an approximation formula is derived for such options. This formula is centered around a Black-Scholes price with an appropriate volatility, plus a correction term that can be interpreted as the expected tracking error. The calibration problem can then be solved very efficiently using semidefinite programming.",2003-02-25T02:48:42Z,http://arxiv.org/pdf/cs/0302034v2,2024-04-28,
cs/0302035v2,"Risk-Management Methods for the Libor Market Model Using Semidefinite Programming","When interest rate dynamics are described by the Libor Market Model as in BGM97, we show how some essential risk-management results can be obtained from the dual of the calibration program. In particular, if the objetive is to maximize another swaption's price, we show that the optimal dual variables describe a hedging portfolio in the sense of \cite{Avel96}. In the general case, the local sensitivity of the covariance matrix to all market movement scenarios can be directly computed from the optimal dual solution. We also show how semidefinite programming can be used to manage the Gamma exposure of a portfolio.",2003-02-25T03:09:11Z,http://arxiv.org/pdf/cs/0302035v2,2024-04-28,
cs/0304009v1,"Stochastic Volatility in a Quantitative Model of Stock Market Returns","Standard quantitative models of the stock market predict a log-normal distribution for stock returns (Bachelier 1900, Osborne 1959), but it is recognised (Fama 1965) that empirical data, in comparison with a Gaussian, exhibit leptokurtosis (it has more probability mass in its tails and centre) and fat tails (probabilities of extreme events are underestimated). Different attempts to explain this departure from normality have coexisted. In particular, since one of the strong assumptions of the Gaussian model concerns the volatility, considered finite and constant, the new models were built on a non finite (Mandelbrot 1963) or non constant (Cox, Ingersoll and Ross 1985) volatility. We investigate in this thesis a very recent model (Dragulescu et al. 2002) based on a Brownian motion process for the returns, and a stochastic mean-reverting process for the volatility. In this model, the forward Kolmogorov equation that governs the time evolution of returns is solved analytically. We test this new theory against different stock indexes (Dow Jones Industrial Average, Standard and Poor s and Footsie), over different periods (from 20 to 105 years). Our aim is to compare this model with the classical Gaussian and with a simple Neural Network, used as a benchmark. We perform the usual statistical tests on the kurtosis and tails of the expected distributions, paying particular attention to the outliers. As claimed by the authors, the new model outperforms the Gaussian for any time lag, but is artificially too complex for medium and low frequencies, where the Gaussian is preferable. Moreover this model is still rejected for high frequencies, at a 0.05 level of significance, due to the kurtosis, incorrectly handled.",2003-04-07T17:37:55Z,http://arxiv.org/pdf/cs/0304009v1,2024-04-28,
cs/0305036v4,"Using Dynamic Simulation in the Development of Construction Machinery","As in the car industry for quite some time, dynamic simulation of complete vehicles is being practiced more and more in the development of off-road machinery. However, specific questions arise due not only to company structure and size, but especially to the type of product. Tightly coupled, non-linear subsystems of different domains make prediction and optimisation of the complete system's dynamic behaviour a challenge. Furthermore, the demand for versatile machines leads to sometimes contradictory target requirements and can turn the design process into a hunt for the least painful compromise. This can be avoided by profound system knowledge, assisted by simulation-driven product development. This paper gives an overview of joint research into this issue by Volvo Wheel Loaders and Linkoping University on that matter, lists the results of a related literature review and introduces the term ""operateability"". Rather than giving detailed answers, the problem space for ongoing and future research is examined and possible solutions are sketched.",2003-05-19T20:51:50Z,http://arxiv.org/pdf/cs/0305036v4,2024-04-28,
cs/0305055v1,"Goodness-of-fit of the Heston model","An analytical formula for the probability distribution of stock-market returns, derived from the Heston model assuming a mean-reverting stochastic volatility, was recently proposed by Dragulescu and Yakovenko in Quantitative Finance 2002. While replicating their results, we found two significant weaknesses in their method to pre-process the data, which cast a shadow over the effective goodness-of-fit of the model. We propose a new method, more truly capturing the market, and perform a Kolmogorov-Smirnov test and a Chi Square test on the resulting probability distribution. The results raise some significant questions for large time lags -- 40 to 250 days -- where the smoothness of the data does not require such a complex model; nevertheless, we also provide some statistical evidence in favour of the Heston model for small time lags -- 1 and 5 days -- compared with the traditional Gaussian model assuming constant volatility.",2003-05-29T18:13:08Z,http://arxiv.org/pdf/cs/0305055v1,2024-04-28,
cs/0306105v1,"Design, implementation and deployment of the Saclay muon reconstruction algorithms (Muonbox/y) in the Athena software framework of the ATLAS experiment","This paper gives an overview of a reconstruction algorithm for muon events in ATLAS experiment at CERN. After a short introduction on ATLAS Muon Spectrometer, we will describe the procedure performed by the algorithms Muonbox and Muonboy (last version) in order to achieve correctly the reconstruction task. These algorithms have been developed in Fortran language and are working in the official C++ framework Athena, as well as in stand alone mode. A description of the interaction between Muonboy and Athena will be given, together with the reconstruction performances (efficiency and momentum resolution) obtained with MonteCarlo data.",2003-06-17T07:01:43Z,http://arxiv.org/pdf/cs/0306105v1,2024-04-28,
cs/0307039v1,"Modeling Business","Business concepts are studied using a metamodel-based approach, using UML 2.0. The Notation Independent Business concepts metamodel is introduced. The approach offers a mapping between different business modeling notations which could be used for bridging BM tools and boosting the MDA approach.",2003-07-17T15:41:13Z,http://arxiv.org/pdf/cs/0307039v1,2024-04-28,
cs/0307053v1,"Hamevol1.0: a C++ code for differential equations based on Runge-Kutta algorithm. An application to matter enhanced neutrino oscillation","We present a C++ implementation of a fifth order semi-implicit Runge-Kutta algorithm for solving Ordinary Differential Equations. This algorithm can be used for studying many different problems and in particular it can be applied for computing the evolution of any system whose Hamiltonian is known. We consider in particular the problem of calculating the neutrino oscillation probabilities in presence of matter interactions. The time performance and the accuracy of this implementation is competitive with respect to the other analytical and numerical techniques used in literature. The algorithm design and the salient features of the code are presented and discussed and some explicit examples of code application are given.",2003-07-23T19:30:25Z,http://arxiv.org/pdf/cs/0307053v1,2024-04-28,
cs/0307064v1,"Implementing an Agent Trade Server","An experimental server for stock trading autonomous agents is presented and made available, together with an agent shell for swift development. The server, written in Java, was implemented as proof-of-concept for an agent trade server for a real financial exchange.",2003-07-29T12:58:45Z,http://arxiv.org/pdf/cs/0307064v1,2024-04-28,
cs/0406021v3,"A direct formulation for sparse PCA using semidefinite programming","We examine the problem of approximating, in the Frobenius-norm sense, a positive, semidefinite symmetric matrix by a rank-one matrix, with an upper bound on the cardinality of its eigenvector. The problem arises in the decomposition of a covariance matrix into sparse factors, and has wide applications ranging from biology to finance. We use a modification of the classical variational representation of the largest eigenvalue of a symmetric matrix, where cardinality is constrained, and derive a semidefinite programming based relaxation for our problem. We also discuss Nesterov's smooth minimization technique applied to the SDP arising in the direct sparse PCA method.",2004-06-16T01:23:03Z,http://arxiv.org/pdf/cs/0406021v3,2024-04-28,
cs/0407029v1,"Static versus Dynamic Arbitrage Bounds on Multivariate Option Prices","We compare static arbitrage price bounds on basket calls, i.e. bounds that only involve buy-and-hold trading strategies, with the price range obtained within a multi-variate generalization of the Black-Scholes model. While there is no gap between these two sets of prices in the univariate case, we observe here that contrary to our intuition about model risk for at-the-money calls, there is a somewhat large gap between model prices and static arbitrage prices, hence a similarly large set of prices on which a multivariate Black-Scholes model cannot be calibrated but where no conclusion can be drawn on the presence or not of a static arbitrage opportunity.",2004-07-10T16:17:26Z,http://arxiv.org/pdf/cs/0407029v1,2024-04-28,
cs/0410064v1,"Intelligent Computer Numerical Control unit for machine tools","The paper describes a new CNC control unit for machining centres with learning ability and automatic intelligent generating of NC programs on the bases of a neural network, which is built-in into a CNC unit as special device. The device performs intelligent and completely automatically the NC part programs only on the bases of 2D, 2,5D or 3D computer model of prismatic part. Intervention of the operator is not needed. The neural network for milling, drilling, reaming, threading and operations alike has learned to generate NC programs in the learning module, which is a part of intelligent CAD/CAM system.",2004-10-25T15:55:43Z,http://arxiv.org/pdf/cs/0410064v1,2024-04-28,
cs/0412031v1,"The Features of the Complex CAD system of Reconstruction of the Industrial Plants","The features of designing of reconstruction of the acting plant by its design department are considered: the results of work are drawings corresponding with the national standards; large number of the small projects for different acting objects; variety of the types of the drawings in one project; large paper archive. The models and methods of developing of the complex CAD system with friend uniform environment of designing, with setting a profile of operations, with usage of the general parts of the project, with a series of problem-oriented subsystems are described on an example of a CAD system TechnoCAD GlassX",2004-12-08T08:52:28Z,http://arxiv.org/pdf/cs/0412031v1,2024-04-28,
cs/0412033v1,"The modelling of the build constructions in a CAD of the renovation of the enterprises by means of units in the drawings","The parametric model of build constructions and features of design operations are described for making drawings, which are the common component of the different parts of the projects of renovation of enterprises. The key moment of the deep design automation is the using of so-called units in the drawings, which are joining a visible graphic part and invisible parameters. The model has passed check during designing of several hundreds of drawings",2004-12-08T09:01:20Z,http://arxiv.org/pdf/cs/0412033v1,2024-04-28,
cs/0412034v1,"The informatization of design works at industry firm during its renovation","The characteristic of design works on firm at its renovation and of the common directions of their informatization is given. The implantation of a CAD is selected as the key direction, and the requirements to a complex CAD-system are stated. The methods of such a CAD-system development are featured, and the connectedness of this development with the process of integration of information space of design department of the firm is characterized. The experience of development and implantation of a complex CAD of renovation of firms TechnoCAD GlassX lies in a basis of this reviewing",2004-12-08T11:08:48Z,http://arxiv.org/pdf/cs/0412034v1,2024-04-28,
cs/0503084v1,"The Peculiarities of Nonstationary Formation of Inhomogeneous Structures of Charged Particles in the Electrodiffusion Processes","In this paper the distribution of charged particles is constructed under the approximation of ambipolar diffusion. The results of mathematical modelling in two-dimensional case taking into account the velocities of the system are presented.",2005-03-30T07:00:29Z,http://arxiv.org/pdf/cs/0503084v1,2024-04-28,
cs/0503087v4,"Dynamic Simulation of Construction Machinery: Towards an Operator Model","In dynamic simulation of complete wheel loaders, one interesting aspect, specific for the working task, is the momentary power distribution between drive train and hydraulics, which is balanced by the operator.   This paper presents the initial results to a simulation model of a human operator. Rather than letting the operator model follow a predefined path with control inputs at given points, it follows a collection of general rules that together describe the machine's working cycle in a generic way. The advantage of this is that the working task description and the operator model itself are independent of the machine's technical parameters. Complete sub-system characteristics can thus be changed without compromising the relevance and validity of the simulation. Ultimately, this can be used to assess a machine's total performance, fuel efficiency and operability already in the concept phase of the product development process.",2005-03-30T20:52:11Z,http://arxiv.org/pdf/cs/0503087v4,2024-04-28,
cs/0505001v1,"Modelling investment in artificial stock markets: Analytical and Numerical Results","In this article we study the behavior of a group of economic agents in the context of cooperative game theory, interacting according to rules based on the Potts Model with suitable modifications. Each agent can be thought of as belonging to a chain, where agents can only interact with their nearest neighbors (periodic boundary conditions are imposed). Each agent can invest an amount &#963;_{i}=0,...,q-1. Using the transfer matrix method we study analytically, among other things, the behavior of the investment as a function of a control parameter (denoted &#946;) for the cases q=2 and 3. For q>3 numerical evaluation of eigenvalues and high precision numerical derivatives are used in order to assess this information.",2005-04-29T20:38:42Z,http://arxiv.org/pdf/cs/0505001v1,2024-04-28,
cs/0506033v4,"An Event-driven Operator Model for Dynamic Simulation of Construction Machinery","Prediction and optimisation of a wheel loader's dynamic behaviour is a challenge due to tightly coupled, non-linear subsystems of different technical domains. Furthermore, a simulation regarding performance, efficiency, and operability cannot be limited to the machine itself, but has to include operator, environment, and work task. This paper presents some results of our approach to an event-driven simulation model of a human operator. Describing the task and the operator model independently of the machine's technical parameters, gives the possibility to change whole sub-system characteristics without compromising the relevance and validity of the simulation.",2005-06-10T10:35:14Z,http://arxiv.org/pdf/cs/0506033v4,2024-04-28,
cs/0507025v1,"Comparison of Resampling Schemes for Particle Filtering","This contribution is devoted to the comparison of various resampling approaches that have been proposed in the literature on particle filtering. It is first shown using simple arguments that the so-called residual and stratified methods do yield an improvement over the basic multinomial resampling approach. A simple counter-example showing that this property does not hold true for systematic resampling is given. Finally, some results on the large-sample behavior of the simple bootstrap filter algorithm are given. In particular, a central limit theorem is established for the case where resampling is performed using the residual approach.",2005-07-08T15:14:51Z,http://arxiv.org/pdf/cs/0507025v1,2024-04-28,
cs/0507055v2,"ReacProc: A Tool to Process Reactions Describing Particle Interactions","ReacProc is a program written in C/C++ programming language which can be used (1) to check out of reactions describing particles interactions against conservation laws and (2) to reduce input reaction into some canonical form. A table with particles properties is available within ReacProc package.",2005-07-21T14:17:47Z,http://arxiv.org/pdf/cs/0507055v2,2024-04-28,
cs/0508102v1,"Investigations of Process Damping Forces in Metal Cutting","Using finite element software developed for metal cutting by Third Wave Systems we investigate the forces involved in chatter, a self-sustained oscillation of the cutting tool. The phenomena is decomposed into a vibrating tool cutting a flat surface work piece, and motionless tool cutting a work piece with a wavy surface. While cutting the wavy surface, the shearplane was seen to oscillate in advance of the oscillation of the depth of cut, as were the cutting, thrust, and shear plane forces. The vibrating tool was used to investigate process damping through the interaction of the relief face of the tool and the workpiece. Crushing forces are isolated and compared to the contact length between the tool and workpiece. We found that the wavelength dependence of the forces depended on the relative size of the wavelength to the length of the relief face of the tool. The results indicate that the damping force from crushing will be proportional to the cutting speed for short tools, and inversely proportional for long tools.",2005-08-23T19:52:38Z,http://arxiv.org/pdf/cs/0508102v1,2024-04-28,
cs/0509002v1,"Component Based Programming in Scientific Computing: The Viable Approach","Computational scientists are facing a new era where the old ways of developing and reusing code have to be left behind and a few daring steps are to be made towards new horizons. The present work analyzes the needs that drive this change, the factors that contribute to the inertia of the community and slow the transition, the status and perspective of present attempts, the principle, practical and technical problems that are to be addressed in the short and long run.",2005-08-31T21:57:04Z,http://arxiv.org/pdf/cs/0509002v1,2024-04-28,
cs/0509003v1,"COMODI: Architecture for a Component-Based Scientific Computing System","The COmputational MODule Integrator (COMODI) is an initiative aiming at a component based framework, component developer tool and component repository for scientific computing. We identify the main ingredients to a solution that would be sufficiently appealing to scientists and engineers to consider alternatives to their deeply rooted programming traditions. The overall structure of the complete solution is sketched with special emphasis on the Component Developer Tool standing at the basis of COMODI.",2005-08-31T22:48:40Z,http://arxiv.org/pdf/cs/0509003v1,2024-04-28,
cs/0509012v5,"Kriging Scenario For Capital Markets","An introduction to numerical statistics.",2005-09-05T08:04:06Z,http://arxiv.org/pdf/cs/0509012v5,2024-04-28,
cs/0510027v2,"A Market Test for the Positivity of Arrow-Debreu Prices","We derive tractable necessary and sufficient conditions for the absence of buy-and-hold arbitrage opportunities in a perfectly liquid, one period market. We formulate the positivity of Arrow-Debreu prices as a generalized moment problem to show that this no arbitrage condition is equivalent to the positive semidefiniteness of matrices formed by the market price of tradeable securities and their products. We apply this result to a market with multiple assets and basket call options.",2005-10-11T13:40:17Z,http://arxiv.org/pdf/cs/0510027v2,2024-04-28,
cs/0511024v1,"Heat kernel expansion for a family of stochastic volatility models : delta-geometry","In this paper, we study a family of stochastic volatility processes; this family features a mean reversion term for the volatility and a double CEV-like exponent that generalizes SABR and Heston's models. We derive approximated closed form formulas for the digital prices, the local and implied volatilities. Our formulas are efficient for small maturities.   Our method is based on differential geometry, especially small time diffusions on riemanian spaces. This geometrical point of view can be extended to other processes, and is very accurate to produce variate smiles for small maturities and small moneyness.",2005-11-04T18:31:49Z,http://arxiv.org/pdf/cs/0511024v1,2024-04-28,
cs/0604090v1,"Simplicial models of social aggregation I","This paper presents the foundational ideas for a new way of modeling social aggregation. Traditional approaches have been using network theory, and the theory of random networks. Under that paradigm, every social agent is represented by a node, and every social interaction is represented by a segment connecting two nodes. Early work in family interactions, as well as more recent work in the study of terrorist organizations, shows that network modeling may be insufficient to describe the complexity of human social structures. Specifically, network theory does not seem to have enough flexibility to represent higher order aggregations, where several agents interact as a group, rather than as a collection of pairs. The model we present here uses a well established mathematical theory, the theory of simplicial complexes, to address this complex issue prevalent in interpersonal and intergroup communication. The theory enables us to provide a richer graphical representation of social interactions, and to determine quantitative mechanisms to describe the robustness of a social structure. We also propose a methodology to create random simplicial complexes, with the purpose of providing a new method to simulate computationally the creation and disgregation of social structures. Finally, we propose several measures which could be taken and observed in order to describe and study an actual social aggregation occurring in interpersonal and intergroup contexts.",2006-04-23T19:28:07Z,http://arxiv.org/pdf/cs/0604090v1,2024-04-28,
cs/0607018v2,"Feynman Checkerboard as a Model of Discrete Space-Time","In 1965, Feynman wrote of using a lattice containing one dimension of space and one dimension of time to derive aspects of quantum mechanics. Instead of summing the behavior of all possible paths as he did, this paper will consider the motion of single particles within this discrete Space-Time lattice, sometimes called Feynman's Checkerboard. This empirical approach yielded several predicted emergent properties for a discrete Space-Time lattice, one of which is novel and testable.",2006-07-06T16:35:38Z,http://arxiv.org/pdf/cs/0607018v2,2024-04-28,
cs/0607083v1,"Mathematical Modelling of the Thermal Accumulation in Hot Water Solar Systems","Mathematical modelling and defining useful recommendations for construction and regimes of exploitation for hot water solar installation with thermal stratification is the main purpose of this work. A special experimental solar module for hot water was build and equipped with sufficient measure apparatus. The main concept of investigation is to optimise the stratified regime of thermal accumulation and constructive parameters of heat exchange equipment (heat serpentine in tank). Accumulation and heat exchange processes were investigated by theoretical end experimental means. Special mathematical model was composed to simulate the energy transfer in stratified tank. Computer program was developed to solve mathematical equations for thermal accumulation and energy exchange. Extensive numerical and experimental tests were carried out. A good correspondence between theoretical and experimental data was arrived. Keywords: Mathematical modelling, accumulation",2006-07-18T08:07:48Z,http://arxiv.org/pdf/cs/0607083v1,2024-04-28,
cs/0607091v1,"Finite element method for thermal analysis of concentrating solar receivers","Application of finite element method and heat conductivity transfer model for calculation of temperature distribution in receiver for dish-Stirling concentrating solar system is described. The method yields discretized equations that are entirely local to the elements and provides complete geometric flexibility. A computer program solving the finite element method problem is created and great number of numerical experiments is carried out. Illustrative numerical results are given for an array of triangular elements in receiver for dish-Stirling system.",2006-07-19T06:58:37Z,http://arxiv.org/pdf/cs/0607091v1,2024-04-28,
cs/0609087v1,"A comparative analysis of the geometrical surface texture of a real and virtual model of a tooth flank of a cylindrical gear","The paper presents the methodology of modelling tooth flanks of cylindrical gears in the Cad environment. The modelling consists in a computer simulation of gear generation. A model of tooth flanks is an envelope curve of a family of envelopes that originate from the rolling motion of a solid tool model in relation to a solid model of the cylindrical gear. The surface stereometry and topography of the tooth flanks, hobbed and chiselled by Fellows method, are compared to their numerical models. Metrological measurements of the real gears were carried out using a coordinated measuring machine and a two - and a three-dimensional profilometer. A computer simulation of the gear generation was performed in the Mechanical Desktop environment.",2006-09-15T14:05:43Z,http://arxiv.org/pdf/cs/0609087v1,2024-04-28,
cs/0609145v1,"A Semidefinite Relaxation for Air Traffic Flow Scheduling","We first formulate the problem of optimally scheduling air traffic low with sector capacity constraints as a mixed integer linear program. We then use semidefinite relaxation techniques to form a convex relaxation of that problem. Finally, we present a randomization algorithm to further improve the quality of the solution. Because of the specific structure of the air traffic flow problem, the relaxation has a single semidefinite constraint of size dn where d is the maximum delay and n the number of flights.",2006-09-26T15:34:40Z,http://arxiv.org/pdf/cs/0609145v1,2024-04-28,
cs/0611044v1,"Protection of the information in a complex CAD system of renovation of industrial firms","The threats to security of the information originating owing to involuntary operations of the users of a CAD, and methods of its protection implemented in a complex CAD system of renovation of firms are considered: rollback, autosave, automatic backup copying and electronic subscript. The specificity of a complex CAD is reflected in necessity of rollback and autosave both of the draw and the parametric representations of its parts, which are the information models of the problem-oriented extensions of the CAD",2006-11-10T07:43:33Z,http://arxiv.org/pdf/cs/0611044v1,2024-04-28,
cs/0611045v1,"The evolution of the parametric models of drawings (modules) in the enterprises reconstruction CAD system","Progressing methods of drawings creating automation is discussed on the basis of so-called modules containing parametric representation of a part of the drawing and the geometrical elements. The stages of evolution of modular technology of automation of engineering are describing alternatives of applying of moduluss for simple association of elements of the drawing without parametric representation with an opportunity of its commenting, for graphic symbols creating in the schemas of automation and drawings of pipelines, for storage of the specific properties of elements, for development of the specialized parts of the project: the axonometric schemas, profiles of outboard pipe networks etc.",2006-11-10T07:56:19Z,http://arxiv.org/pdf/cs/0611045v1,2024-04-28,
cs/0611083v1,"Environment of development of the programs of parametric creating of the drawings in CAD-system of renovation of the enterprises","The main ideas, data structures, structure and realization of operations with them in environment of development of the programs of parametric creating of the drawings are considered for the needs of the automated design engineering system of renovation of the enterprises. The example of such program and example of application of this environment for creating the drawing of the base for equipment in CAD-system TechnoCAD GlassX are presented",2006-11-17T08:27:45Z,http://arxiv.org/pdf/cs/0611083v1,2024-04-28,
cs/0611132v1,"The specifications making in complex CAD-system of renovation of the enterprises on the basis of modules in the drawing and electronic catalogues","The experience of automation of the specifications making of the projects of renovation of the industrial enterprises is described, being based on the special modules in the drawing containing the visible image and additional parameters, and electronic catalogues",2006-11-27T04:31:09Z,http://arxiv.org/pdf/cs/0611132v1,2024-04-28,
cs/0611133v1,"The modelling of the automation schemes of technological processes in CAD-system of renovation of the enterprises","According to the requirements of the Russian standards, the automation schemes are necessary practically in each project of renovation of industrial buildings and facilities, in which any technological processes are realized. The model representations of the automation schemes in CAD-system TechnoCAD GlassX are described. The models follow a principle ""to exclude a repeated input operations""",2006-11-27T04:39:09Z,http://arxiv.org/pdf/cs/0611133v1,2024-04-28,
cs/0701119v1,"The framework for simulation of dynamics of mechanical aggregates","A framework for simulation of dynamics of mechanical aggregates has been developed. This framework enables us to build model of aggregate from models of its parts. Framework is a part of universal framework for science and engineering.",2007-01-19T12:18:14Z,http://arxiv.org/pdf/cs/0701119v1,2024-04-28,
cs/0703068v2,"Option Valuation using Fourier Space Time Stepping","It is well known that the Black-Scholes-Merton model suffers from several deficiencies. Jump-diffusion and Levy models have been widely used to partially alleviate some of the biases inherent in this classical model. Unfortunately, the resulting pricing problem requires solving a more difficult partial-integro differential equation (PIDE) and although several approaches for solving the PIDE have been suggested in the literature, none are entirely satisfactory. All treat the integral and diffusive terms asymmetrically and are difficult to extend to higher dimensions. We present a new, efficient algorithm, based on transform methods, which symmetrically treats the diffusive and integrals terms, is applicable to a wide class of path-dependent options (such as Bermudan, barrier, and shout options) and options on multiple assets, and naturally extends to regime-switching Levy models. We present a concise study of the precision and convergence properties of our algorithm for several classes of options and Levy models and demonstrate that the algorithm is second-order in space and first-order in time for path-dependent options.",2007-03-14T19:48:42Z,http://arxiv.org/pdf/cs/0703068v2,2024-04-28,
0705.0150v2,"Comparison of Discrete and Continuous Wavelet Transforms","In this paper we outline several points of view on the interplay between discrete and continuous wavelet transforms; stressing both pure and applied aspects of both. We outline some new links between the two transform technologies based on the theory of representations of generators and relations. By this we mean a finite system of generators which are represented by operators in Hilbert space. We further outline how these representations yield sub-band filter banks for signal and image processing algorithms.",2007-05-01T18:24:52Z,http://arxiv.org/pdf/0705.0150v2,2024-04-28,
0705.1390v1,"Machine and Component Residual Life Estimation through the Application of Neural Networks","This paper concerns the use of neural networks for predicting the residual life of machines and components. In addition, the advantage of using condition-monitoring data to enhance the predictive capability of these neural networks was also investigated. A number of neural network variations were trained and tested with the data of two different reliability-related datasets. The first dataset represents the renewal case where the failed unit is repaired and restored to a good-as-new condition. Data was collected in the laboratory by subjecting a series of similar test pieces to fatigue loading with a hydraulic actuator. The average prediction error of the various neural networks being compared varied from 431 to 841 seconds on this dataset, where test pieces had a characteristic life of 8,971 seconds. The second dataset was collected from a group of pumps used to circulate a water and magnetite solution within a plant. The data therefore originated from a repaired system affected by reliability degradation. When optimized, the multi-layer perceptron neural networks trained with the Levenberg-Marquardt algorithm and the general regression neural network produced a sum-of-squares error within 11.1% of each other. The potential for using neural networks for residual life prediction and the advantage of incorporating condition-based data into the model were proven for both examples.",2007-05-10T05:52:22Z,http://arxiv.org/pdf/0705.1390v1,2024-04-28,
0705.1672v1,"Principal Component Analysis and Automatic Relevance Determination in Damage Identification","This paper compares two neural network input selection schemes, the Principal Component Analysis (PCA) and the Automatic Relevance Determination (ARD) based on Mac-Kay's evidence framework. The PCA takes all the input data and projects it onto a lower dimension space, thereby reduc-ing the dimension of the input space. This input reduction method often results with parameters that have significant influence on the dynamics of the data being diluted by those that do not influence the dynamics of the data. The ARD selects the most relevant input parameters and discards those that do not contribute significantly to the dynamics of the data being modelled. The ARD sometimes results with important input parameters being discarded thereby compromising the dynamics of the data. The PCA and ARD methods are implemented together with a Multi-Layer-Perceptron (MLP) network for fault identification in structures and the performance of the two methods is as-sessed. It is observed that ARD and PCA give similar accu-racy levels when used as input-selection schemes. There-fore, the choice of input-selection scheme is dependent on the nature of the data being processed.",2007-05-11T15:35:22Z,http://arxiv.org/pdf/0705.1672v1,2024-04-28,
0705.1759v1,"Finite Element Model Updating Using Response Surface Method","This paper proposes the response surface method for finite element model updating. The response surface method is implemented by approximating the finite element model surface response equation by a multi-layer perceptron. The updated parameters of the finite element model were calculated using genetic algorithm by optimizing the surface response equation. The proposed method was compared to the existing methods that use simulated annealing or genetic algorithm together with a full finite element model for finite element model updating. The proposed method was tested on an unsymmetri-cal H-shaped structure. It was observed that the proposed method gave the updated natural frequen-cies and mode shapes that were of the same order of accuracy as those given by simulated annealing and genetic algorithm. Furthermore, it was observed that the response surface method achieved these results at a computational speed that was more than 2.5 times as fast as the genetic algorithm and a full finite element model and 24 times faster than the simulated annealing.",2007-05-12T10:25:22Z,http://arxiv.org/pdf/0705.1759v1,2024-04-28,
0705.2604v1,"Computational Intelligence for Condition Monitoring","Condition monitoring techniques are described in this chapter. Two aspects of condition monitoring process are considered: (1) feature extraction; and (2) condition classification. Feature extraction methods described and implemented are fractals, Kurtosis and Mel-frequency Cepstral Coefficients. Classification methods described and implemented are support vector machines (SVM), hidden Markov models (HMM), Gaussian mixture models (GMM) and extension neural networks (ENN). The effectiveness of these features were tested using SVM, HMM, GMM and ENN on condition monitoring of bearings and are found to give good results.",2007-05-17T21:20:58Z,http://arxiv.org/pdf/0705.2604v1,2024-04-28,
0706.1119v2,"Cointegration of the Daily Electric Power System Load and the Weather","The paper makes a thermal predictive analysis of the electric power system security for a day ahead. This predictive analysis is set as a thermal computation of the expected security. This computation is obtained by cointegrating the daily electric power systen load and the weather, by finding the daily electric power system thermodynamics and by introducing tests for this thermodynamics. The predictive analysis made shows the electricity consumers' wisdom.",2007-06-08T07:04:24Z,http://arxiv.org/pdf/0706.1119v2,2024-04-28,
0706.2331v5,"Pricing American Options for Jump Diffusions by Iterating Optimal Stopping Problems for Diffusions","We approximate the price of the American put for jump diffusions by a sequence of functions, which are computed iteratively. This sequence converges to the price function uniformly and exponentially fast. Each element of the approximating sequence solves an optimal stopping problem for geometric Brownian motion, and can be numerically computed using the classical finite difference methods. We prove the convergence of this numerical scheme and present examples to illustrate its performance.",2007-06-15T16:43:14Z,http://arxiv.org/pdf/0706.2331v5,2024-04-28,
0707.0181v1,"Location and Spectral Estimation of Weak Wave Packets on Noise Background","The method of location and spectral estimation of weak signals on a noise background is being considered. The method is based on the optimized on order and noise dispersion autoregressive model of a sought signal. A new approach of model order determination is being offered. Available estimation of the noise dispersion is close to the real one. The optimized model allows to define function of empirical data spectral and dynamic features changes. The analysis of the signal as dynamic invariant in respect of the linear shift transformation yields the function of model consistency. Use of these both functions enables to detect short-time and nonstationary wave packets at signal to noise ratio as from -20 dB and above.",2007-07-02T09:47:33Z,http://arxiv.org/pdf/0707.0181v1,2024-04-28,
0707.0336v2,"Pricing Options on Defaultable Stocks","In this note, we develop stock option price approximations for a model which takes both the risk o default and the stochastic volatility into account. We also let the intensity of defaults be influenced by the volatility. We show that it might be possible to infer the risk neutral default intensity from the stock option prices. Our option price approximation has a rich implied volatility surface structure and fits the data implied volatility well. Our calibration exercise shows that an effective hazard rate from bonds issued by a company can be used to explain the implied volatility skew of the implied volatility of the option prices issued by the same company.",2007-07-03T03:28:35Z,http://arxiv.org/pdf/0707.0336v2,2024-04-28,
0707.2432v7,"Pricing Asian Options for Jump Diffusions","We construct a sequence of functions that uniformly converge (on compact sets) to the price of Asian option, which is written on a stock whose dynamics follows a jump diffusion, exponentially fast. Each of the element in this sequence solves a parabolic partial differen- tial equation (not an integro-differential equation). As a result we obtain a fast numerical approximation scheme whose accuracy versus speed characteristics can be controlled. We analyze the performance of our numerical algorithm on several examples.",2007-07-17T04:55:18Z,http://arxiv.org/pdf/0707.2432v7,2024-04-28,
0708.3048v2,"Identifying Small Mean Reverting Portfolios","Given multivariate time series, we study the problem of forming portfolios with maximum mean reversion while constraining the number of assets in these portfolios. We show that it can be formulated as a sparse canonical correlation analysis and study various algorithms to solve the corresponding sparse generalized eigenvalue problems. After discussing penalized parameter estimation procedures, we study the sparsity versus predictability tradeoff and the impact of predictability in various markets.",2007-08-22T16:25:17Z,http://arxiv.org/pdf/0708.3048v2,2024-04-28,
0708.3465v1,"An Early Warning System for Bankruptcy Prediction: lessons from the Venezuelan Bank Crisis","During 1993-94 Venezuela experienced a severe banking crisis which ended up with 18 commercial banks intervened by the government. Here we develop an early warning system for detecting credit related bankruptcy through discriminant functions developed on financial and macroeconomic data predating the crisis. A robustness test performed on these functions shows high precision in error estimation. The model calibrated on pre-crisis data could detect abnormal financial tension in the late Banco Capital many months before it was intervened and liquidated.",2007-08-26T05:33:41Z,http://arxiv.org/pdf/0708.3465v1,2024-04-28,
0710.5512v1,"Risk Minimization and Optimal Derivative Design in a Principal Agent Game","We consider the problem of Adverse Selection and optimal derivative design within a Principal-Agent framework. The principal's income is exposed to non-hedgeable risk factors arising, for instance, from weather or climate phenomena. She evaluates her risk using a coherent and law invariant risk measure and tries minimize her exposure by selling derivative securities on her income to individual agents. The agents have mean-variance preferences with heterogeneous risk aversion coefficients. An agent's degree of risk aversion is private information and hidden to the principal who only knows the overall distribution. We show that the principal's risk minimization problem has a solution and illustrate the effects of risk transfer on her income by means of two specific examples. Our model extends earlier work of Barrieu and El Karoui (2005) and Carlier, Ekeland and Touzi (2007).",2007-10-29T20:00:15Z,http://arxiv.org/pdf/0710.5512v1,2024-04-28,
0711.2116v1,"A numerical approach for 3D manufacturing tolerances synthesis","Making a product conform to the functional requirements indicated by the customer suppose to be able to manage the manufacturing process chosen to realise the parts. A simulation step is generally performed to verify that the expected generated deviations fit with these requirements. It is then necessary to assess the actual deviations of the process in progress. This is usually done by the verification of the conformity of the workpiece to manufacturing tolerances at the end of each set-up. It is thus necessary to determine these manufacturing tolerances. This step is called ""manufacturing tolerance synthesis"". In this paper, a numerical method is proposed to perform 3D manufacturing tolerances synthesis. This method uses the result of the numerical analysis of tolerances to determine influent mall displacement of surfaces. These displacements are described by small displacements torsors. An algorithm is then proposed to determine suitable ISO manufacturing tolerances.",2007-11-14T06:21:17Z,http://arxiv.org/pdf/0711.2116v1,2024-04-28,
0712.3617v2,"A Unified Framework for Pricing Credit and Equity Derivatives","We propose a model which can be jointly calibrated to the corporate bond term structure and equity option volatility surface of the same company. Our purpose is to obtain explicit bond and equity option pricing formulas that can be calibrated to find a risk neutral model that matches a set of observed market prices. This risk neutral model can then be used to price more exotic, illiquid or over-the-counter derivatives. We observe that the model implied credit default swap (CDS) spread matches the market CDS spread and that our model produces a very desirable CDS spread term structure. This is observation is worth noticing since without calibrating any parameter to the CDS spread data, it is matched by the CDS spread that our model generates using the available information from the equity options and corporate bond markets. We also observe that our model matches the equity option implied volatility surface well since we properly account for the default risk premium in the implied volatility surface. We demonstrate the importance of accounting for the default risk and stochastic interest rate in equity option pricing by comparing our results to Fouque, Papanicolaou, Sircar and Solna (2003), which only accounts for stochastic volatility.",2007-12-21T02:53:38Z,http://arxiv.org/pdf/0712.3617v2,2024-04-28,
0804.1187v1,"Méthode de calcul du rayonnement acoustique de structures complexes","In the automotive industry, predicting noise during design cycle is a necessary step. Well-known methods exist to answer this issue in low frequency domain. Among these, Finite Element Methods, adapted to closed domains, are quite easy to implement whereas Boundary Element Methods are more adapted to infinite domains, but may induce singularity problems. In this article, the described method, the SDM, allows to use both methods in their best application domain. A new method is also presented to solve the SDM exterior problem. Instead of using Boundary Element Methods, an original use of Finite Elements is made. Efficiency of this new version of the Substructure Deletion Method is discussed.",2008-04-08T06:24:49Z,http://arxiv.org/pdf/0804.1187v1,2024-04-28,
0807.5120v2,"Accelerated Option Pricing in Multiple Scenarios","This paper covers a massive acceleration of Monte-Carlo based pricing method for financial products and financial derivatives. The method is applicable in risk management settings, where a financial product has to be priced under a number of potential future scenarios. Instead of starting a separate nested Monte Carlo simulation for each scenario under consideration, the new method covers the utilization of very few representative nested simulations and estimating the product prices at each scenario by a smoothing method based on the state-space. This smoothing technique can be e.g. non-parametric regression or kernel smoothing.",2008-07-31T17:40:55Z,http://arxiv.org/pdf/0807.5120v2,2024-04-28,
0812.4523v1,"System Theoretic Viewpoint on Modeling of Complex Systems: Design, Synthesis, Simulation, and Control","We consider the basic features of complex dynamic and control systems, including systems having hierarchical structure. Special attention is paid to the problems of design and synthesis of complex systems and control models, and to the development of simulation techniques and systems. A model of complex system is proposed and briefly analyzed.",2008-12-24T12:07:48Z,http://arxiv.org/pdf/0812.4523v1,2024-04-28,
0902.0763v1,"Genetic algorithm based optimization and post optimality analysis of multi-pass face milling","This paper presents an optimization technique for the multi-pass face milling process. Genetic algorithm (GA) is used to obtain the optimum cutting parameters by minimizing the unit production cost for a given amount of material removal. Cutting speed, feed and depth of cut for the finish and rough passes are the cutting parameters. An equal depth of cut for roughing passes has been considered. A lookup table containing the feasible combinations of depth of cut in finish and rough passes is generated so as to reduce the number of variables by one. The resulting mixed integer nonlinear optimization problem is solved in a single step using GA. The entire technique is demonstrated in a case study. Post optimality analysis of the example problem is done to develop a strategy for optimizing without running GA again for different values of total depth of cut.",2009-02-04T18:29:21Z,http://arxiv.org/pdf/0902.0763v1,2024-04-28,
0902.3541v1,"System approach to synthesis, modeling and control of complex dynamical systems","We consider the basic features of complex dynamical and control systems. Special attention is paid to the problems of synthesis of dynamical models of complex systems, construction of efficient control models, and to the development of simulation techniques. We propose an approach to the synthesis of dynamic models of complex systems that integrates expert knowledge with the process of modeling. A set-theoretic model of complex system is defined and briefly analyzed. A mathematical model of complex dynamical system with control, based on aggregate description, is also proposed. The structure of the model is described, and architecture of computer simulation system is presented, requirements to and components of computer simulation systems are analyzed.",2009-02-20T10:28:14Z,http://arxiv.org/pdf/0902.3541v1,2024-04-28,
0908.2578v1,"New method to characterize a machining system: application in turning","Many studies simulates the machining process by using a single degree of freedom spring-mass sytem to model the tool stiffness, or the workpiece stiffness, or the unit tool-workpiece stiffness in modelings 2D. Others impose the tool action, or use more or less complex modelings of the efforts applied by the tool taking account the tool geometry. Thus, all these models remain two-dimensional or sometimes partially three-dimensional. This paper aims at developing an experimental method allowing to determine accurately the real three-dimensional behaviour of a machining system (machine tool, cutting tool, tool-holder and associated system of force metrology six-component dynamometer). In the work-space model of machining, a new experimental procedure is implemented to determine the machining system elastic behaviour. An experimental study of machining system is presented. We propose a machining system static characterization. A decomposition in two distinct blocks of the system ""Workpiece-Tool-Machine"" is realized. The block Tool and the block Workpiece are studied and characterized separately by matrix stiffness and displacement (three translations and three rotations). The Castigliano's theory allows us to calculate the total stiffness matrix and the total displacement matrix. A stiffness center point and a plan of tool tip static displacement are presented in agreement with the turning machining dynamic model and especially during the self induced vibration. These results are necessary to have a good three-dimensional machining system dynamic characterization.",2009-08-18T14:29:42Z,http://arxiv.org/pdf/0908.2578v1,2024-04-28,
0909.0611v2,"Effects of Mechanical Coupling on the Dynamics of Balancing Tasks","Coupled human balancing tasks are investigated based on both pseudo-neural controllers characterized by time-delayed feedback with random gain and natural human balancing tasks. It is shown numerically that, compared to single balancing tasks, balancing tasks coupled by mechanical structures exhibit enhanced stability against balancing errors in terms of both amplitude and velocity and also improve the tracking ability of the controllers. We then perform an experiment in which numerical pseudo-neural controllers are replaced with natural human balancing tasks carried out using computer mice. The results reveal that the coupling structure generates asymmetric tracking abilities in subjects whose tracking abilities are nearly symmetric in their single balancing tasks.",2009-09-03T09:38:18Z,http://arxiv.org/pdf/0909.0611v2,2024-04-28,
0909.2336v2,"Two-Phase Flow in Heterogeneous Media","In this study, we investigate the appeared complexity of two-phase flow (air-water) in a heterogeneous soil where the supposed porous media is non-deformable media which is under the time-dependent gas pressure. After obtaining of governing equations and considering the capillary pressure-saturation and permeability functions, the evolution of the models unknown parameters were obtained. In this way, using COMSOL (FEMLAB) and fluid flow-script Module, the role of heterogeneity in intrinsic permeability was analysed. Also, the evolution of relative permeability of wetting and non-wetting fluid, capillary pressure and other parameters were elicited.",2009-09-12T13:51:59Z,http://arxiv.org/pdf/0909.2336v2,2024-04-28,
0911.3125v2,"A computational model of the bottlenose dolphin sonar: Feature-extracting method","The data describing a process of echo-image formation in bottlenose dolphin sonar perception were accumulated in our experimental explorations. These data were formalized mathematically and used in the computational model, comparative testing of which in echo-discrimination tasks revealed no less capabilities then those of bottlenose dolphins.",2009-11-16T19:24:59Z,http://arxiv.org/pdf/0911.3125v2,2024-04-28,
0911.4230v1,"Introduction to Bioinformatics","Bioinformatics is a new discipline that addresses the need to manage and interpret the data that in the past decade was massively generated by genomic research. This discipline represents the convergence of genomics, biotechnology and information technology, and encompasses analysis and interpretation of data, modeling of biological phenomena, and development of algorithms and statistics. This article presents an introduction to bioinformatics",2009-11-22T04:07:08Z,http://arxiv.org/pdf/0911.4230v1,2024-04-28,
0911.4987v1,"Drip and Mate Operations Acting in Test Tube Systems and Tissue-like P systems","The operations drip and mate considered in (mem)brane computing resemble the operations cut and recombination well known from DNA computing. We here consider sets of vesicles with multisets of objects on their outside membrane interacting by drip and mate in two different setups: in test tube systems, the vesicles may pass from one tube to another one provided they fulfill specific constraints; in tissue-like P systems, the vesicles are immediately passed to specified cells after having undergone a drip or mate operation. In both variants, computational completeness can be obtained, yet with different constraints for the drip and mate operations.",2009-11-26T00:33:04Z,http://arxiv.org/pdf/0911.4987v1,2024-04-28,
1001.3495v1,"Expert System Models in the Companies' Financial and Accounting Domain","The present paper is based on studying, analyzing and implementing the expert systems in the financial and accounting domain of the companies, describing the use method of the informational systems that can be used in the multi-national companies, public interest institutions, and medium and small dimension economical entities, in order to optimize the managerial decisions and render efficient the financial-accounting functionality. The purpose of this paper is aimed to identifying the economical exigencies of the entities, based on the already used accounting instruments and the management software that could consent the control of the economical processes and patrimonial assets.",2010-01-20T08:03:53Z,http://arxiv.org/pdf/1001.3495v1,2024-04-28,
1004.3571v1,"Computer Aided Design Modeling for Heterogeneous Objects","Heterogeneous object design is an active research area in recent years. The conventional CAD modeling approaches only provide geometry and topology of the object, but do not contain any information with regard to the materials of the object and so can not be used for the fabrication of heterogeneous objects (HO) through rapid prototyping. Current research focuses on computer-aided design issues in heterogeneous object design. A new CAD modeling approach is proposed to integrate the material information into geometric regions thus model the material distributions in the heterogeneous object. The gradient references are used to represent the complex geometry heterogeneous objects which have simultaneous geometry intricacies and accurate material distributions. The gradient references helps in flexible manipulability and control to heterogeneous objects, which guarantees the local control over gradient regions of developed heterogeneous objects. A systematic approach on data flow, processing, computer visualization, and slicing of heterogeneous objects for rapid prototyping is also presented.",2010-04-20T20:42:34Z,http://arxiv.org/pdf/1004.3571v1,2024-04-28,
1008.3551v1,"Inventory Allocation for Online Graphical Display Advertising","We discuss a multi-objective/goal programming model for the allocation of inventory of graphical advertisements. The model considers two types of campaigns: guaranteed delivery (GD), which are sold months in advance, and non-guaranteed delivery (NGD), which are sold using real-time auctions. We investigate various advertiser and publisher objectives such as (a) revenue from the sale of impressions, clicks and conversions, (b) future revenue from the sale of NGD inventory, and (c) ""fairness"" of allocation. While the first two objectives are monetary, the third is not. This combination of demand types and objectives leads to potentially many variations of our model, which we delineate and evaluate. Our experimental results, which are based on optimization runs using real data sets, demonstrate the effectiveness and flexibility of the proposed model.",2010-08-20T18:37:23Z,http://arxiv.org/pdf/1008.3551v1,2024-04-28,
1011.0250v1,"Delineation of Raw Plethysmograph using Wavelets for Mobile based Pulse Oximeters","The non-invasive pulse-oximeter is a crucial parameter in continuous monitoring systems. It plays a vital role from admission of the patient to surgeries with general anaesthesia. The paper proposes the application of wavelet transform to delineate the raw plethysmograph signals obtained from basic portable and mobile-powered electronic hardware. The paper primarily focuses on finding peaks and baseline from noisy infrared and red waveforms which are responsible for calculating heart-rate and oxygen saturation percentages.",2010-11-01T04:54:50Z,http://arxiv.org/pdf/1011.0250v1,2024-04-28,
1011.0279v1,"Mobile Based Secure Digital Wallet for Peer to Peer Payment System","E-commerce in today's conditions has the highest dependence on network infrastructure of banking. However, when the possibility of communicating with the Banking network is not provided, business activities will suffer. This paper proposes a new approach of digital wallet based on mobile devices without the need to exchange physical money or communicate with banking network. A digital wallet is a software component that allows a user to make an electronic payment in cash (such as a credit card or a digital coin), and hides the low-level details of executing the payment protocol that is used to make the payment. The main features of proposed architecture are secure awareness, fault tolerance, and infrastructure-less protocol.",2010-11-01T09:55:23Z,http://arxiv.org/pdf/1011.0279v1,2024-04-28,
1011.0488v1,"Measurable Stochastics for Brane Calculus","We give a stochastic extension of the Brane Calculus, along the lines of recent work by Cardelli and Mardare. In this presentation, the semantics of a Brane process is a measure of the stochastic distribution of possible derivations. To this end, we first introduce a labelled transition system for Brane Calculus, proving its adequacy w.r.t. the usual reduction semantics. Then, brane systems are presented as Markov processes over the measurable space generated by terms up-to syntactic congruence, and where the measures are indexed by the actions of this new LTS. Finally, we provide a SOS presentation of this stochastic semantics, which is compositional and syntax-driven.",2010-11-02T01:29:09Z,http://arxiv.org/pdf/1011.0488v1,2024-04-28,
1011.0496v1,"Lumpability Abstractions of Rule-based Systems","The induction of a signaling pathway is characterized by transient complex formation and mutual posttranslational modification of proteins. To faithfully capture this combinatorial process in a mathematical model is an important challenge in systems biology. Exploiting the limited context on which most binding and modification events are conditioned, attempts have been made to reduce the combinatorial complexity by quotienting the reachable set of molecular species, into species aggregates while preserving the deterministic semantics of the thermodynamic limit. Recently we proposed a quotienting that also preserves the stochastic semantics and that is complete in the sense that the semantics of individual species can be recovered from the aggregate semantics. In this paper we prove that this quotienting yields a sufficient condition for weak lumpability and that it gives rise to a backward Markov bisimulation between the original and aggregated transition system. We illustrate the framework on a case study of the EGF/insulin receptor crosstalk.",2010-11-02T01:30:06Z,http://arxiv.org/pdf/1011.0496v1,2024-04-28,
1011.0498v1,"Qualitative modelling and analysis of regulations in multi-cellular systems using Petri nets and topological collections","In this paper, we aim at modelling and analyzing the regulation processes in multi-cellular biological systems, in particular tissues.   The modelling framework is based on interconnected logical regulatory networks a la Rene Thomas equipped with information about their spatial relationships. The semantics of such models is expressed through colored Petri nets to implement regulation rules, combined with topological collections to implement the spatial information.   Some constraints are put on the the representation of spatial information in order to preserve the possibility of an enumerative and exhaustive state space exploration.   This paper presents the modelling framework, its semantics, as well as a prototype implementation that allowed preliminary experimentation on some applications.",2010-11-02T01:37:03Z,http://arxiv.org/pdf/1011.0498v1,2024-04-28,
1011.5481v1,"Using Evolution Strategy with Meta-models for Well Placement Optimization","Optimum implementation of non-conventional wells allows us to increase considerably hydrocarbon recovery. By considering the high drilling cost and the potential improvement in well productivity, well placement decision is an important issue in field development. Considering complex reservoir geology and high reservoir heterogeneities, stochastic optimization methods are the most suitable approaches for optimum well placement. This paper proposes an optimization methodology to determine optimal well location and trajectory based upon the Covariance Matrix Adaptation - Evolution Strategy (CMA-ES) which is a variant of Evolution Strategies recognized as one of the most powerful derivative-free optimizers for continuous optimization. To improve the optimization procedure, two new techniques are investigated: (1). Adaptive penalization with rejection is developed to handle well placement constraints. (2). A meta-model, based on locally weighted regression, is incorporated into CMA-ES using an approximate ranking procedure. Therefore, we can reduce the number of reservoir simulations, which are computationally expensive. Several examples are presented. Our new approach is compared with a Genetic Algorithm incorporating the Genocop III technique. It is shown that our approach outperforms the genetic algorithm: it leads in general to both a higher NPV and a significant reduction of the number of reservoir simulations.",2010-11-24T20:08:30Z,http://arxiv.org/pdf/1011.5481v1,2024-04-28,
1012.4374v1,"Régularisation et optimisation pour l'imagerie sismique des fondations de pylônes","This research report summarizes the progress of work carried out jointly by the IRCCyN and the \'Ecole Polytechnique de Montr\'eal about the resolution of the inverse problem for the seismic imaging of transmission overhead line structure foundations. Several methods aimed at mapping the underground medium are considered. More particularly, we focus on methods based on a bilinear formulation of the forward problem on one hand (CSI, modified gradient, etc.) and on methods based on a ""primal"" formulation on the other hand. The performances of these methods are compared using synthetic data. This work was partially funded by RTE (R\'eseau de Transport d'\'Electricit\'e), which has initiated the project, and was carried out in collaboration with EDF R&D (\'Electricit\'e de France - Recherche et D\'eveloppement).",2010-12-20T15:56:28Z,http://arxiv.org/pdf/1012.4374v1,2024-04-28,
1012.5074v1,"Power-Rate Allocation in DS/CDMA Based on Discretized Verhulst Equilibrium","This paper proposes to extend the discrete Verhulst power equilibrium approach, previously suggested in [1], to the power-rate optimal allocation problem. Multirate users associated to different types of traffic are aggregated to distinct user' classes, with the assurance of minimum rate allocation per user and QoS. Herein, Verhulst power allocation algorithm was adapted to the single-input-single-output DS/CDMA jointly power-rate control problem. The analysis was carried out taking into account the convergence time, quality of solution, in terms of the normalized squared error (NSE), when compared with the analytical solution based on interference matrix inverse, and computational complexity. Numerical results demonstrate the validity of the proposed resource allocation methodology.",2010-12-22T19:09:12Z,http://arxiv.org/pdf/1012.5074v1,2024-04-28,
1103.2447v1,"Mini-step Strategy for Transient Analysis","Domain decomposition methods are widely used to solve sparse linear systems from scientific problems, but they are not suited to solve sparse linear systems extracted from integrated circuits. The reason is that the sparse linear system of integrated circuits may be non-diagonal-dominant, and domain decomposition method might be unconvergent for these non-diagonal-dominant matrices. In this paper, we propose a mini-step strategy to do the circuit transient analysis. Different from the traditional large-step approach, this strategy is able to generate diagonal-dominant sparse linear systems. As a result, preconditioned domain decomposition methods can be used to simulate the large integrated circuits on the supercomputers and clouds.",2011-03-12T14:30:48Z,http://arxiv.org/pdf/1103.2447v1,2024-04-28,
1103.3391v1,"An Integer Linear Programming Model for the Radiotherapy Treatment Scheduling Problem","Radiotherapy represents an important phase of treatment for a large number of cancer patients. It is essential that resources used to deliver this treatment are employed effectively. This paper presents a new integer linear programming model for real-world radiotherapy treatment scheduling and analyses the effectiveness of using this model on a daily basis in a hospital. Experiments are conducted varying the days on which schedules can be created. Results obtained using real-world data from the Nottingham University Hospitals NHS Trust, UK, are presented and show how the proposed model can be used with different policies in order to achieve good quality schedules.",2011-03-17T12:27:10Z,http://arxiv.org/pdf/1103.3391v1,2024-04-28,
1103.4720v2,"Computer Modelling of 3D Geological Surface","The geological surveying presently uses methods and tools for the computer modeling of 3D-structures of the geographical subsurface and geotechnical characterization as well as the application of geoinformation systems for management and analysis of spatial data, and their cartographic presentation. The objectives of this paper are to present a 3D geological surface model of Latur district in Maharashtra state of India. This study is undertaken through the several processes which are discussed in this paper to generate and visualize the automated 3D geological surface model of a projected area.",2011-03-24T10:31:44Z,http://arxiv.org/pdf/1103.4720v2,2024-04-28,
1105.5939v1,"Airborne TDMA for High Throughput and Fast Weather Conditions Notification","As air traffic grows significantly, aircraft accidents increase. Many aviation accidents could be prevented if the precise aircraft positions and weather conditions on the aircraft's route were known. Existing studies propose determining the precise aircraft positions via a VHF channel with an air-to-air radio relay system that is based on mobile ad-hoc networks. However, due to the long propagation delay, the existing TDMA MAC schemes underutilize the networks. The existing TDMA MAC sends data and receives ACK in one time slot, which requires two guard times in one time slot. Since aeronautical communications spans a significant distance, the guard time occupies a significantly large portion of the slot. To solve this problem, we propose a piggybacking mechanism ACK. Our proposed MAC has one guard time in one time slot, which enables the transmission of more data. Using this additional data, we can send weather conditions that pertain to the aircraft's current position. Our analysis shows that this proposed MAC performs better than the existing MAC, since it offers better throughput and network utilization. In addition, our weather condition notification model achieves a much lower transmission delay than a HF (high frequency) voice communication.",2011-05-30T10:41:17Z,http://arxiv.org/pdf/1105.5939v1,2024-04-28,
1106.2794v1,"Power Management during Scan Based Sequential Circuit Testing","This paper shows that not every scan cell contributes equally to the power consumption during scan based test. The transitions at some scan cells cause more toggles at the internal signal lines of a circuit than the transitions at other scan cells. Hence the transitions at these scan cells have a larger impact on the power consumption during test application. These scan cells are called power sensitive scan cells.A verilog based approach is proposed to identify a set of power sensitive scan cells. Additional hardware is added to freeze the outputs of power sensitive scan cells during scan shifting in order to reduce the shift power consumption.when multiple scan chain is incorporated along with freezing the power sensitive scan cell,over all power during testing can be reduced to a larger extend.",2011-06-02T06:52:02Z,http://arxiv.org/pdf/1106.2794v1,2024-04-28,
1106.3977v3,"Models, Calculation and Optimization of Gas Networks, Equipment and Contracts for Design, Operation, Booking and Accounting","There are proposed models of contracts, technological equipment and gas networks and methods of their optimization. The flow in network undergoes restrictions of contracts and equipment to be operated. The values of sources and sinks are provided by contracts. The contract models represent (sub-) networks. The simplest contracts represent either nodes or edges. Equipment is modeled by edges. More sophisticated equipment is represented by sub-networks. Examples of such equipment are multi-poles and compressor stations with many entries and exits. The edges can be of different types corresponding to equipment and contracts. On such edges, there are given systems of equation and inequalities simulating the contracts and equipment. On this base, the methods proposed that allow: calculation and control of contract values for booking on future days and for accounting of sales and purchases; simulation and optimization of design and of operation of gas networks. These models and methods are implemented in software systems ACCORD and Graphicord as well as in the distributed control system used by Wingas, Germany. As numerical example, the industrial computations are presented.",2011-06-20T18:12:19Z,http://arxiv.org/pdf/1106.3977v3,2024-04-28,
1107.0015v1,"Automaton based detection of affected cells in three dimensional biological system","The aim of this research review is to propose the logic and search mechanism for the development of an artificially intelligent automaton (AIA) that can find affected cells in a 3-dimensional biological system. Research on the possible application of such automatons to detect and control cancer cells in the human body are greatly focused MRI and PET scans finds the affected regions at the tissue level even as we can find the affected regions at the cellular level using the framework. The AIA may be designed to ensure optimum utilization as they record and might control the presence of affected cells in a human body. The proposed models and techniques can be generalized and used in any application where cells are injured or affected by some disease or accident. The best method to import AIA into the body without surgery or injection is to insert small pill like automata, carrying material viz drugs or leukocytes that is needed to correct the infection. In this process, the AIA can be compared to nano pills to deliver or support therapy. NanoHive simulation software was used to validate the framework of this paper. The existing nanomedicine models such as obstacle avoidance algorithm based models (Hla K H S et al 2008) and the framework in this model were tested in different simulation based experiments. The existing models such as obstacle avoidance based models failed in complex environmental conditions (such as changing environmental conditions, presence of semi-solid particles, etc) while the model in this paper executed its framework successfully.Come systems biology, this field of automatons deserves a bigger leap of understanding especially when pharmacogenomics is at its peak. The results also indicate the importance of artificial intelligence and other computational capabilities in the proposed model for the successful detection of affected cells.",2011-06-30T20:11:58Z,http://arxiv.org/pdf/1107.0015v1,2024-04-28,
1107.1128v1,"AISMOTIF-An Artificial Immune System for DNA Motif Discovery","Discovery of transcription factor binding sites is a much explored and still exploring area of research in functional genomics. Many computational tools have been developed for finding motifs and each of them has their own advantages as well as disadvantages. Most of these algorithms need prior knowledge about the data to construct background models. However there is not a single technique that can be considered as best for finding regulatory motifs. This paper proposes an artificial immune system based algorithm for finding the transcription factor binding sites or motifs and two new weighted scores for motif evaluation. The algorithm is enumerative, but sufficient pruning of the pattern search space has been incorporated using immune system concepts. The performance of AISMOTIF has been evaluated by comparing it with eight state of art composite motif discovery algorithms and found that AISMOTIF predicts known motifs as well as new motifs from the benchmark dataset without any prior knowledge about the data.",2011-07-05T06:01:20Z,http://arxiv.org/pdf/1107.1128v1,2024-04-28,
1107.4918v2,"Fluid Flow Complexity in Fracture Networks: Analysis with Graph Theory and LBM","Through this research, embedded synthetic fracture networks in rock masses are studied. To analysis the fluid flow complexity in fracture networks with respect to the variation of connectivity patterns, two different approaches are employed, namely, the Lattice Boltzmann method and graph theory. The Lattice Boltzmann method is used to show the sensitivity of the permeability and fluid velocity distribution to synthetic fracture networks' connectivity patterns. Furthermore, the fracture networks are mapped into the graphs, and the characteristics of these graphs are compared to the main spatial fracture networks. Among different characteristics of networks, we distinguish the modularity of networks and sub-graphs distributions. We map the flow regimes into the proper regions of the network's modularity space. Also, for each type of fluid regime, corresponding motifs shapes are scaled. Implemented power law distributions of fracture length in spatial fracture networks yielded the same node's degree distribution in transformed networks. Two general spatial networks are considered: random networks and networks with ""hubness"" properties mimicking a spatial damage zone (both with power law distribution of fracture length). In the first case, the fractures are embedded in uniformly distributed fracture sets; the second case covers spatial fracture zones. We prove numerically that the abnormal change (transition) in permeability is controlled by the hub growth rate. Also, comparing LBM results with the characteristic mean length of transformed networks' links shows a reverse relationship between the aforementioned parameters. In addition, the abnormalities in advection through nodes are presented.",2011-07-25T12:28:50Z,http://arxiv.org/pdf/1107.4918v2,2024-04-28,
1109.3524v2,"cuIBM -- A GPU-accelerated Immersed Boundary Method","A projection-based immersed boundary method is dominated by sparse linear algebra routines. Using the open-source Cusp library, we observe a speedup (with respect to a single CPU core) which reflects the constraints of a bandwidth-dominated problem on the GPU. Nevertheless, GPUs offer the capacity to solve large problems on commodity hardware. This work includes validation and a convergence study of the GPU-accelerated IBM, and various optimizations.",2011-09-16T04:02:15Z,http://arxiv.org/pdf/1109.3524v2,2024-04-28,
1110.2049v1,"Acceleration of Uncertainty Updating in the Description of Transport Processes in Heterogeneous Materials","The prediction of thermo-mechanical behaviour of heterogeneous materials such as heat and moisture transport is strongly influenced by the uncertainty in parameters. Such materials occur e.g. in historic buildings, and the durability assessment of these therefore needs a reliable and probabilistic simulation of transport processes, which is related to the suitable identification of material parameters. In order to include expert knowledge as well as experimental results, one can employ an updating procedure such as Bayesian inference. The classical probabilistic setting of the identification process in Bayes's form requires the solution of a stochastic forward problem via computationally expensive sampling techniques, which makes the method almost impractical. In this paper novel stochastic computational techniques such as the stochastic Galerkin method are applied in order to accelerate the updating procedure. The idea is to replace the computationally expensive forward simulation via the conventional finite element (FE) method by the evaluation of a polynomial chaos expansion (PCE). Such an approximation of the FE model for the forward simulation perfectly suits the Bayesian updating. The presented uncertainty updating techniques are applied to the numerical model of coupled heat and moisture transport in heterogeneous materials with spatially varying coefficients defined by random fields.",2011-10-10T14:22:16Z,http://arxiv.org/pdf/1110.2049v1,2024-04-28,
1110.3382v1,"Sampling Techniques in Bayesian Finite Element Model Updating","Recent papers in the field of Finite Element Model (FEM) updating have highlighted the benefits of Bayesian techniques. The Bayesian approaches are designed to deal with the uncertainties associated with complex systems, which is the main problem in the development and updating of FEMs. This paper highlights the complexities and challenges of implementing any Bayesian method when the analysis involves a complicated structural dynamic model. In such systems an analytical Bayesian formulation might not be available in an analytic form; therefore this leads to the use of numerical methods, i.e. sampling methods. The main challenge then is to determine an efficient sampling of the model parameter space. In this paper, three sampling techniques, the Metropolis-Hastings (MH) algorithm, Slice Sampling and the Hybrid Monte Carlo (HMC) technique, are tested by updating a structural beam model. The efficiency and limitations of each technique is investigated when the FEM updating problem is implemented using the Bayesian Approach. Both MH and HMC techniques are found to perform better than the Slice sampling when Young's modulus is chosen as the updating parameter. The HMC method gives better results than MH and Slice sampling techniques, when the area moment of inertias and section areas are updated.",2011-10-15T05:17:18Z,http://arxiv.org/pdf/1110.3382v1,2024-04-28,
1111.2514v1,"A more appropriate Protein Classification using Data Mining","Research in bioinformatics is a complex phenomenon as it overlaps two knowledge domains, namely, biological and computer sciences. This paper has tried to introduce an efficient data mining approach for classifying proteins into some useful groups by representing them in hierarchy tree structure. There are several techniques used to classify proteins but most of them had few drawbacks on their grouping. Among them the most efficient grouping technique is used by PSIMAP. Even though PSIMAP (Protein Structural Interactome Map) technique was successful to incorporate most of the protein but it fails to classify the scale free property proteins. Our technique overcomes this drawback and successfully maps all the protein in different groups, including the scale free property proteins failed to group by PSIMAP. Our approach selects the six major attributes of protein: a) Structure comparison b) Sequence Comparison c) Connectivity d) Cluster Index e) Interactivity f) Taxonomic to group the protein from the databank by generating a hierarchal tree structure. The proposed approach calculates the degree (probability) of similarity of each protein newly entered in the system against of existing proteins in the system by using probability theorem on each six properties of proteins.",2011-10-12T12:18:39Z,http://arxiv.org/pdf/1111.2514v1,2024-04-28,
1112.2954v2,"Synthesis of Spherical 4R Mechanism for Path Generation using Differential Evolution","The problem of path generation for the spherical 4R mechanism is solved using the Differential Evolution algorithm (DE). Formulas for the spherical geodesics are employed in order to obtain the parametric equation for the generated trajectory. Direct optimization of the objective function gives the solution to the path generation task without prescribed timing. Therefore, there is no need to separate this task into two stages to make the optimization. Moreover, the order defect problem can be solved without difficulty by means of manipulations of the individuals in the DE algorithm. Two examples of optimum synthesis showing the simplicity and effectiveness of this approach are included.",2011-12-13T16:48:05Z,http://arxiv.org/pdf/1112.2954v2,2024-04-28,
1112.4895v3,"3D Finite Element Analysis of HMA Overlay Mix Design to Control Reflective Cracking","This study examines the effectiveness of HMA overlay design strategies for the purpose of controlling the development of reflective cracking. A parametric study was conducted using a 3D Finite Element (FE) model of a rigid pavement section including Linear Viscoelastic (LVE) material properties for the Hot Mix Asphalt (HMA) overlay and non-uniform tire-pavement contact stresses. Several asphalt mixtures were tested in the surface, intermediate, and leveling course of the HMA overlay. Results obtained show that no benefits can be anticipated by using either Polymer-Modified (PM) or Dense-Graded (DG) mixtures instead of Standard Binder (SB) mixtures in the surface or intermediate course. For the leveling course, the use of a PM asphalt binder was found beneficial in terms of mitigating reflective cracking. As compared to the SB mix, the use of PM asphalt mixture in the leveling course reduced the level of longitudinal tensile stress at the bottom of the HMA overlay above the PCC joint by approximately 30%.",2011-12-21T00:39:12Z,http://arxiv.org/pdf/1112.4895v3,2024-04-28,
1201.0469v1,"Computing Critical $k$-tuples in Power Networks","In this paper the problem of finding the sparsest (i.e., minimum cardinality) critical $k$-tuple including one arbitrarily specified measurement is considered. The solution to this problem can be used to identify weak points in the measurement set, or aid the placement of new meters. The critical $k$-tuple problem is a combinatorial generalization of the critical measurement calculation problem. Using topological network observability results, this paper proposes an efficient and accurate approximate solution procedure for the considered problem based on solving a minimum-cut (Min-Cut) problem and enumerating all its optimal solutions. It is also shown that the sparsest critical $k$-tuple problem can be formulated as a mixed integer linear programming (MILP) problem. This MILP problem can be solved exactly using available solvers such as CPLEX and Gurobi. A detailed numerical study is presented to evaluate the efficiency and the accuracy of the proposed Min-Cut and MILP calculations.",2012-01-02T14:10:57Z,http://arxiv.org/pdf/1201.0469v1,2024-04-28,
1201.3479v1,"Simple Numerical Model of Laminated Glass Beams","This contribution presents a simple Finite Element model aimed at efficient simulation of layered glass units. The adopted approach is based on considering independent kinematics of each layer, tied together via Lagrange multipliers. Validation and verification of the resulting model against independent data demonstrate its accuracy, showing its potential for generalization towards more complex problems.",2012-01-17T10:57:29Z,http://arxiv.org/pdf/1201.3479v1,2024-04-28,
1203.2499v2,"A framework for integrated design of algorithmic architectural forms","This paper presents a methodology and software tools for parametric design of complex architectural objects, called digital or algorithmic forms. In order to provide a flexible tool, the proposed design philosophy involves two open source utilities Donkey and MIDAS written in Grasshopper algorithm editor and C++, respectively, that are to be linked with a scripting-based architectural modellers Rhinoceros, IntelliCAD and the open source Finite Element solver OOFEM. The emphasis is put on the mechanical response in order to provide architects with a consistent learning framework and an insight into structural behaviour of designed objects. As demonstrated on three case studies, the proposed modular solution is capable of handling objects of considerable structural complexity, thereby accelerating the process of finding procedural design parameters from orders of weeks to days or hours.",2012-03-12T14:26:05Z,http://arxiv.org/pdf/1203.2499v2,2024-04-28,
1203.2528v1,"Knowledge-based antenna pattern extrapolation","We describe a theoretically-motivated algorithm for extrapolation of antenna radiation patterns from a small number of measurements. This algorithm exploits constraints on the antenna's underlying design to avoid ambiguities, but is sufficiently general to address many different antenna types. A theoretical basis for the robustness of this algorithm is developed, and its performance is verified in simulation using a number of popular antenna designs.",2012-03-12T16:02:46Z,http://arxiv.org/pdf/1203.2528v1,2024-04-28,
1203.6728v1,"System Identification for Indoor Climate Control","The study focuses on the applicability of system identification to identify building and system dynamics for climate control design. The main problem regarding the simulation of the dynamic response of a building using building simulation software is that (1) the simulation of a large complex building is time consuming, and (2) simulation results often lack information regarding fast dynamic behaviour (in the order of seconds), since most software uses a discrete time step, usually fixed to one hour. The first objective is to study the applicability of system identification to reduce computing time for the simulation of large complex buildings. The second objective is to research the applicability of system identification to identify building dynamics based on discrete time data (one hour) for climate control design. The study illustrates that system identification is applicable for the identification of building dynamics with a frequency that is smaller as the maximum sample frequency as used for identification. The research shows that system identification offers good perspectives for the modelling of heat, air and moisture processes in a building. The main advantages of system identification models compared to the modelling of building dynamics using building simulation software are, that (1) the computing time is reduced significantly, and (2) system identification models run in a MATLAB environment, in which many building simulation tools have been developed",2012-03-30T07:27:23Z,http://arxiv.org/pdf/1203.6728v1,2024-04-28,
1204.4366v1,"Multipath-dominant, pulsed doppler analysis of rotating blades","We present a novel angular fingerprinting algorithm for detecting changes in the direction of rotation of a target with a monostatic, stationary sonar platform. Unlike other approaches, we assume that the target's centroid is stationary, and exploit doppler multipath signals to resolve the otherwise unavoidable ambiguities that arise. Since the algorithm is based on an underlying differential topological theory, it is highly robust to distortions in the collected data. We demonstrate performance of this algorithm experimentally, by exhibiting a pulsed doppler sonar collection system that runs on a smartphone. The performance of this system is sufficiently good to both detect changes in target rotation direction using angular fingerprints, and also to form high-resolution inverse synthetic aperature images of the target.",2012-04-19T15:02:30Z,http://arxiv.org/pdf/1204.4366v1,2024-04-28,
1204.5174v2,"Christhin: Quantitative Analysis of Thin Layer Chromatography","Manual for Christhin 0.1.36 Christhin (Chromatography Riser Thin) is software developed for the quantitative analysis of data obtained from thin-layer chromatographic techniques (TLC). Once installed on your computer, the program is very easy to use, and provides data quickly and accurately. This manual describes the program, and reading should be enough to use it properly.",2012-04-23T17:15:17Z,http://arxiv.org/pdf/1204.5174v2,2024-04-28,
1206.0638v2,"WM Program manual","This user manual has been written to describe the open source code WM to be distributed associated with a research article submitted to the information technology journal 45001-ITJ-ANSI, entitled: ""Maintenance and Reengineering of software: Creating a Visual C++ Graphical User Interface to Perform Specific Tasks Related to Soil Structure Interaction in Poroelastic Soil"".",2012-06-04T14:51:18Z,http://arxiv.org/pdf/1206.0638v2,2024-04-28,
1206.4438v1,"Inverse Modeling of Climate Responses of Monumental Buildings","The indoor climate conditions of monumental buildings are very important for the conservation of these objects. Simplified models with physical meaning are desired that are capable of simulating temperature and relative humidity. In this paper we research state-space models as methodology for the inverse modeling of climate responses of unheated monumental buildings. It is concluded that this approach is very promising for obtaining physical models and parameters of indoor climate responses. Furthermore state space models can be simulated very efficiently: the simulation duration time of a 100 year hourly based period take less than a second on an ordinary computer.",2012-06-20T10:07:31Z,http://arxiv.org/pdf/1206.4438v1,2024-04-28,
1207.0313v1,"Intellectual Management of Enterprise","A new technology (in addition to ERP) is proposed to provide an increase of profit and normal cash flow. This technology involves the next functions: forming of intellectual interface on a natural language to communicate with a control system; joint planning of production and sales to get the maximal profit; an adaptation of control system to internal and external events. The use of the natural language permits to overcome a barrier between the control system and upper managers. To solve posed actual problems of management the selection of information from a database and call to mathematical methods are executed automatically. Optimal planning provides the maximal use of available resources and opportunities of market. Adaptive control implements the efficient reaction to critical events that lead up to a decrease of profit and increase of accounts receivable.",2012-07-02T09:18:53Z,http://arxiv.org/pdf/1207.0313v1,2024-04-28,
1207.1547v1,"Hybrid Forecasting of Exchange Rate by Using Chaos Wavelet SVM-Markov Model and Grey Relation Degree","This paper proposes an exchange rate forecasting method by using the grey relative combination approach of chaos wavelet SVM-Markov model. The problem of short-term forecast of exchange rate by using the comprehensive method of the phase space reconstitution and SVM method has been researched. We have suggested a wavelet-SVR-Markov forecasting model to predict the finance time series and demonstrated that can more improve the forecasting performance by the rational combination of the forecast results through various combinational tests. Our test result has been showed that the two-stage combination model is more excellent than the normal combination model. Also we have comprehensively estimated the combination forecast methods according to the forecasting performance indicators.The estimated result have been shown that the combination forecast methods on the basic of the degree of grey relation and the optimal grey relation combination have fine forecast performance.",2012-07-06T07:54:16Z,http://arxiv.org/pdf/1207.1547v1,2024-04-28,
1207.1933v1,"A Hybrid Forecast of Exchange Rate based on ARFIMA,Discrete Grey-Markov, and Fractal Kalman Model","We propose a hybrid forecast based on extended discrete grey Markov and variable dimension Kalman model and show that our hybrid model can improve much more the performance of forecast than traditional grey Markov and Kalman models. Our simulation results are given to demonstrate that our hybrid forecast method combined with degree of grey incidence are better than grey Markov and ARFIMA model or Kalman methods.",2012-07-09T02:08:26Z,http://arxiv.org/pdf/1207.1933v1,2024-04-28,
1207.2169v2,"High-throughput Genome-wide Association Analysis for Single and Multiple Phenotypes","The variance component tests used in genomewide association studies of thousands of individuals become computationally exhaustive when multiple traits are analysed in the context of omics studies. We introduce two high-throughput algorithms -- CLAK-CHOL and CLAK-EIG -- for single and multiple phenotype genome-wide association studies (GWAS). The algorithms, generated with the help of an expert system, reduce the computational complexity to the point that thousands of traits can be analyzed for association with millions of polymorphisms in a course of days on a standard workstation. By taking advantage of problem specific knowledge, CLAK-CHOL and CLAK-EIG significantly outperform the current state-of-the-art tools in both single and multiple trait analysis.",2012-07-09T20:25:26Z,http://arxiv.org/pdf/1207.2169v2,2024-04-28,
1207.2254v1,"A Hybrid Forecast of Exchange Rate based on Discrete Grey-Markov and Grey Neural Network Model","We propose a hybrid forecast model based on discrete grey-fuzzy Markov and grey neural network model and show that our hybrid model can improve much more the performance of forecast than traditional grey-Markov model and neural network models. Our simulation results are shown that our hybrid forecast method with the combinational weight based on optimal grey relation degree method is better than the hybrid model with combinational weight based minimization of error-squared criterion.",2012-07-10T07:46:04Z,http://arxiv.org/pdf/1207.2254v1,2024-04-28,
1208.3122v1,"Defect Diagnosis in Rotors Systems by Vibrations Data Collectors Using Trending Software","Vibration measurements have been used to reliably diagnose performance problems in machinery and related mechanical products. A vibration data collector can be used effectively to measure and analyze the machinery vibration content in gearboxes, engines, turbines, fans, compressors, pumps and bearings. Ideally, a machine will have little or no vibration, indicating that the rotating components are appropriately balanced, aligned, and well maintained. Quick analysis and assessment of the vibration content can lead to fault diagnosis and prognosis of a machine's ability to continue running. The aim of this research used vibration measurements to pinpoint mechanical defects such as (unbalance, misalignment, resonance, and part loosening), consequently diagnosis all necessary process for engineers and technicians who desire to understand the vibration that exists in structures and machines.   Keywords- vibration data collectors; analysis software; rotating components.",2012-08-15T14:18:01Z,http://arxiv.org/pdf/1208.3122v1,2024-04-28,
1208.3681v1,"Calculations of Frequency Response Functions (FRF) Using Computer Smart Office Software and Nyquist Plot under Gyroscopic Effect Rotation","Regenerated (FRF curves), synthesis of (FRF) curves there are two main requirement in the form of response model, The first being that of regenerating ""Theoretical"" curve for the frequency response function actually measured and analysis and the second being that of synthesising the other functions which were not measured,(FRF) that isolates the inherent dynamic properties of a mechanical structure. Experimental modal parameters (frequency, damping, and mode shape) are also obtained from a set of (FRF) measurements. The (FRF) describes the input-output relationship between two points on a structure as a function of frequency. Therefore, an (FRF) is actually defined between a single input DOF (point & direction), and a single output (DOF), although the FRF was previously defined as a ratio of the Fourier transforms of an output and input signal. In this paper we detection FRF curve using Nyquist plot under gyroscopic effect in revolving structure using computer smart office software.   Keywords - FRF curve; modal test; Nyquist plot; software engineering; gyroscopic effect; smart office.",2012-08-16T14:37:35Z,http://arxiv.org/pdf/1208.3681v1,2024-04-28,
1208.3849v1,"Analysis of parametric biological models with non-linear dynamics","In this paper we present recent results on parametric analysis of biological models. The underlying method is based on the algorithms for computing trajectory sets of hybrid systems with polynomial dynamics. The method is then applied to two case studies of biological systems: one is a cardiac cell model for studying the conditions for cardiac abnormalities, and the second is a model of insect nest-site choice.",2012-08-19T16:01:06Z,http://arxiv.org/pdf/1208.3849v1,2024-04-28,
1209.0616v1,"Well Placement Optimization under Uncertainty with CMA-ES Using the Neighborhood","In the well placement problem, as well as in other field development optimization problems, geological uncertainty is a key source of risk affecting the viability of field development projects. Well placement problems under geological uncertainty are formulated as optimization problems in which the objective function is evaluated using a reservoir simulator on a number of possible geological realizations. In this paper, we present a new approach to handle geological uncertainty for the well placement problem with a reduced number of reservoir simulations. The proposed approach uses already simulated well configurations in the neighborhood of each well configuration for the objective function evaluation. We use thus only one single reservoir simulation performed on a randomly chosen realization together with the neighborhood to estimate the objective function instead of using multiple simulations on multiple realizations. This approach is combined with the stochastic optimizer CMA-ES. The proposed approach is shown on the benchmark reservoir case PUNQ-S3 to be able to capture the geological uncertainty using a smaller number of reservoir simulations. This approach is compared to the reference approach using all the possible realizations for each well configuration, and shown to be able to reduce significantly the number of reservoir simulations (around 80%).",2012-09-04T11:52:14Z,http://arxiv.org/pdf/1209.0616v1,2024-04-28,
1209.2641v1,"C-PASS-PC: A Cloud-driven Prototype of Multi-Center Proactive Surveillance System for Prostate Cancer","Currently there are many clinical trials using paper case report forms as the primary data collection tool. Cloud Computing platforms provide big potential for increasing efficiency through a web-based data collection interface, especially for large-scale multi-center trials. Traditionally, clinical and biological data for multi-center trials are stored in one dedicated, centralized database system running at a data coordinating center (DCC). This paper presents C-PASS-PC, a cloud-driven prototype of multi-center proactive surveillance system for prostate cancer. The prototype is developed in PHP, JQuery and CSS with an Oracle backend in a local Web server and database server and deployed on Google App Engine (GAE) and Google Cloud SQL-MySQL. The deploying process is fast and easy to follow. The C-PASS-PC prototype can be accessed through an SSL-enabled web browser. Our approach proves the concept that cloud computing platforms such as GAE is a suitable and flexible solution in the near future for multi-center clinical trials.",2012-09-12T15:29:12Z,http://arxiv.org/pdf/1209.2641v1,2024-04-28,
1211.2743v1,"Systematic and Integrative Analysis of Proteomic Data using Bioinformatics Tools","The analysis and interpretation of relationships between biological molecules is done with the help of networks. Networks are used ubiquitously throughout biology to represent the relationships between genes and gene products. Network models have facilitated a shift from the study of evolutionary conservation between individual gene and gene products towards the study of conservation at the level of pathways and complexes. Recent work has revealed much about chemical reactions inside hundreds of organisms as well as universal characteristics of metabolic networks, which shed light on the evolution of the networks. However, characteristics of individual metabolites have been neglected in this network. The current paper provides an overview of bioinformatics software used in visualization of biological networks using proteomic data, their main functions and limitations of the software.",2012-11-12T19:19:04Z,http://arxiv.org/pdf/1211.2743v1,2024-04-28,
1211.3233v2,"New algorithm for footstep localization using seismic sensors in an indoor environment","In this study, we consider the use of seismic sensors for footstep localization in indoor environments. A popular strategy of localization is to use the measured differences in arrival times of source signals at multiple pairs of receivers. In the literature, most algorithms that are based on time differences of arrival (TDOA) assume that the propagation velocity is a constant as a function of the source position, which is valid for air propagation or even for narrow band signals. However a bounded medium such as a concrete slab (encountered in indoor environement) is usually dispersive and damped. In this study, we demonstrate that under such conditions, the concrete slab can be assimilated to a thin plate; considering a Kelvin-Voigt damping model, we introduce the notion of {\em perceived propagation velocity}, which decreases when the source-sensor distance increases. This peculiar behaviour precludes any possibility to rely on existing localization methods in indoor environment. Therefore, a new localization algorithm that is adapted to a damped and dispersive medium is proposed, using only on the sign of the measured TDOA (SO-TDOA). A simulation and some experimental results are included, to define the performance of this SO-TDOA algorithm.",2012-11-14T08:21:06Z,http://arxiv.org/pdf/1211.3233v2,2024-04-28,
1211.4654v1,"Application of Data mining in Protein sequence Classification","Protein sequence classification involves feature selection for accurate classification. Popular protein sequence classification techniques involve extraction of specific features from the sequences. Researchers apply some well-known classification techniques like neural networks, Genetic algorithm, Fuzzy ARTMAP,Rough Set Classifier etc for accurate classification. This paper presents a review is with three different classification models such as neural network model, fuzzy ARTMAP model and Rough set classifier model. This is followed by a new technique for classifying protein sequences. The proposed model is typically implemented with an own designed tool and tries to reduce the computational overheads encountered by earlier approaches and increase the accuracy of classification",2012-11-20T02:59:28Z,http://arxiv.org/pdf/1211.4654v1,2024-04-28,
1211.5890v1,"Adaptive Control of Enterprise","Modern progress in artificial intelligence permits to realize algorithms of adaptation for critical events (in addition to ERP). A production emergence, an appearance of new competitive goods, a major change in financial state of partners, a radical change in exchange rate, a change in custom and tax legislation, a political and energy crisis, an ecocatastrophe can lead up to a decrease of profit or bankruptcy of enterprise. Therefore it is necessary to assess a probability of threat and to take preventive actions. If a critical event took place, one must estimate restoration expenses and possible consequences as well as to prepare appropriate propositions. This is provided using modern methods of diagnostics, prediction, and decision making as well as an inference engine and semantic analysis. Mathematical methods in use are called in algorithms of adaptation automatically. Because the enterprise is a complex system, to overcome complexity of control it is necessary to apply semantic representations. Such representations are formed from descriptions of events, facts, persons, organizations, goods, operations, scripts on a natural language. Semantic representations permit as well to formulate actual problems and to find ways to resolve these problems.",2012-11-26T09:09:21Z,http://arxiv.org/pdf/1211.5890v1,2024-04-28,
1212.1107v1,"Twitter Sentiment Analysis: How To Hedge Your Bets In The Stock Markets","Emerging interest of trading companies and hedge funds in mining social web has created new avenues for intelligent systems that make use of public opinion in driving investment decisions. It is well accepted that at high frequency trading, investors are tracking memes rising up in microblogging forums to count for the public behavior as an important feature while making short term investment decisions. We investigate the complex relationship between tweet board literature (like bullishness, volume, agreement etc) with the financial market instruments (like volatility, trading volume and stock prices). We have analyzed Twitter sentiments for more than 4 million tweets between June 2010 and July 2011 for DJIA, NASDAQ-100 and 11 other big cap technological stocks. Our results show high correlation (upto 0.88 for returns) between stock prices and twitter sentiments. Further, using Granger's Causality Analysis, we have validated that the movement of stock prices and indices are greatly affected in the short term by Twitter discussions. Finally, we have implemented Expert Model Mining System (EMMS) to demonstrate that our forecasted returns give a high value of R-square (0.952) with low Maximum Absolute Percentage Error (MaxAPE) of 1.76% for Dow Jones Industrial Average (DJIA). We introduce a novel way to make use of market monitoring elements derived from public mood to retain a portfolio within limited risk state (highly improved hedging bets) during typical market conditions.",2012-12-05T17:24:06Z,http://arxiv.org/pdf/1212.1107v1,2024-04-28,
1212.3922v1,"Interroom radiative couplings through windows and large openings in buildings: Proposal of a simplified model","A simplified model of indoor short wave radiation couplings adapted to multi-zone simulations is proposed, thanks to a simplifying hypothesis and to the introduction of an indoor short wave exchange matrix. The specific properties of this matrix appear useful to quantify the thermal radiation exchanges between the zones separated by windows or large openings. Integrated in CODYRUN software, this module is detailed and compared to experimental measurements carried out on a real scale tropical building.",2012-12-17T08:17:14Z,http://arxiv.org/pdf/1212.3922v1,2024-04-28,
1212.3924v1,"Building ventilation: A pressure airflow model computer generation and elements of validation","The calculation of airflows is of great importance for detailed building thermal simulation computer codes, these airflows most frequently constituting an important thermal coupling between the building and the outside on one hand, and the different thermal zones on the other. The driving effects of air movement, which are the wind and the thermal buoyancy, are briefly outlined and we look closely at their coupling in the case of buildings, by exploring the difficulties associated with large openings. Some numerical problems tied to the resolving of the non-linear system established are also covered. Part of a detailled simulation software (CODYRUN), the numerical implementation of this airflow model is explained, insisting on data organization and processing allowing the calculation of the airflows. Comparisons are then made between the model results and in one hand analytical expressions and in another and experimental measurements in case of a collective dwelling.",2012-12-17T08:20:43Z,http://arxiv.org/pdf/1212.3924v1,2024-04-28,
1212.3925v1,"Elaboration of global quality standards for natural and low energy cooling in French tropical island buildings","Electric load profiles of tropical islands in developed countries are characterised by morning, midday and evening peaks arising from all year round high power demand in the commercial and residential sectors, due mostly to air conditioning appliances and bad thermal conception of the building. The work presented in this paper has led to the conception of a global quality standards obtained through optimized bioclimatic urban planning and architectural design, the use of passive cooling architectural components, natural ventilation and energy efficient systems such as solar water heaters. We evaluated, with the aid of an airflow and thermal building simulation software (CODYRUN), the impact of each technical solution on thermal comfort within the building. These technical solutions have been implemented in 280 new pilot dwelling projects through the year 1996.",2012-12-17T08:21:02Z,http://arxiv.org/pdf/1212.3925v1,2024-04-28,
1212.3928v1,"A validation methodology aid for improving a thermal building model: Case of diffuse radiation accounting in a tropical climate","As part of our efforts to complete the software CODYRUN validation, we chose as test building a block of flats constructed in Reunion Island, which has a humid tropical climate. The sensitivity analysis allowed us to study the effects of both diffuse and direct solar radiation on our model of this building. With regard to the choice and location of sensors, this stage of the study also led us to measure the solar radiation falling on the windows. The comparison of measured and predicted radiation clearly showed that our predictions over-estimated the incoming solar radiation, and we were able to trace the problem to the algorithm which calculates diffuse solar radiation. By calculating view factors between the windows and the associated shading devices, changes to the original program allowed us to improve the predictions, and so this article shows the importance of sensitivity analysis in this area of research.",2012-12-17T08:22:49Z,http://arxiv.org/pdf/1212.3928v1,2024-04-28,
1212.3930v1,"Detailed weather data generator for building simulations","Thermal buildings simulation softwares need meteorological files in thermal comfort, energetic evaluation studies. Few tools can make significant meteorological data available such as generated typical year, representative days, or artificial meteorological database. This paper deals about the presentation of a new software, RUNEOLE, used to provide weather data in buildings applications with a method adapted to all kind of climates. RUNEOLE associates three modules of description, modelling and generation of weather data. The statistical description of an existing meteorological database makes typical representative days available and leads to the creation of model libraries. The generation module leads to the generation of non existing sequences. This software tends to be usable for the searchers and designers, by means of interactivity, facilitated use and easy communication. The conceptual basis of this tool will be exposed and we'll propose two examples of applications in building physics for tropical humid climates.",2012-12-17T08:23:27Z,http://arxiv.org/pdf/1212.3930v1,2024-04-28,
1212.5095v1,"Modelling of Optimal Design of Manufacturing Cell Layout Considering Material Flow and Closeness Rating Factors","Developing a group of machine cells and their corresponding part families to minimize the inter-cell and intra-cell material flow is the basic objective of the designing of a cellular manufacturing system (CMS). Afterwards achieving a competent cell layout is essential in order to minimize the total inter-cell part travels, which is principally noteworthy. There are plentiful articles of CMS literature which considered cell formation problems; however cell layout topic has rarely been addressed. Therefore this research is intended to focus on an adapted mathematical model of the layout design problem considering material handling cost and closeness ratings of manufacturing cells. Owing to the combinatorial class of the said problem, an efficient NP-hard technique based on Simulated Annealing metaheuristic is proposed henceforth. Some test problems are solved using the proposed technique. Computational results show that the proposed metaheuristic approach is extremely effective and efficient in terms of solution quality and computational complexity.",2012-12-20T15:47:50Z,http://arxiv.org/pdf/1212.5095v1,2024-04-28,
1212.5253v1,"Development of a new model to predict indoor daylighting: Integration in CODYRUN software and validation","Many models exist in the scientific literature for determining indoor daylighting values. They are classified in three categories: numerical, simplified and empirical models. Nevertheless, each of these categories of models are not convenient for every application. Indeed, the numerical model requires high calculation time; conditions of use of the simplified models are limited, and experimental models need not only important financial resources but also a perfect control of experimental devices (e.g. scale model), as well as climatic characteristics of the location (e.g. in situ experiment). In this article, a new model based on a combination of multiple simplified models is established. The objective is to improve this category of model. The originality of our paper relies on the coupling of several simplified models of indoor daylighting calculations. The accuracy of the simulation code, introduced into CODYRUN software to simulate correctly indoor illuminance, is then verified. Besides, the software consists of a numerical building simulation code, developed in the Physics and Mathematical Engineering Laboratory for Energy and Environment (P.I.M.E.N.T) at the University of Reunion. Initially dedicated to the thermal, airflow and hydrous phenomena in the buildings, the software has been completed for the calculation of indoor daylighting. New models and algorithms - which rely on a semi-detailed approach - will be presented in this paper. In order to validate the accuracy of the integrated models, many test cases have been considered as analytical, inter-software comparisons and experimental comparisons. In order to prove the accuracy of the new model - which can properly simulate the illuminance - a confrontation between the results obtained from the software (developed in this research paper) and the major made at a given place is described in details. A new statistical indicator to appreciate the margins of errors - named RSD (Reliability of Software Degrees) - is also be defined.",2012-12-18T07:37:07Z,http://arxiv.org/pdf/1212.5253v1,2024-04-28,
1212.5255v1,"A Comparison between CODYRUN and TRNSYS, simulation models for thermal buildings behaviour","Simulation codes of thermal behaviour could significantly improve housing construction design. Among the existing software, CODYRUN and TRNSYS are calculations codes of different conceptions. CODYRUN is exclusively dedicated to housing thermal behaviour, whereas TRNSYS is more generally used on any thermal system. The purpose of this article is to compare these two instruments in two different conditions . We will first modelize a mono-zone test cell, and analyse the results by means of signal treatment methods. Then, we will modelize a real case of multi-zone housing, representative of housing in wet tropical climates. We could so evaluate influences of meteorological and building description data on model errors.",2012-12-18T07:39:56Z,http://arxiv.org/pdf/1212.5255v1,2024-04-28,
1212.5256v1,"Thermal Building Simulation and Computer Generation of Nodal Models","The designer's preoccupation to reduce the energy needs and get a better thermal quality of ambiances helped in the development of several packages simulating the dynamic behaviour of buildings. This paper shows the adaptation of a method of thermal analysis, the nodal analysis, linked to the case of building's thermal behaviour. We take successively an interest in the case of conduction into a wall, in the coupling with superficial exchanges and finally in the constitution of thermal state models of the building. Big variations existing from one building to another, it's necessary to build the thermal model from the building description. This article shows the chosen method in the case of our thermal simulation program for buildings, CODYRUN",2012-12-18T12:37:04Z,http://arxiv.org/pdf/1212.5256v1,2024-04-28,
1212.5260v1,"Heat transfer in buildings : application to air solar heating and Trombe wall design","The aim of this paper is to briefly recall heat transfer modes and explain their integration within a software dedicated to building simulation (CODYRUN). Detailed elements of the validation of this software are presented and two applications are finally discussed. One concerns the modeling of a flat plate air collector and the second focuses on the modeling of Trombe solar walls. In each case, detailed modeling of heat transfer allows precise understanding of thermal and energetic behavior of the studied structures. Recent decades have seen a proliferation of tools for building thermal simulation. These applications cover a wide spectrum from very simplified steady state models to dynamic simulation ones, including computational fluid dynamics modules (Clarke, 2001). These tools are widely available in design offices and engineering firms. They are often used for the design of HVAC systems and still subject to detailed research, particularly with respect to the integration of new fields (specific insulation materials, lighting, pollutants transport, etc.). Available from: http://www.intechopen.com/books/evaporation-condensation-and-heat-transfer/heat-transfer-in-buildings-application-to-solar-air-collector-and-trombe-wall-design",2012-12-20T08:44:54Z,http://arxiv.org/pdf/1212.5260v1,2024-04-28,
1212.5262v1,"A multimodel approach to building thermal simulation for design and research purposes","The designers pre-occupation to reduce energy consumption and to achieve better thermal ambience levels, has favoured the setting up of numerous building thermal dynamic simulation programs. The progress in the modelling of phenomenas and its transfer into the professional field has resulted in various numerical approaches ranging from softwares dedicated to architects for design use to tools for laboratory use by the expert thermal researcher. This analysis shows that each approach tends to fulfil the specific needs of a certain kind of manipulator only, in the building conception process. Our objective is notably different as it is a tool which can be used from the very initial stage of a construction project, to the energy audit for the existing building. In each of these cases, the objective results, the precision advocated and the time delay of the results are different parameters which call for a multiple model approach of the building system",2012-12-20T13:25:56Z,http://arxiv.org/pdf/1212.5262v1,2024-04-28,
1212.5263v1,"Use of BESTEST procedure to improve a building thermal simulation program","Validation of building energy simulation programs is of major interest to both users and modellers. To achieve such a task, it is essential to apply a methodology based on a priori test and empirical validation. A priori test consists in verifying that models embedded in a program and their implementation are correct. this should be achieved before carrying out experiments. The aim of this report is to present results from the application of the BESTEST procedure to our code. We will emphasise the way it allows to find bugs in our program and also how it permits to qualify models of heat transfer by conduction",2012-12-20T13:28:22Z,http://arxiv.org/pdf/1212.5263v1,2024-04-28,
1212.5264v1,"Statistical Traffic State Analysis in Large-scale Transportation Networks Using Locality-Preserving Non-negative Matrix Factorization","Statistical traffic data analysis is a hot topic in traffic management and control. In this field, current research progresses focus on analyzing traffic flows of individual links or local regions in a transportation network. Less attention are paid to the global view of traffic states over the entire network, which is important for modeling large-scale traffic scenes. Our aim is precisely to propose a new methodology for extracting spatio-temporal traffic patterns, ultimately for modeling large-scale traffic dynamics, and long-term traffic forecasting. We attack this issue by utilizing Locality-Preserving Non-negative Matrix Factorization (LPNMF) to derive low-dimensional representation of network-level traffic states. Clustering is performed on the compact LPNMF projections to unveil typical spatial patterns and temporal dynamics of network-level traffic states. We have tested the proposed method on simulated traffic data generated for a large-scale road network, and reported experimental results validate the ability of our approach for extracting meaningful large-scale space-time traffic patterns. Furthermore, the derived clustering results provide an intuitive understanding of spatial-temporal characteristics of traffic flows in the large-scale network, and a basis for potential long-term forecasting.",2012-12-20T14:53:44Z,http://arxiv.org/pdf/1212.5264v1,2024-04-28,
1212.5265v1,"An Effective Machine-Part Grouping Algorithm to Construct Manufacturing Cells","The machine-part cell formation problem consists of creating machine cells and their corresponding part families with the objective of minimizing the inter-cell and intra-cell movement while maximizing the machine utilization. This article demonstrates a hybrid clustering approach for the cell formation problem in cellular manufacturing that conjoins Sorenson s similarity coefficient based method to form the production cells. Computational results are shown over the test datasets obtained from the past literature. The hybrid technique is shown to outperform the other methods proposed in literature and including powerful soft computing approaches such as genetic algorithms, genetic programming by exceeding the solution quality on the test problems.",2012-12-20T15:51:13Z,http://arxiv.org/pdf/1212.5265v1,2024-04-28,
1212.5275v1,"A Picard Newton method to solve non linear airflow networks","In detailled buiding simulation models, airflow modelling and solving are still open and crucial problems, specially in the case of open buildings as encountered in tropical climates. As a consequence, wind speed conditioning indoor thermal comfort or energy needs in case of air conditionning are uneasy to predict. A first part of the problem is the lack of reliable and usable large opening elementary modelling and another one concerns the numerical solving of airflow network. This non linear pressure system is solved by numerous methods mainly based on Newton Raphson (NR) method. This paper is adressing this part of the difficulty, in our software CODYRUN. After model checks, we propose to use Picard method (known also as fixed point) to initialise zone pressures. A linear system (extracted from the non linear set of equations) is solved around 10 times at each time step and NR uses this result for initial values. Known to be uniformly but slowly convergent, this method appears to be really powerful for the building pressure system. The comparison of the methods in terms of number of iterations is illustrated using a real test case experiment.",2012-12-20T21:19:35Z,http://arxiv.org/pdf/1212.5275v1,2024-04-28,
1212.5589v1,"CODYRUN, outil de simulation et d'aide à la conception thermo-aéraulique de bâtiments","This article presents the CODYRUN software developped by University of La R\'eunion. It is a multizone thermal software, with detailled airflow and humidity transfer calculations. One of its specific aspects is that it constitutes a research tool, a design tool used by the lab and professionnals and also a teaching tool. After a presentation of the multiple model aspect, some details of the tree modules associated to physical phenomenons are given. Elements of validation are exposed in next paraghaph, and then a few details of the front end.",2012-12-20T21:21:41Z,http://arxiv.org/pdf/1212.5589v1,2024-04-28,
1212.5592v1,"Multiple model software for airflow and thermal building simulation. A case study under tropical humid climate, in Réunion Island","The first purpose of our work has been to allow -as far as heat transfer modes, airflow calculation and meteorological data reconstitution are concerned- the integration of diverse interchangeable physical models in a single software tool for professional use, CODYRUN. The designer's objectives, precision requested and calculation time consideration, lead us to design a structure accepting selective use of models, taking into account multizone description and airflow patterns. With a building case study in Reunion Island, we first analyse the sensibility of the thermal model to diffuse radiation reconstitution on tilted surfaces. Then, a realistic balance between precision required and calculation time leads us to select detailed models for the zone of main interest, but to choose simplified models for the other zones.",2012-12-21T13:50:14Z,http://arxiv.org/pdf/1212.5592v1,2024-04-28,
1212.5593v1,"Time-variant Linear reduction model approximation : application to thermal and airflow building simulation","Considering the natural ventilation, the thermal behavior of buildings can be described by a linear time varying model. In this paper, we describe an implementation of model reduction of linear time varying systems. We show the consequences of the model reduction on computing time and accuracy. Finally, we compare experimental measures and simulation results using the initial model or the reduced model. The reduced model shows negligible difference in accuracy, and the computing time shortens.",2012-12-21T13:52:10Z,http://arxiv.org/pdf/1212.5593v1,2024-04-28,
1212.5599v1,"Elaboration of a new tool for weather data sequences generation","This paper deals about the presentation of a new software RUNEOLE used to provide weather data in buildings physics. RUNEOLE associates three modules leading to the description, the modelling and the generation of weather data. The first module is dedicated to the description of each climatic variable included in the database. Graphic representation is possible (with histograms for example). Mathematical tools used to compare statistical distributions, determine daily characteristic evolutions, find typical days, and the correlations between the different climatic variables have been elaborated in the second module. Artificial weather datafiles adapted to different simulation codes are available at the issue of the third module. This tool can then be used in HVAC system evaluation, or in the study of thermal comfort. The studied buildings can then be tested under different thermal, aeraulic, and radiative solicitations, leading to a best understanding of their behaviour for example in humid climates.",2012-12-21T18:55:04Z,http://arxiv.org/pdf/1212.5599v1,2024-04-28,
1212.5664v1,"Weather sequences for predicting HVAC system behaviour in residential units located in tropical climates","The purpose of our research deals with the description of a methodology for the definition of specific weather sequences and their influence on the energy needs of HVAC system. We'll apply the method on the tropical Reunion Island. The methodological approach based on a detailed analysis of weather sequences leads to a classification of climatic situations that can be applied to the site. These sequences have been used to simulate buildings and air handling systems thanks to a thermal simulation code, CODYRUN. Results bring to the light how necessary it is to have coherent meteorological data for this kind of simulation.",2012-12-22T07:40:43Z,http://arxiv.org/pdf/1212.5664v1,2024-04-28,
1212.5665v1,"Multiple model approach and experimental validation of a residential air-to-air heat pump","The beginning of this work is the achievement of a design tool, which is a multiple model software called "" CODYRUN "", suitable for professionnals and usable by researchers. The original aspect of this software is that the designer has at his disposal a wide panel of choices between different heat transfer models More precisely, it consists in a multizone software integrating both natural ventilation and moisture tranfers . This software is developed on PC micro computer and gets advantage of the Microsoft WINDOWS front-end. Most of time, HVAC systems and specially domestic air conditioners, are taken into account in a very simplified way, or in a elaborated one. On one side,they are just supposed to supply the demand of cooling loads with an ideal control loop (no delay between the sollicitations and the time response of the system), The available outputs are initially the hourly cooling and heating consumptions without integrating the real caracteristics of the HVAC system This paper is also following the same multiple model approach than for the building modelling by defining different modelling levels for the air conditionning systems, from a very simplified one to a detailled one. An experimental validation is achieved in order to compare the sensitivity of each defined model and to point out the interaction between the thermal behaviour of the envelop and the electrical system consumption. For validation purposes, we will describe the data acquisition system. and the used real size test cell located in the University of Reunion island, Indian Ocean.",2012-12-22T07:42:21Z,http://arxiv.org/pdf/1212.5665v1,2024-04-28,
1301.1409v3,"A Dual Number Approach for Numerical Calculation of Velocity and Acceleration in the Spherical 4R Mechanism","This paper proposes a methodology to calculate both the first and second derivatives of a vector function of one variable in a single computation step. The method is based on the nested application of the dual number approach for first order derivatives.   It has been implemented in Fortran language, a module which contains the dual version of elementary functions as well as more complex functions, which are common in the field of rotational kinematics. Since we have three quantities of interest, namely the function itself and its first and second derivative, our basic numerical entity has three elements. Then, for a given vector function $f:\mathbb{R}\to \mathbb{R}^m$, its dual version will have the form $\tilde{f}:\mathbb{R}^3\to \mathbb{R}^{3m}$.   As a study case, the proposed methodology is used to calculate the velocity and acceleration of a point moving on the coupler-point curve generated by a spherical four-bar mechanism.",2013-01-08T04:40:27Z,http://arxiv.org/pdf/1301.1409v3,2024-04-28,
1301.5595v1,"A discrete analysis of metal-v belt drive","The metal-V belt drive includes a large number of parts which interact between them to transmit power from the input to the output pulleys. A compression belt composed of a great number of struts is maintained by a tension flat belt. Power is them shared into the two belts that moves generally in opposite directions. Due to the particular geometry of the elements and to the great number of parts, a numerical approach achieves the global equilibrium of the mechanism from the elementary part equilibrium. Sliding arc on each pulley can be thus defined both for the compression and tension belts. Finally, power sharing can be calculated as differential motion between the belts, is defined. The first part of the paper will present the different steps of the quasi-static mechanical analysis and their numerical implementations. Load distributions, speed profiles and sliding angle values will be discussed. The second part of the paper will deal to a systematic use of the computer software. Speed ratio, transmitted torque, strut geometry and friction coefficients effect will be analysed with the output parameter variations. Finally, the effect pulley deformable flanges will be discussed.",2013-01-20T17:52:48Z,http://arxiv.org/pdf/1301.5595v1,2024-04-28,
1304.1625v1,"Mathematical modeling of thermal stabilization of vertical wells on high performance computing systems","Temperature stabilization of oil and gas wells is used to ensure stability and prevent deformation of a subgrade estuary zone. In this work, we consider the numerical simulation of thermal stabilization using vertical seasonal freezing columns.   A mathematical model of such problems is described by a time-dependent temperature equation with phase transitions from water to ice. The resulting equation is a standard nonlinear parabolic equation.   Numerical implementation is based on the finite element method using the package Fenics. After standard purely implicit approximation in time and simple linearization, we obtain a system of linear algebraic equations. Because the size of freezing columns are substantially less than the size of the modeled area, we obtain mesh refinement near columns. Due to this, we get a large system of equations which are solved using high performance computing systems.",2013-04-05T07:13:58Z,http://arxiv.org/pdf/1304.1625v1,2024-04-28,
1304.2170v1,"On sampling SCJ rearrangement scenarios","The Single Cut or Join (SCJ) operation on genomes, generalizing chromosome evolution by fusions and fissions, is the computationally simplest known model of genome rearrangement. While most genome rearrangement problems are already hard when comparing three genomes, it is possible to compute in polynomial time a most parsimonious SCJ scenario for an arbitrary number of genomes related by a binary phylogenetic tree.   Here we consider the problems of sampling and counting the most parsimonious SCJ scenarios. We show that both the sampling and counting problems are easy for two genomes, and we relate SCJ scenarios to alternating permutations. However, for an arbitrary number of genomes related by a binary phylogenetic tree, the counting and sampling problems become hard. We prove that if a Fully Polynomial Randomized Approximation Scheme or a Fully Polynomial Almost Uniform Sampler exist for the most parsimonious SCJ scenario, then RP = NP.   The proof has a wider scope than genome rearrangements: the same result holds for parsimonious evolutionary scenarios on any set of discrete characters.",2013-04-08T11:22:12Z,http://arxiv.org/pdf/1304.2170v1,2024-04-28,
1304.6099v2,"Soft computing-based calibration of microplane M4 model parameters: Methodology and validation","Constitutive models for concrete based on the microplane concept have repeatedly proven their ability to well-reproduce its non-linear response on material as well as structural scales. The major obstacle to a routine application of this class of models is, however, the calibration of microplane-related constants from macroscopic data. The goal of this paper is two-fold: (i) to introduce the basic ingredients of a robust inverse procedure for the determination of dominant parameters of the M4 model proposed by Bazant and co-workers based on cascade Artificial Neural Networks trained by Evolutionary Algorithm and (ii) to validate the proposed methodology against a representative set of experimental data. The obtained results demonstrate that the soft computing-based method is capable of delivering the searched response with an accuracy comparable to the values obtained by expert users.",2013-04-19T08:11:55Z,http://arxiv.org/pdf/1304.6099v2,2024-04-28,
1304.7226v1,"Lay-up Optimization of Laminated Composites: Mixed Approach with Exact Feasibility Bounds on Lamination Parameters","We suggest modified bi-level approach for finding the best stacking sequence of laminated composite structures subject to mechanical, blending and manufacturing constraints. We propose to use both the number of plies laid up at predefined angles and lamination parameters as independent variables at outer (global) stage of bi-level scheme aimed to satisfy buckling, strain and percentage constraints. Our formulation allows precise definition of the feasible region of lamination parameters and greatly facilitates the solution of inner level problem of finding the optimal stacking sequence.",2013-04-26T16:48:48Z,http://arxiv.org/pdf/1304.7226v1,2024-04-28,
1305.0596v1,"An Empirical Investigation of V-I Trajectory based Load Signatures for Non-Intrusive Load Monitoring","Choice of load signature or feature space is one of the most fundamental design choices for non-intrusive load monitoring or energy disaggregation problem. Electrical power quantities, harmonic load characteristics, canonical transient and steady-state waveforms are some of the typical choices of load signature or load signature basis for current research addressing appliance classification and prediction. This paper expands and evaluates appliance load signatures based on V-I trajectory - the mutual locus of instantaneous voltage and current waveforms - for precision and robustness of prediction in classification algorithms used to disaggregate residential overall energy use and predict constituent appliance profiles. We also demonstrate the use of variants of differential evolution as a novel strategy for selection of optimal load models in context of energy disaggregation. A publicly available benchmark dataset REDD is employed for evaluation purposes. Our experimental evaluations indicate that these load signatures, in conjunction with a number of popular classification algorithms, offer better or generally comparable overall precision of prediction, robustness and reliability against dynamic, noisy and highly similar load signatures with reference to electrical power quantities and harmonic content. Herein, wave-shape features are found to be an effective new basis of classification and prediction for semi-automated energy disaggregation and monitoring.",2013-05-02T23:32:00Z,http://arxiv.org/pdf/1305.0596v1,2024-04-28,
1305.1112v1,"json2run: a tool for experiment design & analysis","json2run is a tool to automate the running, storage and analysis of experiments. The main advantage of json2run is that it allows to describe a set of experiments concisely as a JSON-formatted parameter tree. It also supports parallel execution of experiments, automatic parameter tuning through the F-Race framework and storage and analysis of experiments with MongoDB and R.",2013-05-06T08:31:48Z,http://arxiv.org/pdf/1305.1112v1,2024-04-28,
1305.2322v1,"Simulation of a typical house in the region of Antananarivo, Madagascar. Determination of passive solutions using local materials","This paper deals with new proposals for the design of passive solutions adapted to the climate of the highlands of Madagascar. While the strongest population density is located in the central highlands, the problem of thermal comfort in buildings occurs mainly during winter time. Currently, people use raw wood to warm the poorly designed houses. This leads to a large scale deforestation of the areas and causes erosion and environmental problems. The methodology used consisted of the identification of a typical building and of a typical meteorological year. Simulations were carried out using a thermal and airflow software (CODYRUN) to improve each building component (roof, walls, windows, and soil) in such a way as to estimate the influence of some technical solutions on each component in terms of thermal comfort. The proposed solutions also took into account the use of local materials and the standard of living of the country.",2013-05-10T12:16:41Z,http://arxiv.org/pdf/1305.2322v1,2024-04-28,
1305.7422v1,"Evaluating Different Cost-Benefit Analysis Methods for Port Security Operations","Service industries, such as ports, are attentive to their standards, a smooth service flow and economic viability. Cost benefit analysis has proven itself as a useful tool to support this type of decision making; it has been used by businesses and governmental agencies for many years. In this book chapter we demonstrate different modelling methods that are used for estimating input factors required for conducting cost benefit analysis based on a single case study. These methods are: scenario analysis, decision trees, Monte-Carlo simulation modelling and discrete event simulation modelling. Our aims are, on the one hand, to guide the analyst through the modelling processes and, on the other hand, to demonstrate what additional decision support information can be obtained from applying each of these modelling methods.",2013-05-31T14:36:59Z,http://arxiv.org/pdf/1305.7422v1,2024-04-28,
1305.7424v1,"Investigating the effectiveness of Variance Reduction Techniques in Manufacturing, Call Center and Cross-docking Discrete Event Simulation Models","Variance reduction techniques have been shown by others in the past to be a useful tool to reduce variance in Simulation studies. However, their application and success in the past has been mainly domain specific, with relatively little guidelines as to their general applicability, in particular for novices in this area. To facilitate their use, this study aims to investigate the robustness of individual techniques across a set of scenarios from different domains. Experimental results show that Control Variates is the only technique which achieves a reduction in variance across all domains. Furthermore, applied individually, Antithetic Variates and Control Variates perform particularly well in the Cross-docking scenarios, which was previously unknown.",2013-05-31T14:39:34Z,http://arxiv.org/pdf/1305.7424v1,2024-04-28,
1306.2019v1,"Proceedings Fourth International Workshop on Computational Models for Cell Processes","The fourth international workshop on Computational Models for Cell Processes (CompMod 2013) took place on June 11, 2013 at the {\AA}bo Akademi University, Turku, Finland, in conjunction with iFM 2013. The first edition of the workshop (2008) took place in Turku, Finland, in conjunction with Formal Methods 2008, the second edition (2009) took place in Eindhoven, the Netherlands, as well in conjunction with Formal Methods 2009, and the third one took place in Aachen, Germany, in conjunction with CONCUR 2013. This volume contains the final versions of all contributions accepted for presentation at the workshop.   The goal of the CompMod workshop series is to bring together researchers in Computer Science and Mathematics (both discrete and continuous), interested in the opportunities and the challenges of Systems Biology. The Program Committee of CompMod 2013 selected 3 papers for presentation at the workshop. In addition, we had two invited talks and five informal presentations.   The scientific program of the workshop spans an interesting mix of approaches to systems and even synthetic biology, encompassing several different modeling approaches, ranging from quantitative to qualitative techniques, from continuous to discrete mathematics, and from deterministic to stochastic methods. We thank our invited speakers Daniela Besozzi (Universita degli Studi di Milano, Milano, Italy) and Juho Rousu (Aalto University, Finland) for accepting our invitation and for presenting some of their recent results at CompMod 2013.   The technical contributions address the mathematical modeling of the PDGF signalling pathway, the canonical labelling of site graphs, rule-based modeling of polymerization reactions, rule-based modeling as a platform for the analysis of synthetic self-assembled nano-systems, robustness analysis of stochastic systems, an algebraic approach to gene assembly in ciliates, and large-scale text mining of biomedical literature.",2013-06-09T13:42:56Z,http://arxiv.org/pdf/1306.2019v1,2024-04-28,
1306.2843v3,"On Some Recent Insights in Integral Biomathics","This paper summarizes the results in Integral Biomathics obtained to this moment and provides an outlook for future research in the field.",2013-06-11T05:45:57Z,http://arxiv.org/pdf/1306.2843v3,2024-04-28,
1306.3011v2,"Proximity-Aware Calculation of Cable Series Impedance for Systems of Solid and Hollow Conductors","Wide-band cable models for the prediction of electromagnetic transients in power systems require the accurate calculation of the cable series impedance as function of frequency. A surface current approach was recently proposed for systems of round solid conductors, with inclusion of skin and proximity effects. In this paper we extend the approach to include tubular conductors, allowing to model realistic cables with tubular sheaths, armors and pipes. We also include the effect of a lossy ground. A noteworthy feature of the proposed technique is the accurate prediction of proximity effects, which can be of major importance in three-phase, pipe type, and closely-packed single-core cables. The new approach is highly efficient compared to finite elements. In the case of a cross-bonded cable system featuring three phase conductors and three screens, the proposed technique computes the required 120 frequency samples in only six seconds of CPU time.",2013-06-13T02:49:13Z,http://arxiv.org/pdf/1306.3011v2,2024-04-28,
1306.4092v1,"Application of particle swarm optimization for enhanced cyclic steam stimulation in a offshore heavy oil reservoir","Three different variations of PSO algorithms, i.e. Canonical, Gaussian Bare-bone and L\'evy Bare-bone PSO, are tested to optimize the ultimate oil recovery of a large heavy oil reservoir. The performance of these algorithms was compared in terms of convergence behaviour and the final optimization results. It is found that, in general, all three types of PSO methods are able to improve the objective function. The best objective function is found by using the Canonical PSO, while the other two methods give similar results. The Gaussian Bare-bone PSO may picks positions that are far away from the optimal solution. The L\'evy Bare-bone PSO has similar convergence behaviour as the Canonical PSO. For the specific optimization problem investigated in this study, it is found that the temperature of the injection steam, CO2 composition in the injection gas, and the gas injection rates have bigger impact on the objective function, while steam injection rate and the liquid production rate have less impact on the objective function.",2013-06-18T07:42:20Z,http://arxiv.org/pdf/1306.4092v1,2024-04-28,
1306.5160v1,"Towards modelling cost and risks of infrequent events in the cargo screening process","We introduce a simulation model of the port of Calais with a focus on the operation of immigration controls. Our aim is to compare the cost and benefits of different screening policies. Methodologically, we are trying to understand the limits of discrete event simulation of rare events. When will they become 'too rare' for simulation to give meaningful results?",2013-06-21T14:49:29Z,http://arxiv.org/pdf/1306.5160v1,2024-04-28,
1307.0747v1,"Simulating the Dynamics of T Cell Subsets Throughout the Lifetime","It is widely accepted that the immune system undergoes age-related changes correlating with increased disease in the elderly. T cell subsets have been implicated. The aim of this work is firstly to implement and validate a simulation of T regulatory cell (Treg) dynamics throughout the lifetime, based on a model by Baltcheva. We show that our initial simulation produces an inversion between precursor and mature Treys at around 20 years of age, though the output differs significantly from the original laboratory dataset. Secondly, this report discusses development of the model to incorporate new data from a cross-sectional study of healthy blood donors addressing balance between Treys and Th17 cells with novel markers for Treg. The potential for simulation to add insight into immune aging is discussed.",2013-07-02T16:19:54Z,http://arxiv.org/pdf/1307.0747v1,2024-04-28,
1307.0749v1,"Comparing Decison Support Tools for Cargo Screening Processes","When planning to change operations at ports there are two key stake holders with very different interests involved in the decision making processes. Port operators are attentive to their standards, a smooth service flow and economic viability while border agencies are concerned about national security. The time taken for security checks often interferes with the compliance to service standards that port operators would like to achieve. Decision support tools as for example Cost-Benefit Analysis or Multi Criteria Analysis are useful helpers to better understand the impact of changes to a system. They allow investigating future scenarios and helping to find solutions that are acceptable for all parties involved in port operations. In this paper we evaluate two different modelling methods, namely scenario analysis and discrete event simulation. These are useful for driving the decision support tools (i.e. they provide the inputs the decision support tools require). Our aims are, on the one hand, to guide the reader through the modelling processes and, on the other hand, to demonstrate what kind of decision support information one can obtain from the different modelling methods presented.",2013-07-02T16:31:44Z,http://arxiv.org/pdf/1307.0749v1,2024-04-28,
1307.1073v1,"Modelling Reactive and Proactive Behaviour in Simulation: A Case Study in a University Organisation","Simulation is a well established what-if scenario analysis tool in Operational Research (OR). While traditionally Discrete Event Simulation (DES) and System Dynamics Simulation (SDS) are the predominant simulation techniques in OR, a new simulation technique, namely Agent-Based Simulation (ABS), has emerged and is gaining more attention. In our research we focus on discrete simulation methods (i.e. DES and ABS). The contribution made by this paper is the comparison of DES and combined DES/ABS for modelling human reactive and different level of detail of human proactive behaviour in service systems. The results of our experiments show that the level of proactiveness considered in the model has a big impact on the simulation output. However, there is not a big difference between the results from the DES and the combined DES/ABS simulation models. Therefore, for service systems of the type we investigated we would suggest to use DES as the preferred analysis tool.",2013-07-03T16:43:23Z,http://arxiv.org/pdf/1307.1073v1,2024-04-28,
1307.1466v1,"Detect adverse drug reactions for the drug Pravastatin","Adverse drug reaction (ADR) is widely concerned for public health issue. ADRs are one of most common causes to withdraw some drugs from market. Prescription event monitoring (PEM) is an important approach to detect the adverse drug reactions. The main problem to deal with this method is how to automatically extract the medical events or side effects from high-throughput medical data, which are collected from day to day clinical practice. In this study we propose an original approach to detect the ADRs using feature matrix and feature selection. The experiments are carried out on the drug Pravastatin. Major side effects for the drug are detected. The detected ADRs are based on computerized method, further investigation is needed.",2013-07-04T16:42:02Z,http://arxiv.org/pdf/1307.1466v1,2024-04-28,
1307.1597v2,"A Beginners Guide to Systems Simulation in Immunology","Some common systems modelling and simulation approaches for immune problems are Monte Carlo simulations, system dynamics, discrete-event simulation and agent-based simulation. These methods, however, are still not widely adopted in immunology research. In addition, to our knowledge, there is few research on the processes for the development of simulation models for the immune system. Hence, for this work, we have two contributions to knowledge. The first one is to show the importance of systems simulation to help immunological research and to draw the attention of simulation developers to this research field. The second contribution is the introduction of a quick guide containing the main steps for modelling and simulation in immunology, together with challenges that occur during the model development. Further, this paper introduces an example of a simulation problem, where we test our guidelines.",2013-07-05T12:43:04Z,http://arxiv.org/pdf/1307.1597v2,2024-04-28,
1307.1598v1,"Extending a Microsimulation of the Port of Dover","Modelling and simulating the traffic of heavily used but secure environments such as seaports and airports is of increasing importance. This paper discusses issues and problems that may arise when extending an existing microsimulation strategy. We also discuss how extensions of these simulations can aid planners with optimal physical and operational feedback. Conclusions are drawn about how microsimulations can be moved forward as a robust planning tool for the 21st century.",2013-07-05T12:46:33Z,http://arxiv.org/pdf/1307.1598v1,2024-04-28,
1307.1625v2,"Robust Causality Check for Sampled Scattering Parameters via a Filtered Fourier Transform","We introduce a robust numerical technique to verify the causality of sampled scattering parameters given on a finite bandwidth. The method is based on a filtered Fourier transform and includes a rigorous estimation of the errors caused by missing out-of-band samples. Compared to existing techniques, the method is simpler to implement and provides a useful insight on the time-domain characteristics of the detected violation. Through an applicative example, we shows its usefulness to improve the accuracy and reliability of macromodeling techniques used to convert sampled scattering parameters into models for transient analysis.",2013-07-05T14:45:00Z,http://arxiv.org/pdf/1307.1625v2,2024-04-28,
1307.5076v1,"Low-rank Approximations for Computing Observation Impact in 4D-Var Data Assimilation","We present an efficient computational framework to quantify the impact of individual observations in four dimensional variational data assimilation. The proposed methodology uses first and second order adjoint sensitivity analysis, together with matrix-free algorithms to obtain low-rank approximations of ob- servation impact matrix. We illustrate the application of this methodology to important applications such as data pruning and the identification of faulty sensors for a two dimensional shallow water test system.",2013-07-18T20:34:20Z,http://arxiv.org/pdf/1307.5076v1,2024-04-28,
1307.7757v4,"Household Electricity Consumption Data Cleansing","Load curve data in power systems refers to users' electrical energy consumption data periodically collected with meters. It has become one of the most important assets for modern power systems. Many operational decisions are made based on the information discovered in the data. Load curve data, however, usually suffers from corruptions caused by various factors, such as data transmission errors or malfunctioning meters. To solve the problem, tremendous research efforts have been made on load curve data cleansing. Most existing approaches apply outlier detection methods from the supply side (i.e., electricity service providers), which may only have aggregated load data. In this paper, we propose to seek aid from the demand side (i.e., electricity service users). With the help of readily available knowledge on consumers' appliances, we present a new appliance-driven approach to load curve data cleansing. This approach utilizes data generation rules and a Sequential Local Optimization Algorithm (SLOA) to solve the Corrupted Data Identification Problem (CDIP). We evaluate the performance of SLOA with real-world trace data and synthetic data. The results indicate that, comparing to existing load data cleansing methods, such as B-spline smoothing, our approach has an overall better performance and can effectively identify consecutive corrupted data. Experimental results also demonstrate that our method is robust in various tests. Our method provides a highly feasible and reliable solution to an emerging industry application.",2013-07-29T22:22:56Z,http://arxiv.org/pdf/1307.7757v4,2024-04-28,
1308.1365v1,"Mathematical model of concentrating solar cooker","The main purpose of this work is to obtain a mathematical model consistent with the thermal behavior of concentrating solar cookers, such as Jorhejpataranskua. We also want to simulate different conditions respect to the parameters involved of several materials for its construction and efficiency. The model is expressed in terms of a coupled nonlinear system of differential equations which are solved using Mathematica 8. The results obtained by our model are compared with measurements of solar cooker in field testing operation. We obtained good results in agreement with experimental data. Moreover, the simulation results are used by calculating cooking power and standardized cooking power of solar cooker for different parameters.",2013-08-06T18:11:15Z,http://arxiv.org/pdf/1308.1365v1,2024-04-28,
1308.1860v2,"An Optimization Framework to Improve 4D-Var Data Assimilation System Performance","This paper develops a computational framework for optimizing the parameters of data assimilation systems in order to improve their performance. The approach formulates a continuous meta-optimization problem for parameters; the meta-optimization is constrained by the original data assimilation problem. The numerical solution process employs adjoint models and iterative solvers. The proposed framework is applied to optimize observation values, data weighting coefficients, and the location of sensors for a test problem. The ability to optimize a distributed measurement network is crucial for cutting down operating costs and detecting malfunctions.",2013-08-08T14:16:28Z,http://arxiv.org/pdf/1308.1860v2,2024-04-28,
1308.2773v3,"Wind Speed Data Analysis for Various Seasons during a Decade by Wavelet and S transform","The appropriate weather prediction is a challenging task and it can be feasible with proper wind speed fluctuation analysis. In this current paper daubechies-4 wavelet is used to analyze the winter wind speed fluctuations due to lesser agitated wind data samples of winter. In summer abrupt changes in wind speed occurs which creates difficulty for wavelets to keep proper track of wind speed fluctuations. So, in that case the concept of the S-transform is introduced.",2013-08-13T07:16:03Z,http://arxiv.org/pdf/1308.2773v3,2024-04-28,
1308.4801v1,"The Mapping of Simulated Climate-Dependent Building Innovations","Performances of building energy innovations are most of the time dependent on the external climate conditions. This means a high performance of a specific innovation in a certain part of Europe, does not imply the same performances in other regions. The mapping of simulated building performances at the EU scale could prevent the waste of potential good ideas by identifying the best region for a specific innovation. This paper presents a methodology for obtaining maps of performances of building innovations that are virtually spread over whole Europe. It is concluded that these maps are useful for finding regions at the EU where innovations have the highest expected performances.",2013-08-22T09:20:15Z,http://arxiv.org/pdf/1308.4801v1,2024-04-28,
1308.5144v1,"Detect adverse drug reactions for drug Pioglitazone","In this study we propose a novel method to successfully detect the ADRs using feature matrix and feature selection. A feature matrix, which characterizes the medical events before patients take drugs or after patients take drugs, is created from THIN database. The feature selection method of Student's t-test is used to detect the significant features from thousands of medical events. The significant ADRs, which are corresponding to significant features, are detected. Experiments are performed on the drug Pioglitazone. Compared to other computerized methods, our proposed method achieves good performance.",2013-08-23T14:44:29Z,http://arxiv.org/pdf/1308.5144v1,2024-04-28,
1308.6697v1,"Detect adverse drug reactions for drug Atorvastatin","Adverse drug reactions (ADRs) are big concern for public health. ADRs are one of most common causes to withdraw some drugs from markets. Now two major methods for detecting ADRs are spontaneous reporting system (SRS), and prescription event monitoring (PEM). The World Health Organization (WHO) defines a signal in pharmacovigilance as ""any reported information on a possible causal relationship between an adverse event and a drug, the relationship being unknown or incompletely documented previously"". For spontaneous reporting systems, many machine learning methods are used to detect ADRs, such as Bayesian confidence propagation neural network (BCPNN), decision support methods, genetic algorithms, knowledge based approaches, etc. One limitation is the reporting mechanism to submit ADR reports, which has serious underreporting and is not able to accurately quantify the corresponding risk. Another limitation is hard to detect ADRs with small number of occurrences of each drug-event association in the database. In this paper we propose feature selection approach to detect ADRs from The Health Improvement Network (THIN) database. First a feature matrix, which represents the medical events for the patients before and after taking drugs, is created by linking patients' prescriptions and corresponding medical events together. Then significant features are selected based on feature selection methods, comparing the feature matrix before patients take drugs with one after patients take drugs. Finally the significant ADRs can be detected from thousands of medical events based on corresponding features. Experiments are carried out on the drug Atorvastatin. Good performance is achieved.",2013-08-30T09:55:56Z,http://arxiv.org/pdf/1308.6697v1,2024-04-28,
1309.4429v1,"Comsol Simulations of Cracking in Point Loaded Masonry with Randomly Distributed Material Properties","This paper describes COMSOL simulations of the stress and crack development in the area where a masonry wall supports a floor. In these simulations one of the main material properties of calcium silicate, its E-value, was assigned randomly to the finite elements of the modeled specimen. Calcium silicate is a frequently used building material with a relatively brittle fracture characteristic. Its initial E-value varies, as well as tensile strength and post peak behavior. Therefore, in the simulation, initial E-values were randomly assigned to the elements of the model and a step function used for describing the descending branch. The method also allows for variation in strength to be taken into account in future research. The performed non-linear simulation results are compared with experimental findings. They show the stress distribution and cracking behavior in point loaded masonry when varying material properties are used.",2013-09-17T19:12:06Z,http://arxiv.org/pdf/1309.4429v1,2024-04-28,
1309.5677v1,"Checkerboard Problem to Topology Optimization of Continuum Structures","The area of topology optimization of continuum structures of which is allowed to change in order to improve the performance is now dominated by methods that employ the material distribution concept. The typical methods of the topology optimization based on the structural optimization of two phase composites are the so-called variable density ones, like the SIMP (Solid Isotropic Material with Penalization) and the BESO (Bi-directional Evolutional Structure Optimization). The topology optimization problem refers to the saddle-point variation one as well as the so-called Stokes flow problem of the compressive fluid. The checkerboard patterns often appear in the results computed by the SIMP and the BESO in which the Q1-P0 element is used for FEM (Finite Element Method), since these patterns are more favourable than uniform density regions. Computational experiments of SIMP and BESO have shown that filtering of sensitivity information of the optimization problem is a highly efficient way that the checkerboard patterns disappeared and to ensure mesh-independency. SIn this paper, we discuss the theoretical basis for the filtering method of the SIMP and the BESO and as a result, the filtering method can be understood by the theorem of partition of unity and the convolution operator of low-pass filter.",2013-09-23T01:16:42Z,http://arxiv.org/pdf/1309.5677v1,2024-04-28,
1310.0068v2,"Automatic estimation of the regularization parameter in 2-D focusing gravity inversion: an application to the Safo manganese mine in northwest of Iran","We investigate the use of Tikhonov regularization with the minimum support stabilizer for underdetermined 2-D inversion of gravity data. This stabilizer produces models with non-smooth properties which is useful for identifying geologic structures with sharp boundaries. A very important aspect of using Tikhonov regularization is the choice of the regularization parameter that controls the trade off between the data fidelity and the stabilizing functional. The L-curve and generalized cross validation techniques, which only require the relative sizes of the uncertainties in the observations are considered. Both criteria are applied in an iterative process for which at each iteration a value for regularization parameter is estimated. Suitable values for the regularization parameter are successfully determined in both cases for synthetic but practically relevant examples. Whenever the geologic situation permits, it is easier and more efficient to model the subsurface with a 2-D algorithm, rather than to apply a full 3-D approach. Then, because the problem is not large it is appropriate to use the generalized singular value decomposition for solving the problem efficiently. The method is applied on a profile of gravity data acquired over the Safo mining camp in Maku-Iran, which is well known for manganese ores. The presented results demonstrate success in reconstructing the geometry and density distribution of the subsurface source.",2013-09-30T21:43:25Z,http://arxiv.org/pdf/1310.0068v2,2024-04-28,
1310.2361v1,"Survey on Modelling Methods Applicable to Gene Regulatory Network","Gene Regulatory Network (GRN) plays an important role in knowing insight of cellular life cycle. It gives information about at which different environmental conditions genes of particular interest get over expressed or under expressed. Modelling of GRN is nothing but finding interactive relationships between genes. Interaction can be positive or negative. For inference of GRN, time series data provided by Microarray technology is used. Key factors to be considered while constructing GRN are scalability, robustness, reliability and maximum detection of true positive interactions between genes. This paper gives detailed technical review of existing methods applied for building of GRN along with scope for future work.",2013-10-09T05:58:26Z,http://arxiv.org/pdf/1310.2361v1,2024-04-28,
1310.3360v1,"A Probabilistic Approach to Risk Mapping for Mt. Etna","We evaluate susceptibility to lava flows on Mt. Etna based on specially designed die-toss experiments using probabilities for type, time and place of activation from the volcano's 400-year recorded history and current studies on its known fractures and fissures. The types of activations were forcast using a table of probabilities for events, typed by duration and volume of ejecta. Lengths of time were represented by the number of activations to expect within a given time-frame, calculated assuming Poisson-distributed inter-arrival times for activations. Locations of future activations were forecast with a probability distribution function for activation probabilities. Most likely scenarios for risk and resulting topography were generated for Etna's next activation (average 7.76 years), the next 25, 50 and 100 years. Forecasts for areas most likely affected are in good agreement with previous risk studies made. Forecasts for risks of lava invasions, as well as future topographies might be a first. Threats to lifelines are also discussed.",2013-10-12T11:08:54Z,http://arxiv.org/pdf/1310.3360v1,2024-04-28,
1310.6876v2,"Application of Fourier and Wavelet Transform for analysing 300 years Sunspot numbers to Explain the Solar Cycles","In this paper Fourier Transform and Wavelet Transform are applied in case of recent 300 years of sunspot numbers to explain the solar cycles. Here basically parallel study of Fourier and Wavelet analysis are done and we have observed that the better result can be obtained from Wavelet analysis during sunspot number analysis. We are able to show various minima and maxima in the recent ages of solar cycles with this tool. The exact periodicity and other possible periodicities in the cyclic phenomenon of sunspot activity are determined.",2013-10-25T11:00:02Z,http://arxiv.org/pdf/1310.6876v2,2024-04-28,
1312.0264v3,"Competitive Fragmentation Modeling of ESI-MS/MS spectra for putative metabolite identification","Electrospray tandem mass spectrometry (ESI-MS/MS) is commonly used in high throughput metabolomics. One of the key obstacles to the effective use of this technology is the difficulty in interpreting measured spectra to accurately and efficiently identify metabolites. Traditional methods for automated metabolite identification compare the target MS or MS/MS spectrum to the spectra in a reference database, ranking candidates based on the closeness of the match. However the limited coverage of available databases has led to an interest in computational methods for predicting reference MS/MS spectra from chemical structures.   This work proposes a probabilistic generative model for the MS/MS fragmentation process, which we call Competitive Fragmentation Modeling (CFM), and a machine learning approach for learning parameters for this model from MS/MS data. We show that CFM can be used in both a MS/MS spectrum prediction task (ie, predicting the mass spectrum from a chemical structure), and in a putative metabolite identification task (ranking possible structures for a target MS/MS spectrum).   In the MS/MS spectrum prediction task, CFM shows significantly improved performance when compared to a full enumeration of all peaks corresponding to substructures of the molecule. In the metabolite identification task, CFM obtains substantially better rankings for the correct candidate than existing methods (MetFrag and FingerID) on tripeptide and metabolite data, when querying PubChem or KEGG for candidate structures of similar mass.",2013-12-01T19:20:57Z,http://arxiv.org/pdf/1312.0264v3,2024-04-28,
1312.2841v1,"Predictive Comparative QSAR Analysis Of As 5-Nitofuran-2-YL Derivatives Myco bacterium tuberculosis H37RV Inhibitors Bacterium Tuberculosis H37RV Inhibitors","Antitubercular activity of 5-nitrofuran-2-yl Derivatives series were subjected to Quantitative Structure Activity Relationship (QSAR) Analysis with an effort to derive and understand a correlation between the biological activity as response variable and different molecular descriptors as independent variables. QSAR models are built using 40 molecular descriptor dataset. Different statistical regression expressions were got using Partial Least Squares (PLS),Multiple Linear Regression (MLR) and Principal Component Regression (PCR) techniques. The among these technique, Partial Least Square Regression (PLS) technique has shown very promising result as compared to MLR technique A QSAR model was build by a training set of 30 molecules with correlation coefficient ($r^2$) of 0.8484, significant cross validated correlation coefficient ($q^2$) is 0.0939, F test is 48.5187, ($r^2$) for external test set (pred$_r^2$) is -0.5604, coefficient of correlation of predicted data set (pred$_r^2se$) is 0.7252 and degree of freedom is 26 by Partial Least Squares Regression technique.",2013-12-10T15:50:39Z,http://arxiv.org/pdf/1312.2841v1,2024-04-28,
1312.2859v1,"A Robust Missing Value Imputation Method MifImpute For Incomplete Molecular Descriptor Data And Comparative Analysis With Other Missing Value Imputation Methods","Missing data imputation is an important research topic in data mining. Large-scale Molecular descriptor data may contains missing values (MVs). However, some methods for downstream analyses, including some prediction tools, require a complete descriptor data matrix. We propose and evaluate an iterative imputation method MiFoImpute based on a random forest. By averaging over many unpruned regression trees, random forest intrinsically constitutes a multiple imputation scheme. Using the NRMSE and NMAE estimates of random forest, we are able to estimate the imputation error. Evaluation is performed on two molecular descriptor datasets generated from a diverse selection of pharmaceutical fields with artificially introduced missing values ranging from 10% to 30%. The experimental result demonstrates that missing values has a great impact on the effectiveness of imputation techniques and our method MiFoImpute is more robust to missing value than the other ten imputation methods used as benchmark. Additionally, MiFoImpute exhibits attractive computational efficiency and can cope with high-dimensional data.",2013-12-10T16:24:28Z,http://arxiv.org/pdf/1312.2859v1,2024-04-28,
1312.2861v1,"Identification Of Outliers In Oxazolines AND Oxazoles High Dimension Molecular Descriptor Dataset Using Principal Component Outlier Detection Algorithm And Comparative Numerical Study Of Other Robust Estimators","From the past decade outlier detection has been in use. Detection of outliers is an emerging topic and is having robust applications in medical sciences and pharmaceutical sciences. Outlier detection is used to detect anomalous behaviour of data. Typical problems in Bioinformatics can be addressed by outlier detection. A computationally fast method for detecting outliers is shown, that is particularly effective in high dimensions. PrCmpOut algorithm make use of simple properties of principal components to detect outliers in the transformed space, leading to significant computational advantages for high dimensional data. This procedure requires considerably less computational time than existing methods for outlier detection. The properties of this estimator (Outlier error rate (FN), Non-Outlier error rate(FP) and computational costs) are analyzed and compared with those of other robust estimators described in the literature through simulation studies. Numerical evidence based Oxazolines and Oxazoles molecular descriptor dataset shows that the proposed method performs well in a variety of situations of practical interest. It is thus a valuable companion to the existing outlier detection methods.",2013-12-10T16:35:25Z,http://arxiv.org/pdf/1312.2861v1,2024-04-28,
1312.3808v1,"Information Maps: A Practical Approach to Position Dependent Parameterization","In this contribution a practical approach to determine and store position dependent parameters is presented. These parameters can be obtained, among others, using experimental results or expert knowledge and are stored in 'Information Maps'. Each Information Map can be interpreted as a kind of static grid map and the framework allows to link different maps hierarchically. The Information Maps can be local or global, with static and dynamic information in it. One application of Information Maps is the representation of position dependent characteristics of a sensor. Thus, for instance, it is feasible to store arbitrary attributes of a sensor's preprocessing in an Information Map and utilize them by simply taking the map value at the current position. This procedure is much more efficient than using the attributes of the sensor itself. Some examples where and how Information Maps can be used are presented in this publication. The Information Map is meant to be a simple and practical approach to the problem of position dependent parameterization in all kind of algorithms when the analytical description is not possible or can not be implemented efficiently.",2013-12-13T13:45:52Z,http://arxiv.org/pdf/1312.3808v1,2024-04-28,
1312.3858v1,"Computational impact of hydrophobicity in protein stability","Among the various features of amino acids, the hydrophobic property has most visible impact on stability of a sequence folding. This is mentioned in many protein folding related work, in this paper we more elaborately discuss the computational impact of the well defined hydrophobic aspect in determining stability, approach with the help of a developed free energy computing algorithm covering various aspects preprocessing of an amino acid sequence, generating the folding and calculating free energy. Later discussing its use in protein structure related research work.",2013-12-13T16:19:35Z,http://arxiv.org/pdf/1312.3858v1,2024-04-28,
1401.1152v3,"Hygro-thermo-mechanical analysis of spalling in concrete walls at high temperatures as a moving boundary problem","A mathematical model allowing coupled hygro-thermo-mechanical analysis of spalling in concrete walls at high temperatures by means of the moving boundary problem is presented. A simplified mechanical approach to account for effects of thermal stresses and pore pressure build-up on spalling is incorporated into the model. The numerical algorithm based on finite element discretization in space and the semi-implicit method for discretization in time is presented. The validity of the developed model is carefully examined by a comparison between experimental tests performed by Kalifa et al. (2000) and Mindeguia (2009) on concrete prismatic specimens under unidirectional heating of temperature of 600 ${\deg}$C and ISO 834 fire curve and the results obtained from the numerical model.",2014-01-06T17:42:41Z,http://arxiv.org/pdf/1401.1152v3,2024-04-28,
1401.5162v1,"A Simple Software Application for Simulating Commercially Available Solar Panels","This article addresses the formulation and validation of a simple PC based software application developed for simulating commercially available solar panels. The important feature of this application is its capability to produce speedy results in the form of solar panel output characteristics at given environmental conditions by using minimal input data. Besides, it is able to deliver critical information about the maximum power point of the panel at a given environmental condition in quick succession. The application is based on a standard equation which governs solar panels and works by means of estimating unknown parameters in the equation to fit a given solar panel. The process of parameter estimation is described in detail with the aid of equations and data of a commercial solar panel. A validation of obtained results for commercial solar panels is also presented by comparing the panel manufacturers' results with the results generated by the application. In addition, implications of the obtained results are discussed along with possible improvements to the developed software application.",2014-01-21T03:20:10Z,http://arxiv.org/pdf/1401.5162v1,2024-04-28,
1401.5791v1,"Advanced Signal Processing Techniqes to Study Normal and Epileptic EEG","EEG monitoring has an important milestone provide valuable information of those candidates who suffer from epilepsy.In this paper human normal and epileptic Electroencephalogram signals are analyzed with popular and efficient signal processing techniques like Fourier and Wavelet transform. The delta, theta, alpha, beta and gamma sub bands of EEG are obtained and studied for detection of seizure and epilepsy. The extracted feature is then applied to ANN for classification of the EEG signals.",2014-01-22T11:22:19Z,http://arxiv.org/pdf/1401.5791v1,2024-04-28,
1401.6759v1,"Modeling the behavior of reinforced concrete walls under fire, considering the impact of the span on firewalls","Numerical modeling using computers is known to present several advantages compared to experimental testing. The high cost and the amount of time required to prepare and to perform a test were among the main problems on the table when the first tools for modeling structures in fire were developed. The discipline structures-in-fire modeling is still currently the subject of important research efforts around the word, those research efforts led to develop many software. In this paper, our task is oriented to the study of fire behavior and the impact of the span reinforced concrete walls with different sections belonging to a residential building braced by a system composed of porticoes and sails. Regarding the design and mechanical loading (compression forces and moments) exerted on the walls in question, we are based on the results of a study conducted at cold. We use on this subject the software Safir witch obeys to the Eurocode laws, to realize this study. It was found that loading, heating, and sizing play a capital role in the state of failed walls. Our results justify well the use of reinforced concrete walls, acting as a firewall. Their role is to limit the spread of fire from one structure to another structure nearby, since we get fire resistance reaching more than 10 hours depending on the loading considered.",2014-01-27T07:50:29Z,http://arxiv.org/pdf/1401.6759v1,2024-04-28,
1401.7631v1,"Slope Instability of the Earthen Levee in Boston, UK: Numerical Simulation and Sensor Data Analysis","The paper presents a slope stability analysis for a heterogeneous earthen levee in Boston, UK, which is prone to occasional slope failures under tidal loads. Dynamic behavior of the levee under tidal fluctuations was simulated using a finite element model of variably saturated linear elastic perfectly plastic soil. Hydraulic conductivities of the soil strata have been calibrated according to piezometers readings, in order to obtain correct range of hydraulic loads in tidal mode. Finite element simulation was complemented with series of limit equilibrium analyses. Stability analyses have shown that slope failure occurs with the development of a circular slip surface located in the soft clay layer. Both models (FEM and LEM) confirm that the least stable hydraulic condition is the combination of the minimum river levels at low tide with the maximal saturation of soil layers. FEM results indicate that in winter time the levee is almost at its limit state, at the margin of safety (strength reduction factor values are 1.03 and 1.04 for the low-tide and high-tide phases, respectively); these results agree with real-life observations. The stability analyses have been implemented as real-time components integrated into the UrbanFlood early warning system for flood protection.",2014-01-29T19:27:24Z,http://arxiv.org/pdf/1401.7631v1,2024-04-28,
1404.1990v2,"Estimating the Accuracy of the Return on Investment (ROI) Performance Evaluations","Return on Investment (ROI) is one of the most popular performance measurement and evaluation metrics. ROI analysis (when applied correctly) is a powerful tool in comparing solutions and making informed decisions on the acquisitions of information systems. The ROI sensitivity to error is a natural thought, and common sense suggests that ROI evaluations cannot be absolutely accurate. However, literature review revealed that in most publications and analyst firms reports, this issue is just overlooked. On the one hand, the results of the ROI calculations are implied to be produced with a mathematical rigor, possibility of errors is not mentioned and amount of errors is not estimated. On the contrary, another approach claims ROI evaluations to be absolutely inaccurate because, in view of their authors, future benefits (especially, intangible) cannot be estimated within any reasonable boundaries. The purpose of this study is to provide a systematic research of the accuracy of the ROI evaluations in the context of the information systems implementations. The main contribution of the study is that this is the first systematic effort to evaluate ROI accuracy. Analytical expressions have been derived for estimating errors of the ROI evaluations. Results of the Monte Carlo simulation will help practitioners in making informed decisions based on explicitly stated factors influencing the ROI uncertainties. The results of this research are intended for researchers in information systems, technology solutions and business management, and also for information specialists, project managers, program managers, technology directors, and information systems evaluators. Most results are applicable to ROI evaluations in a wider subject area.",2014-04-08T01:50:15Z,http://arxiv.org/pdf/1404.1990v2,2024-04-28,
1404.2872v1,"TreQ-CG: Clustering Accelerates High-Throughput Sequencing Read Mapping","As high-throughput sequencers become standard equipment outside of sequencing centers, there is an increasing need for efficient methods for pre-processing and primary analysis. While a vast literature proposes methods for HTS data analysis, we argue that significant improvements can still be gained by exploiting expensive pre-processing steps which can be amortized with savings from later stages. We propose a method to accelerate and improve read mapping based on an initial clustering of possibly billions of high-throughput sequencing reads, yielding clusters of high stringency and a high degree of overlap. This clustering improves on the state-of-the-art in running time for small datasets and, for the first time, makes clustering high-coverage human libraries feasible. Given the efficiently computed clusters, only one representative read from each cluster needs to be mapped using a traditional readmapper such as BWA, instead of individually mapping all reads. On human reads, all processing steps, including clustering and mapping, only require 11%-59% of the time for individually mapping all reads, achieving speed-ups for all readmappers, while minimally affecting mapping quality. This accelerates a highly sensitive readmapper such as Stampy to be competitive with a fast readmapper such as BWA on unclustered reads.",2014-04-10T16:29:09Z,http://arxiv.org/pdf/1404.2872v1,2024-04-28,
1404.3286v1,"A Continuous Optimization Approach for the Financial Portfolio Selection under Discrete Asset Choice Constraints","In this paper we consider a generalization of the Markowitz's Mean-Variance model under linear transaction costs and cardinality constraints. The cardinality constraints are used to limit the number of assets in the optimal portfolio. The generalized model is formulated as a mixed integer quadratic programming (MIP) problem. The purpose of this paper is to investigate a continuous approach based on difference of convex functions (DC) programming for solving the MIP model. The preliminary comparative results of the proposed approach versus CPLEX are presented.",2014-04-12T12:30:06Z,http://arxiv.org/pdf/1404.3286v1,2024-04-28,
1404.3329v1,"Portfolio Selection Under Buy-In Threshold Constraints Using DC Programming and DCA","In matter of Portfolio selection, we consider a generalization of the Markowitz Mean-Variance model which includes buy-in threshold constraints. These constraints limit the amount of capital to be invested in each asset and prevent very small investments in any asset. The new model can be converted into a NP-hard mixed integer quadratic programming problem. The purpose of this paper is to investigate a continuous approach based on DC programming and DCA for solving this new model. DCA is a local continuous approach to solve a wide variety of nonconvex programs for which it provided quite often a global solution and proved to be more robust and efficient than standard methods. Preliminary comparative results of DCA and a classical Branch-and-Bound algorithm will be presented. These results show that DCA is an efficient and promising approach for the considered portfolio selection problem.",2014-04-12T23:50:21Z,http://arxiv.org/pdf/1404.3329v1,2024-04-28,
1404.3330v1,"A DC programming approach for constrained two-dimensional non-guillotine cutting problem","We investigate a new application of Difference of Convex functions programming and DCA in solving the constrained two-dimensional non-guillotine cutting problem. This problem consists of cutting a number of rectangular pieces from a large rectangular object. The cuts are done under some constraints and the objective is to maximize the total value of the pieces cut. We reformulate this problem as a DC program and solve it by DCA. The performance of the approach is compared with the standard solver CPLEX.",2014-04-12T23:58:20Z,http://arxiv.org/pdf/1404.3330v1,2024-04-28,
1404.4282v4,"Modeling the wind circulation around mills with a Lagrangian stochastic approach","This work aims at introducing model methodology and numerical studies related to a Lagrangian stochastic approach applied to the computation of the wind circulation around mills. We adapt the Lagrangian stochastic downscaling method that we have introduced in [3] and [4] to the atmospheric boundary layer and we introduce here a Lagrangian version of the actuator disc methods to take account of the mills. We present our numerical method and numerical experiments in the case of non rotating and rotating actuator disc models. We also present some features of our numerical method, in particular the computation of the probability distribution of the wind in the wake zone, as a byproduct of the fluid particle model and the associated PDF method.",2014-04-16T15:23:49Z,http://arxiv.org/pdf/1404.4282v4,2024-04-28,
1404.5062v1,"Rapid prototyping for sling design optimization","This paper deals with combination of two modern engineering methods in order to optimise the shape of a representative casting product. The product being analysed is a sling, which is used to attach pulling rope in timber transportation. The first step was 3D modelling and static stress/strain analysis using CAD/CAE software NX4. The slinger shape optimization was performed using Traction method, by means of software Optishape-TS. To define constraints for shape optimization, FEA software FEMAP was used. The mould pattern with optimized 3D shape was then prepared using Fused Deposition Modelling (FDM) Rapid prototyping method. The sling mass decreased by 20%, while signifficantly better stress distribution was achieved, with maximum stress 3.5 times less than initial value. The future researches should use 3D scanning technology in order to provide more accurate 3D model of initial part. Results of this research can be used by toolmakers in order to engage FEA/RP technology to design and manufacture lighter products with acceptable stress distribution.",2014-04-20T18:31:52Z,http://arxiv.org/pdf/1404.5062v1,2024-04-28,
1404.5254v1,"Simultaneous Source for non-uniform data variance and missing data","The use of simultaneous sources in geophysical inverse problems has revolutionized the ability to deal with large scale data sets that are obtained from multiple source experiments. However, the technique breaks when the data has non-uniform standard deviation or when some data are missing. In this paper we develop, study, and compare a number of techniques that enable to utilize advantages of the simultaneous source framework for these cases. We show that the inverse problem can still be solved efficiently by using these new techniques. We demonstrate our new approaches on the Direct Current Resistivity inverse problem.",2014-04-21T17:55:34Z,http://arxiv.org/pdf/1404.5254v1,2024-04-28,
1404.6020v1,"A Fast Multiple Attractor Cellular Automata with Modified Clonal Classifier for Splicing Site Prediction in Human Genome","Bioinformatics encompass storing, analyzing and interpreting the biological data. Most of the challenges for Machine Learning methods like Cellular Automata is to furnish the functional information with the corresponding biological sequences. In eukaryotes DNA is divided into introns and exons. The introns will be removed to make the coding region by a process called splicing. By indentifying a splice site we can easily specify the DNA sequence category (Donor/Accepter/Neither).Splicing sites play an important role in understanding the genes. A class of CA which can handle fuzzy logic is employed with modified clonal algorithm is proposed to identify the splicing site. This classifier is tested with Irvine Primate Splice Junction Database. It is compared with NNspIICE, GENIO, HSPL and SPIICE VIEW. The reported accuracy and efficiency of prediction is quite promising.",2014-04-24T03:52:24Z,http://arxiv.org/pdf/1404.6020v1,2024-04-28,
1407.3373v1,"Car-following model on two lanes and stability analysis","Considering lateral influence from adjacent lane, an improved car-following model is developed in this paper. Then linear and non-linear stability analyses are carried out. The modified Korteweg-de Vries (MKdV) equation is derived with the kink-antikink soliton solution. Numerical simulations are implemented and the result shows good consistency with theoretical study.",2014-07-12T11:21:05Z,http://arxiv.org/pdf/1407.3373v1,2024-04-28,
1407.3661v1,"Modeling structural change in spatial system dynamics: A Daisyworld example","System dynamics (SD) is an effective approach for helping reveal the temporal behavior of complex systems. Although there have been recent developments in expanding SD to include systems' spatial dependencies, most applications have been restricted to the simulation of diffusion processes; this is especially true for models on structural change (e.g. LULC modeling). To address this shortcoming, a Python program is proposed to tightly couple SD software to a Geographic Information System (GIS). The approach provides the required capacities for handling bidirectional and synchronized interactions of operations between SD and GIS. In order to illustrate the concept and the techniques proposed for simulating structural changes, a fictitious environment called Daisyworld has been recreated in a spatial system dynamics (SSD) environment. The comparison of spatial and non-spatial simulations emphasizes the importance of considering spatio-temporal feedbacks. Finally, practical applications of structural change models in agriculture and disaster management are proposed.",2014-07-14T14:11:42Z,http://arxiv.org/pdf/1407.3661v1,2024-04-28,
1407.4650v1,"Protein Folding in the Hexagonal Prism Lattice with Diagonals","Predicting protein secondary structure using lattice model is one of the most studied computational problem in bioinformatics. Here secondary structure or three dimensional structure of protein is predicted from its amino acid sequence. Secondary structure refers to local sub-structures of protein. Mostly founded secondary structures are alpha helix and beta sheets. Since, it is a problem of great potential complexity many simplified energy model have been proposed in literature on basis of interaction of amino acid residue in protein. Here we use well researched Hydrophobic-Polar (HP) energy model. In this paper, we proposed hexagonal prism lattice with diagonal that can overcome the problems of other lattice structure, e.g., parity problem. We give two approximation algorithm for protein folding on this lattice. Our first algorithm leads us to similar structure of helix structure which is commonly found in protein structure. This motivated us to find next algorithm which improves the algorithm ratio of 9/7.",2014-07-17T12:12:35Z,http://arxiv.org/pdf/1407.4650v1,2024-04-28,
1407.8476v2,"A comparative study between seasonal wind speed by Fourier and Wavelet analysis","Wind Energy is a useful resource for Renewable energy purpose. Wind speed plays a vital role for wind energy calculation of certain location. So, it is very much necessary to know the wind speed data characteristics. In this paper fourier and wavelet transform are applied to study the wind speed data. We have compared wind speed of winter with summer by taking their speed into account using various discrete wavelets namely Haar and Daubechies-4 (Db-4). Also the periodicity of wind speed is checked using Continuous Wavelet Transform (MCWT) like Morlet. Thereafter a comparative study is done for detecting the periodicity of both summer and winter. Then wavelet coherence is checked between these two data for extracting the phase coherency information.",2014-07-31T16:17:39Z,http://arxiv.org/pdf/1407.8476v2,2024-04-28,
1408.5592v1,"HyDA-Vista: Towards Optimal Guided Selection of k-mer Size for Sequence Assembly","Motivation: Intimately tied to assembly quality is the complexity of the de Bruijn graph built by the assembler. Thus, there have been many paradigms developed to decrease the complexity of the de Bruijn graph. One obvious combinatorial paradigm for this is to allow the value of $k$ to vary; having a larger value of $k$ where the graph is more complex and a smaller value of $k$ where the graph would likely contain fewer spurious edges and vertices. One open problem that affects the practicality of this method is how to predict the value of $k$ prior to building the de Bruijn graph. We show that optimal values of $k$ can be predicted prior to assembly by using the information contained in a phylogenetically-close genome and therefore, help make the use of multiple values of $k$ practical for genome assembly.   Results: We present HyDA-Vista, which is a genome assembler that uses homology information to choose a value of $k$ for each read prior to the de Bruijn graph construction. The chosen $k$ is optimal if there are no sequencing errors and the coverage is sufficient. Fundamental to our method is the construction of the {\em maximal sequence landscape}, which is a data structure that stores for each position in the input string, the largest repeated substring containing that position. In particular, we show the maximal sequence landscape can be constructed in $O(n + n \log n)$-time and $O(n)$-space. HyDA-Vista first constructs the maximal sequence landscape for a homologous genome. The reads are then aligned to this reference genome, and values of $k$ are assigned to each read using the maximal sequence landscape and the alignments. Eventually, all the reads are assembled by an iterative de Bruijn graph construction method. Our results and comparison to other assemblers demonstrate that HyDA-Vista achieves the best assembly of {\em E. coli} before repeat resolution or scaffolding.",2014-08-24T11:26:37Z,http://arxiv.org/pdf/1408.5592v1,2024-04-28,
1410.1233v10,"EnKF-C user guide","EnKF-C provides a compact generic framework for off-line data assimilation into large-scale layered geophysical models with the ensemble Kalman filter (EnKF). It is coded in C for GNU/Linux platform and can work either in EnKF, ensemble optimal interpolation (EnOI), or hybrid (EnKF/EnOI) modes.",2014-10-06T01:29:42Z,http://arxiv.org/pdf/1410.1233v10,2024-04-28,
1410.4399v1,"Constrained Runs algorithm as a lifting operator for the Boltzmann equation","Lifting operators play an important role in starting a kinetic Boltzmann model from given macroscopic information. The macroscopic variables need to be mapped to the distribution functions, mesoscopic variables of the Boltzmann model. A well-known numerical method for the initialization of Boltzmann models is the Constrained Runs algorithm. This algorithm is used in literature for the initialization of lattice Boltzmann models, special discretizations of the Boltzmann equation. It is based on the attraction of the dynamics toward the slow manifold and uses lattice Boltzmann steps to converge to the desired dynamics on the slow manifold. We focus on applying the Constrained Runs algorithm to map density, average flow velocity, and temperature, the macroscopic variables, to distribution functions. Furthermore, we do not consider only lattice Boltzmann models. We want to perform the algorithm for different discretizations of the Boltzmann equation and consider a standard finite volume discretization.",2014-10-16T12:46:27Z,http://arxiv.org/pdf/1410.4399v1,2024-04-28,
1410.4428v1,"Initialization of lattice Boltzmann models with the help of the numerical Chapman-Enskog expansion","We extend the applicability of the numerical Chapman-Enskog expansion as a lifting operator for lattice Boltzmann models to map density and momentum to distribution functions. In earlier work [Vanderhoydonc et al. Multiscale Model. Simul. 10(3): 766-791, 2012] such an expansion was constructed in the context of lifting only the zeroth order velocity moment, namely the density. A lifting operator is necessary to convert information from the macroscopic to the mesoscopic scale. This operator is used for the initialization of lattice Boltzmann models. Given only density and momentum, the goal is to initialize the distribution functions of lattice Boltzmann models. For this initialization, the numerical Chapman-Enskog expansion is used in this paper.",2014-10-16T13:58:04Z,http://arxiv.org/pdf/1410.4428v1,2024-04-28,
1410.8616v1,"Data Driven Prognosis: A multi-physics approach verified via balloon burst experiment","A multi-physics formulation for Data Driven Prognosis (DDP) is developed. Unlike traditional predictive strategies that require controlled off-line measurements or training for determination of constitutive parameters to derive the transitional statistics, the proposed DDP algorithm relies solely on in situ measurements. It utilizes a deterministic mechanics framework, but the stochastic nature of the solution arises naturally from the underlying assumptions regarding the order of the conservation potential as well as the number of dimensions involved. The proposed DDP scheme is capable of predicting onset of instabilities. Since the need for off-line testing (or training) is obviated, it can be easily implemented for systems where such a priori testing is difficult or even impossible to conduct. The prognosis capability is demonstrated here via a balloon burst experiment where the instability is predicted utilizing only on-line visual observations. The DDP scheme never failed to predict the incipient failure, and no false positives were issued. The DDP algorithm is applicable to others types of datasets. Time horizons of DDP predictions can be adjusted by using memory over different time windows. Thus, a big dataset can be parsed in time to make a range of predictions over varying time horizons.",2014-10-31T02:05:09Z,http://arxiv.org/pdf/1410.8616v1,2024-04-28,
1410.8674v1,"Finite element model based on refined plate theories for laminated glass units","Laminated glass units exhibit complex response as a result of different mechanical behavior and properties of glass and polymer foil. We aim to develop a finite element model for elastic laminated glass plates based on the refined plate theory by Mau. For a geometrically nonlinear description of the behavior of units, each layer behaves according to the Reissner-Mindlin kinematics, complemented with membrane effects and the von K\'{a}rm\'{a}n assumptions. Nodal Lagrange multipliers enforce the compatibility of independent layers in this approach. We have derived the discretized model by the energy-minimization arguments, assuming that the unknown fields are approximated by bi-linear functions at the element level, and solved the resulting system by the Newton method with consistent linearization. We have demonstrated through verification and validation examples that the proposed formulation is reliable and accurately reproduces the behavior of laminated glass units. This study represents a first step to the development of a comprehensive, mechanics-based model for laminated glass systems that is suitable for implementation in common engineering finite element solvers.",2014-10-31T09:00:37Z,http://arxiv.org/pdf/1410.8674v1,2024-04-28,
1411.0001v1,"Prognosis of Anterior Cruciate Ligament (ACL) Reconstruction: A Data Driven Approach","Individuals who suffer anterior cruciate ligament (ACL) injury are at higher risk of developing knee osteoarthritis (OA) and almost 50% display symptoms 10 to 20 years post injury. Anterior cruciate ligament reconstruction (ACLR) often does not protect against knee OA development. Accordingly, a multiscale formulation for Data Driven Prognosis (DDP) of post ACLR is developed. Unlike traditional predictive strategies that require controlled off-line measurements or training for determination of constitutive parameters to derive the transitional statistics, the proposed DDP algorithm relies solely on in situ measurements. The proposed DDP scheme is capable of predicting onset of instabilities. Since the need for off line testing (or training) is obviated, it can be easily implemented for ACLR, where such controlled a priori testing is almost impossible to conduct. The DDP algorithm facilitates hierarchical handling of the large data set, and can assess the state of recovery in post ACLR conditions based on data collected from stair ascent and descent exercises of subjects. The DDP algorithm identifies inefficient knee varus motion and knee rotation as primary difficulties experienced by some of the post ACLR population. In such cases, levels of energy dissipation rate at the knee, and its fluctuation may be used as measures for assessing progress after ACL reconstruction.",2014-10-30T22:54:07Z,http://arxiv.org/pdf/1411.0001v1,2024-04-28,
1411.2684v3,"Dual Algorithms","The cubic spline interpolation method, the Runge--Kutta method, and the Newton-Raphson method are extended to dual versions (developed in the context of dual numbers). This extension allows the calculation of the derivatives of complicated compositions of functions which are not necessarily defined by a closed form expression. The code for the algorithms has been written in Fortran and some examples are presented. Among them, we use the dual Newton--Raphson method to obtain the derivatives of the output angle in the RRRCR spatial mechanism; we use the dual normal cubic spline interpolation algorithm to obtain the thermal diffusivity using photothermal techniques; and we use the dual Runge--Kutta method to obtain the derivatives of functions depending on the solution of the Duffing equation.",2014-11-11T02:35:50Z,http://arxiv.org/pdf/1411.2684v3,2024-04-28,
1411.3508v1,"Geometrically nonlinear isogeometric analysis of laminated composite plates based on higher-order shear deformation theory","In this paper, we present an effectively numerical approach based on isogeometric analysis (IGA) and higher-order shear deformation theory (HSDT) for geometrically nonlinear analysis of laminated composite plates. The HSDT allows us to approximate displacement field that ensures by itself the realistic shear strain energy part without shear correction factors. IGA utilizing basis functions namely B-splines or non-uniform rational B-splines (NURBS) enables to satisfy easily the stringent continuity requirement of the HSDT model without any additional variables. The nonlinearity of the plates is formed in the total Lagrange approach based on the von-Karman strain assumptions. Numerous numerical validations for the isotropic, orthotropic, cross-ply and angle-ply laminated plates are provided to demonstrate the effectiveness of the proposed method.",2014-11-13T11:39:39Z,http://arxiv.org/pdf/1411.3508v1,2024-04-28,
1411.3923v1,"Robust topology optimisation of microstructural details without length scale separation - using a spectral coarse basis preconditioner","This paper applies topology optimisation to the design of structures with periodic microstructural details without length scale separation, i.e. considering the complete macroscopic structure and its response, while resolving all microstructural details, as compared to the often used homogenisation approach. The approach takes boundary conditions into account and ensures connected and macroscopically optimised microstructures regardless of the difference in micro- and macroscopic length scales. This results in microstructures tailored for specific applications rather than specific properties.   Dealing with the complete macroscopic structure and its response is computationally challenging as very fine discretisations are needed in order to resolve all microstructural details. Therefore, this article shows the benefits of applying a contrast-independent spectral preconditioner based on the multiscale finite element method (MsFEM) to large structures with fully-resolved microstructural details.   The density-based topology optimisation approach combined with a Heaviside projection filter and a stochastic robust formulation is used on various problems, with both periodic and layered microstructures. The presented approach is shown to allow for the topology optimisation of very large problems in \textsc{Matlab}, specifically a problem with 26 million displacement degrees of freedom in 26 hours using a single computational thread.",2014-11-13T15:49:40Z,http://arxiv.org/pdf/1411.3923v1,2024-04-28,
1411.4110v2,"Dynamic aerodynamic-structural coupling numerical simulation on the flexible wing of a cicada based on ansys","Most biological flyers undergo orderly deformation in flight, and the deformations of wings lead to complex fluid-structure interactions. In this paper, an aerodynamic-structural coupling method of flapping wing is developed based on ANSYS to simulate the flapping of flexible wing. Fluent module and Transient Structural module are connected through the System Coupling module to make a two-way fluid-structure Coupling computational framework. Comparing with the rigid wing of a cicada, the coupling results of the flexible wing shows that the flexible deformation can increase the aerodynamic performances of flapping flight.",2014-11-15T03:47:16Z,http://arxiv.org/pdf/1411.4110v2,2024-04-28,
1411.6884v1,"Proportional Topology Optimization: A new non-gradient method for solving stress constrained and minimum compliance problems and its implementation in MATLAB","A new topology optimization method called the Proportional Topology Optimization (PTO) is presented. As a non-gradient method, PTO is simple to understand, easy to implement, and is also efficient and accurate at the same time. It is implemented into two MATLAB programs to solve the stress constrained and minimum compliance problems. Descriptions of the algorithm and computer programs are provided in detail. The method is applied to solve three numerical examples for both types of problems. The method shows comparable efficiency and accuracy with an existing gradient optimality criteria method. Also, the PTO stress constrained algorithm and minimum compliance algorithm are compared by feeding output from one algorithm to the other in an alternative manner, where the former yields lower maximum stress and volume fraction but higher compliance compared to the latter. Advantages and disadvantages of the proposed method and future works are discussed. The computer programs are self-contained and publicly shared in the website www.ptomethod.org.",2014-11-21T15:04:31Z,http://arxiv.org/pdf/1411.6884v1,2024-04-28,
1411.7462v1,"Optimal Boundary Control for Water Hammer Suppression in Fluid Transmission Pipelines","When fluid flow in a pipeline is suddenly halted, a pressure surge or wave is created within the pipeline. This phenomenon, called water hammer, can cause major damage to pipelines, including pipeline ruptures. In this paper, we model the problem of mitigating water hammer during valve closure by an optimal boundary control problem involving a nonlinear hyperbolic PDE system that describes the fluid flow along the pipeline. The control variable in this system represents the valve boundary actuation implemented at the pipeline terminus. To solve the boundary control problem, we first use {the method of lines} to obtain a finite-dimensional ODE model based on the original PDE system. Then, for the boundary control design, we apply the control parameterization method to obtain an approximate optimal parameter selection problem that can be solved using nonlinear optimization techniques such as Sequential Quadratic Programming (SQP). We conclude the paper with simulation results demonstrating the capability of optimal boundary control to significantly reduce flow fluctuation.",2014-11-27T03:45:43Z,http://arxiv.org/pdf/1411.7462v1,2024-04-28,
1412.4192v3,"Finite Element Method Based Modeling of Cardiac Deformation Estimation under Abnormal Ventricular Muscle Conditions","Deformation modeling of cardiac muscle is an important issue in the field of cardiac analysis. Many approaches have been developed to better estimate the cardiac muscle deformation, and to obtain a practical model to be used in diagnostic procedures. But there are some conditions, like in case of myocardial infarction, in which the regular modeling approaches are not useful. In this article, using a point-wise approach, we try to estimate the deformation under some abnormal conditions of cardiac muscle. First, the endocardial and epicardial contour points are ordered with respect to the center of gravity of the endocardial contour and displacement vectors of boundary points are extracted. Then to solve the governing equations of the deformation, which is an elliptic equation, we apply boundary conditions in accordance with the computed displacement vectors and then the Finite Element method (FEM) will be used to solve the governing equations. Using the obtained displacement field of the cardiac muscle, strain map is extracted to show the mechanical behavior of the cardiac muscle. Several tests are conducted using phantom and real cardiac data in order to show the validity of the proposed method.",2014-12-13T04:49:31Z,http://arxiv.org/pdf/1412.4192v3,2024-04-28,
1412.5496v1,"Conversion of G-code programs for milling into STEP-NC","STEP-NC (ISO 14649) is becoming a promising standard to replace or supplement the conventional G-code programs based on ISO 6983 due to its feature based machine independent characteristics and its centric role to enable efficient CAD/CAM/CNC interoperability. The re-use of G-code programs is important for both manufacturing and capitalization of machining knowledge, nevertheless the conversion is a tedious task when carried out manually and machining knowledge is almost hidden in the low level G-code. Mapping G-code into STEP-NC should benefit from more expressiveness of the manufacturing feature-based characteristics of this new standard. The work presented here proposes an overall method for G-code to STEP-NC conversion. First, G-code is converted into canonical machining functions, this can make the method more applicable and make subsequent processes easier to implement; then these functions are parsed to generate the neutral format of STEP-NC Part21 toolpath file, this turns G-code into object instances, and can facilitate company's usage of legacy programs; and finally, also optionally, machining features are extracted to generate Part21 CC2 (conformance class) file. The proposed extraction method employs geometric information of cutting area inferred from toolpaths and machining strategies, in addition to cutting tools' data and workpiece's dimension data. This comprehensive use of available data makes the extraction more accurate and reliable. The conversion method is holistic, and can be extended to process a wide range of G-code programs (e.g. turning or mill-turn codes) with as few user interventions as possible.",2014-12-16T20:34:48Z,http://arxiv.org/pdf/1412.5496v1,2024-04-28,
1412.6306v1,"Multiprocessor System Dedicated to Multi-Rotor Mini-UAV Capable of 3D flying","The paper describes an electronic multiprocessor system that assures functionality of a miniature UAV capable of 3D flying. The apparatus consists of six independently controlled brushless DC motors, each having a propeller attached to it. Since the brushless motor requires complex algorithms in order to achieve maximum torque, efficiency and response time a DSP must be used. All the motors are then controlled by a main microprocessor which is capable of reading sensors (Inertial Measurement Unit (IMU)-orientation and GPS), receiving input commands (remote controller or trajectory plan) and sending independent commands to each of the six motors. The apparatus contains a total of eight microcontrollers: the main unit, the IMU mathematical processor and one microcontroller for each of the six brushless DC motors. Applications for such an apparatus could include not only military, but also search-and-rescue, geodetics, aerial photography and aerial assistance.",2014-12-19T11:53:38Z,http://arxiv.org/pdf/1412.6306v1,2024-04-28,
1412.6412v1,"Numerical simulation of liver perfusion: from CT scans to FE model","We use a collection of Python programs for numerical simulation of liver perfusion. We have an application for semi-automatic generation of a finite element mesh of the human liver from computed tomography scans and for reconstruction of the liver vascular structure. When the real vascular trees can not be obtained from the CT data we generate artificial trees using the constructive optimization method. The generated FE mesh and vascular trees are imported into SfePy (Simple Finite Elements in Python) and numerical simulations are performed in order to get the pressure distribution and perfusion flows in the liver tissue. In the post-processing steps we calculate transport of a contrast fluid through the liver parenchyma.",2014-12-19T16:12:02Z,http://arxiv.org/pdf/1412.6412v1,2024-04-28,
1412.6850v2,"Synthesis Method for the Spherical 4R Mechanism with Minimum Center of Mass Acceleration","In the mechanisms area, minimization of the magnitude of the acceleration of the center of mass (ACoM) implies shaking force balancing. For a mechanism operating in cycles, the case when the ACoM is zero implies that the gravitational potential energy (GPE) is constant. This article shows an efficient and effective optimum synthesis method for minimum acceleration of the center of mass of a spherical 4R mechanism by using dual functions and the counterweights balancing method. Once the dual function for ACoM has been written, one can minimize the shaking forces from a kinematic point of view. We present the synthesis of a spherical 4R mechanism for the case of a path generation task. The synthesis process involves the optimization of two objective functions, this multiobjective problem is solved by using the weighted sum method implemented in the evolutionary algorithm known as Differential Evolution.",2014-12-22T00:55:39Z,http://arxiv.org/pdf/1412.6850v2,2024-04-28,
1412.7811v1,"A Structured Hardware Software Architecture for Peptide Based Diagnosis of Baylisascaris Procyonis Infection (ICIAfS14)","The problem of inferring proteins from complex peptide cocktails (digestion products of biological samples) in shotgun proteomic workflow sets extreme demands on computational resources in respect of the required very high processing throughputs, rapid processing rates and reliability of results. This is exacerbated by the fact that, in general, a given protein cannot be defined by a fixed sequence of amino acids due to the existence of splice variants and isoforms of that protein. Therefore, the problem of protein inference could be considered as one of identifying sequences of amino acids with some limited tolerance. In the current paper a model-based hardware acceleration of a structured and practical inference approach is developed and validated on a mass spectrometry experiment of realistic size. We have achieved 10 times maximum speed-up in the co-designed workflow compared to a similar software-only workflow run on the processor used for co-design.",2014-12-25T11:33:59Z,http://arxiv.org/pdf/1412.7811v1,2024-04-28,
1412.7929v2,"Designing pricing schemes based on progressive tariff and consumer grouping in migration to a future smart grid","We study the design of pricing schemes for a group of consumers with smart meters (e.g., in a Greenfield area) who are connected through a gateway to a traditional electricity greed with a progressive tariff. Because the progressive tariff cannot take into account the time aspect of electricity demands, we apply it to consumers in both an individual and a group basis over a shorter time period, which can flatten the overall demand over time and thereby reduce peak load. This scenario for the coexistence of traditional and smart girds and the pricing schemes under this scenario can enable smooth migration to a future smart grid.",2014-12-26T11:50:51Z,http://arxiv.org/pdf/1412.7929v2,2024-04-28,
1503.03605v3,"An improved return-mapping scheme for nonsmooth yield surfaces: PART I - the Haigh-Westergaard coordinates","The paper is devoted to the numerical solution of elastoplastic constitutive initial value problems. An improved form of the implicit return-mapping scheme for nonsmooth yield surfaces is proposed that systematically builds on a subdifferential formulation of the flow rule. The main advantage of this approach is that the treatment of singular points, such as apices or edges at which the flow direction is multivalued involves only a uniquely defined set of non-linear equations, similarly to smooth yield surfaces. This paper (PART I) is focused on isotropic models containing: $a)$ yield surfaces with one or two apices (singular points) laying on the hydrostatic axis; $b)$ plastic pseudo-potentials that are independent of the Lode angle; $c)$ nonlinear isotropic hardening (optionally). It is shown that for some models the improved integration scheme also enables to a priori decide about a type of the return and investigate existence, uniqueness and semismoothness of discretized constitutive operators in implicit form. Further, the semismooth Newton method is introduced to solve incremental boundary-value problems. The paper also contains numerical examples related to slope stability with available Matlab implementation.",2015-03-12T06:31:41Z,http://arxiv.org/pdf/1503.03605v3,2024-04-28,
1503.05146v1,"Accurate Impedance Calculation for Underground and Submarine Power Cables using MoM-SO and a Multilayer Ground Model","An accurate knowledge of the per-unit length impedance of power cables is necessary to correctly predict electromagnetic transients in power systems. In particular, skin, proximity, and ground return effects must be properly estimated. In many applications, the medium that surrounds the cable is not uniform and can consist of multiple layers of different conductivity, such as dry and wet soil, water, or air. We introduce a multilayer ground model for the recently-proposed MoM-SO method, suitable to accurately predict ground return effects in such scenarios. The proposed technique precisely accounts for skin, proximity, ground and tunnel effects, and is applicable to a variety of cable configurations, including underground and submarine cables. Numerical results show that the proposed method is more accurate than analytic formulas typically employed for transient analyses, and delivers an accuracy comparable to the finite element method (FEM). With respect to FEM, however, MoM-SO is over 1000 times faster, and can calculate the impedance of a submarine cable inside a three-layer medium in 0.10~s per frequency point.",2015-03-17T18:14:04Z,http://arxiv.org/pdf/1503.05146v1,2024-04-28,
1503.05275v1,"Abrupt Change Detection in Power System Fault Analysis using Adaptive Whitening Filter and Wavelet Transform","This paper describes the application of the adaptive whitening filter and the wavelet transform used to detect the abrupt changes in the signals recorded during disturbances in the electrical power network in South Africa. Main focus has been to estimate exactly the time-instants of the changes in the signal model parameters during the pre-fault condition and following events like initiation of fault, circuit-breaker opening, auto-reclosure of the circuit-breakers. The key idea is to decompose the fault signals, de-noised using the adaptive whitening filter, into effective detailed and smoothed version using the multiresolution signal decomposition technique based on discrete wavelet transform. Then we apply the threshold method on the decomposed signals to estimate the change time-instants, segmenting the fault signals into the event-specific sections for further signal processing and analysis. This paper presents application on the recorded signals in the power transmission network of South Africa.",2015-03-18T03:10:41Z,http://arxiv.org/pdf/1503.05275v1,2024-04-28,
1503.07122v1,"On damping created by heterogeneous yielding in the numerical analysis of nonlinear reinforced concrete frame elements","In the dynamic analysis of structural engineering systems, it is common practice to introduce damping models to reproduce experimentally observed features. These models, for instance Rayleigh damping, account for the damping sources in the system altogether and often lack physical basis. We report on an alternative path for reproducing damping coming from material nonlinear response through the consideration of the heterogeneous character of material mechanical properties. The parameterization of that heterogeneity is performed through a stochastic model. It is shown that such a variability creates the patterns in the concrete cyclic response that are classically regarded as source of damping.",2015-03-21T19:34:49Z,http://arxiv.org/pdf/1503.07122v1,2024-04-28,
1503.07809v1,"Numerical solution of moving plate problem with uncertain parameters","This paper deals with uncertain parabolic fluid flow problem where the uncertainty occurs due to the initial conditions and parameters involved in the system. Uncertain values are considered as fuzzy and these are handled through a recently developed method. Here the concepts of fuzzy numbers are combined with Finite Difference Method (FDM) and then Fuzzy Finite Difference Method (FFDM) has been proposed. The proposed FFDM has been used to solve the fluid flow problem bounded by two parallel plates. Finally sensitivity of the fuzzy parameters has also been analysed.",2015-03-14T12:31:02Z,http://arxiv.org/pdf/1503.07809v1,2024-04-28,
1504.01310v3,"Reproducibility as a Technical Specification","Reproducibility of computationally-derived scientific discoveries should be a certainty. As the product of several person-years' worth of effort, results -- whether disseminated through academic journals, conferences or exploited through commercial ventures -- should at some level be expected to be repeatable by other researchers. While this stance may appear to be obvious and trivial, a variety of factors often stand in the way of making it commonplace. Whilst there has been detailed cross-disciplinary discussions of the various social, cultural and ideological drivers and (potential) solutions, one factor which has had less focus is the concept of reproducibility as a technical challenge. Specifically, that the definition of an unambiguous and measurable standard of reproducibility would offer a significant benefit to the wider computational science community.   In this paper, we propose a high-level technical specification for a service for reproducibility, presenting cyberinfrastructure and associated workflow for a service which would enable such a specification to be verified and validated. In addition to addressing a pressing need for the scientific community, we further speculate on the potential contribution to the wider software development community of services which automate de novo compilation and testing of code from source. We illustrate our proposed specification and workflow by using the BioModelAnalyzer tool as a running example.",2015-04-06T15:59:49Z,http://arxiv.org/pdf/1504.01310v3,2024-04-28,
1504.06664v1,"Fast and Rigorous DC Solution in Finite Element Method for Integrated Circuit Analysis","Large scale circuit simulation, such as power delivery network analysis, has become increasingly challenge in the VLSI design verification flow. Power delivery network can be simulated by both SPICE-type circuit-based model and eletromagnetics-based model when full-wave accuracy is desired. In the early time of the time domain finite element simulation for integrated circuit, the modes having the highest eigenvalues supported by the numerical system will be excited. Because of the band limited source, after the early time, the modes having a resonance frequency well beyond the input frequency band will die down, and all physically important high-order modes and DC mode will show up and become dominant. Among these modes, the DC mode is the last one to show up. Although the convergence criterion is not applied on the DC mode, the existence of DC mode in the field solution will deteriorate the convergence rate of the first several high order modes. Therefore, this paper first analyzed the mathematic characteristics of the DC mode and proposed a rigorous and fast solution to extract the DC mode from the numerical system in order to speed up the convergence rate. Experimental results demonstrated the robustness and superior performance of this method.",2015-04-24T23:23:33Z,http://arxiv.org/pdf/1504.06664v1,2024-04-28,
1506.00571v5,"Calculation of the confidence bounds for the fraction nonconforming of normal populations of measurements in clinical laboratory medicine","The fraction nonconforming is a key quality measure used in statistical quality control design in clinical laboratory medicine. The confidence bounds of normal populations of measurements for the fraction nonconforming each of the lower and upper quality specification limits when both the random and the systematic error are unknown can be calculated using the noncentral t-distribution, as it is described in detail and illustrated with examples.",2015-06-01T17:04:23Z,http://arxiv.org/pdf/1506.00571v5,2024-04-28,
1506.00716v1,"Tackling Exascale Software Challenges in Molecular Dynamics Simulations with GROMACS","GROMACS is a widely used package for biomolecular simulation, and over the last two decades it has evolved from small-scale efficiency to advanced heterogeneous acceleration and multi-level parallelism targeting some of the largest supercomputers in the world. Here, we describe some of the ways we have been able to realize this through the use of parallelization on all levels, combined with a constant focus on absolute performance. Release 4.6 of GROMACS uses SIMD acceleration on a wide range of architectures, GPU offloading acceleration, and both OpenMP and MPI parallelism within and between nodes, respectively. The recent work on acceleration made it necessary to revisit the fundamental algorithms of molecular simulation, including the concept of neighborsearching, and we discuss the present and future challenges we see for exascale simulation - in particular a very fine-grained task parallelism. We also discuss the software management, code peer review and continuous integration testing required for a project of this complexity.",2015-06-02T00:37:50Z,http://arxiv.org/pdf/1506.00716v1,2024-04-28,
1506.01190v1,"Modeling of through-reactors with allowance of Large-Scale Effect on Heat and Mass Efficiency of Chemical Apparatuses","This paper deals also with a problem of gas absorption accompanied by an instantaneous, irreversible reaction in the liquid layer. The well-known methods for calculating such processes are based usually on the certain amendments to solutions, which are obtained disregarding the chemical reaction. Unlike the known work (1. D. Baetens, R. Van Keer, L.H. Hosten. Gas-liquid reaction: absorption accompanied by an instantaneous, irreversible reaction// Moving Boundaries IV, Southampton, Boston, 1997) the approach we used takes into account the influence of reaction resulting product on the arising and velocity of a moving reaction plane. The known results in the theory of chemical apparatuses scaling are devoted to apparatuses with non-regular packings mainly. However how the phases distribution over the regular packings of chemical columns effects the heat and mass efficiency is studied lesser. This paper deals with the methods of simulation the scaling effects applying to chemical towers with regular packings of various types. The models for describing the influence of initial liquid and gas distribution in chemical columns with regular packing on the mass transfer efficiency have been submitted. The sufficiently simple methods for evaluating the influence of large-scale factor on the efficiency of mass transfer have been obtained. These methods are suitable for use in engineering calculation techniques.",2015-06-03T10:12:55Z,http://arxiv.org/pdf/1506.01190v1,2024-04-28,
1506.03611v1,"A correction to the enhanced bottom drag parameterisation of tidal turbines","Hydrodynamic modelling is an important tool for the development of tidal stream energy projects. Many hydrodynamic models incorporate the effect of tidal turbines through an enhanced bottom drag. In this paper we show that although for coarse grid resolutions (kilometre scale) the resulting force exerted on the flow agrees well with the theoretical value, the force starts decreasing with decreasing grid sizes when these become smaller than the length scale of the wake recovery. This is because the assumption that the upstream velocity can be approximated by the local model velocity, is no longer valid. Using linear momentum actuator disc theory however, we derive a relationship between these two velocities and formulate a correction to the enhanced bottom drag formulation that consistently applies a force that remains closed to the theoretical value, for all grid sizes down to the turbine scale. In addition, a better understanding of the relation between the model, upstream, and actual turbine velocity, as predicted by actuator disc theory, leads to an improved estimate of the usefully extractable energy. We show how the corrections can be applied (demonstrated here for the models MIKE 21 and Fluidity) by a simple modification of the drag coefficient.",2015-06-11T10:18:17Z,http://arxiv.org/pdf/1506.03611v1,2024-04-28,
1506.04447v1,"An Optimal Framework for Residential Load Aggregator","Due to the development of intelligent demand-side management with automatic control, distributed populations of large residential loads, such as air conditioners (ACs) and electrical water heaters (EWHs), have the opportunities to provide effective demand-side ancillary services for load serving entities (LSEs) to reduce the emissions and network operating costs. Most present approaches are restricted to 1) the scenarios involving with efficiently scheduling the large number of appliances in real time, 2) the issues about evaluating the contributions of individual residents towards participating demand response (DR) program, and fairly distributing the rewards, and 3) the concerns on performing cost-effective demand reduction request (DRR) for LSEs with minimal rewards costs while not affecting their living comfortableness. Therefore, this paper presents an optimal framework for residential load aggregators (RLAs) which helps solve the problems mentioned above. Under this framework, RLAs are able to realize the DRR for LSEs to generate optimal control strategies over residential appliances quickly and efficiently. To residents, the framework is designed with probabilistic model of comfortableness, which minimizes the impact of DR program to their daily life. To LSEs, the framework helps minimize the total reward costs of performing DRRs. Moreover, the framework fairly and strategically distributes the financial rewards to residents, which may stimulate the potential capability of loads optimized and controlled by RLAs in demand side management. The proposed framework has been validated on several numerical case studies.",2015-06-14T22:30:14Z,http://arxiv.org/pdf/1506.04447v1,2024-04-28,
1506.07214v1,"Convex Relaxations for Gas Expansion Planning","Expansion of natural gas networks is a critical process involving substantial capital expenditures with complex decision-support requirements. Given the non-convex nature of gas transmission constraints, global optimality and infeasibility guarantees can only be offered by global optimisation approaches. Unfortunately, state-of-the-art global optimisation solvers are unable to scale up to real-world size instances. In this study, we present a convex mixed-integer second-order cone relaxation for the gas expansion planning problem under steady-state conditions. The underlying model offers tight lower bounds with high computational efficiency. In addition, the optimal solution of the relaxation can often be used to derive high-quality solutions to the original problem, leading to provably tight optimality gaps and, in some cases, global optimal soluutions. The convex relaxation is based on a few key ideas, including the introduction of flux direction variables, exact McCormick relaxations, on/off constraints, and integer cuts. Numerical experiments are conducted on the traditional Belgian gas network, as well as other real larger networks. The results demonstrate both the accuracy and computational speed of the relaxation and its ability to produce high-quality solutions.",2015-06-24T00:04:44Z,http://arxiv.org/pdf/1506.07214v1,2024-04-28,
1506.07411v1,"Selecting the Best Traffic Scheme for the Bicutan Roundabout: A Microsimulation Approach with Multiple Driver-Agents","We present the result of our microsimulation study on the effects of six traffic schemes $T=\{t_0, t_1, \dots, t_5\}$ on the mean total delay time ($\Delta$) and mean speed ($\Sigma$) of vehicles at the non-signalized Bicutan Roundabout (BR) in Upper Bicutan, Taguig City, Metro Manila, with $t_0$ as the current traffic scheme being enforced and $t_{i>0}$ as the proposed ones. We present first that our simulation approach, a hybridized multi-agent system (MAS) with the car-following and lane-changing models (CLM), can mimic the current observed traffic scenario $C$ at a statistical significance of $\alpha=0.05$. That is, the respective absolute differences of the $\Delta$ and $\Sigma$ between $C$ and $t_0$ are not statistically different from zero. Next, using our MAS-CLM, we simulated all proposed $t_{i>0}$ and compared their respective $\Delta$ and $\Sigma$. We found out using DMRT that the best traffic scheme is $t_3$ (i.e., when we converted the bi-directional 4-lane PNR-PNCC road into a bi-directional 1-lane PNR-to-PNCC and 3-lane PNCC-to-PNR routes during rush hours). Then, we experimented on converting BR into a signalized junction and re-implemented all $t_3$ with controlled stops of $S=\{15s, 45s\}$. We found out that $t_3$ with a 15-s stop has the best performance. Finally, we simulated the effect of increased in vehicular volume $V$ due to traffic improvement and we found out that $t_3$ with 15-s stop still outperforms the others for all increased in $V=\{10\%, 50\%, 100\%\}$.",2015-06-24T15:07:12Z,http://arxiv.org/pdf/1506.07411v1,2024-04-28,
1506.07424v1,"Simulating the Effects of Various Road Infrastructure Improvements to Vehicular Traffic in a Busy Three-road Fork","Using microsimulations of vehicular dynamics, we studied the effects of several proposed infrastructure developments to the mean travel delay time~$\Delta$ and mean speed~$\Sigma$ of vehicles passing a busy three-road fork, particularly in the non-signalized roundabout junction of Lower Bicutan, Taguig City, Metro Manila. We designed and implemented multi-agent-based microsimulation models to mimic the autonomous driving behavior of heterogeneous individuals and measured the effect of various proposed infrastructure developments on~$\Delta$ and~$\Sigma$. Our aim is to find out the best infrastructure development from among three choices being considered by the local government for the purpose of solving the traffic problems in the area. We created simulation models of the current vehicular traffic situation in the area using the mean travel times~$\tau$ of statistically sampled vehicles to show that our model can simulate the real-world at a significance level of $\alpha=0.05$. Based on these models, we then simulated the effect of the proposed infrastructure developments on~$\Delta$ and~$\Sigma$ and used these metrics as our basis of comparison. We found out that the proposed widening of one fork from two lanes to three lanes has the most improved metrics at the same $\alpha=0.05$ compared to the metrics we observed in the current situation. Under this infrastructure development, the~$\Delta$ increases linearly ($R^2=0.98$) at the rate of 1.03~$s$, while the~$\Sigma$ decreases linearly ($R^2>0.99$) at the rate of 0.14~$km/h$ per percent increase in the total vehicle volume~$\mathcal{V}$.",2015-06-24T15:36:11Z,http://arxiv.org/pdf/1506.07424v1,2024-04-28,
1506.08231v3,"A zero-sum monetary system, interest rates, and implications","To the knowledge of the author, this is the first time it has been shown that interest rates that are extremely high by modern standards (100% and higher) are necessary within a zero-sum monetary system, and not just driven by greed. Extreme interest rates that appeared in various places and times reinforce the idea that hard money may have contributed to high rates of interest. Here a model is presented that examines the interest rate required to succeed as an investor in a zero-sum fixed quantity hard-money system. Even when the playing field is significantly tilted toward the investor, interest rates need to be much higher than expected. In a completely fair zero-sum system, an investor cannot break even without charging 100% interest. Even with a 5% advantage, an investor won't break even at 15% interest. From this it is concluded that what we consider usurious rates today are, within a hard-money system, driven by necessity.   Cryptocurrency is a novel form of hard-currency. The inability to virtualize the money creates a system close to zero-sum because of the limited supply design. Therefore, within the bounds of a cryptocurrency system that limits money creation, interest rates must rise to levels that the modern world considers usury. It is impossible, therefore, that a cryptocurrency that is not expandable could take over a modern economy and replace modern fiat currency.",2015-06-26T22:23:22Z,http://arxiv.org/pdf/1506.08231v3,2024-04-28,
1506.09000v1,"Decision-level multi-method fusion of spatially scattered data from nondestructive inspection of ferromagnetic parts","This article deals with the fusion of flaw detections from multi-sensor nondestructive materials testing. Because each testing method makes use of different physical effects for defect localization, a multi-method approach is promising to effectively distinguish the many false alarms from actual material defects. To this end, we propose a new fusion technique for scattered two- or three-dimensional location data. Using a density-based approach, the proposed method is able to explicitly address the localization uncertainties such as registration errors. We provide guidelines on how to set all key parameters and demonstrate the technique's robustness. Finally, we apply our fusion approach to experimental data and demonstrate its ability to find small defects by substantially reducing false alarms under conditions where no single-sensor method is adequate.",2015-06-30T09:13:13Z,http://arxiv.org/pdf/1506.09000v1,2024-04-28,
1507.00219v1,"TurboMOR: an Efficient Model Order Reduction Technique for RC Networks with Many Ports","Model order reduction (MOR) techniques play a crucial role in the computer-aided design of modern integrated circuits, where they are used to reduce the size of parasitic networks. Unfortunately, the efficient reduction of passive networks with many ports is still an open problem. Existing techniques do not scale well with the number of ports, and lead to dense reduced models that burden subsequent simulations. In this paper, we propose TurboMOR, a novel MOR technique for the efficient reduction of passive RC networks. TurboMOR is based on moment-matching, achieved through efficient congruence transformations based on Householder reflections. A novel feature of TurboMOR is the block-diagonal structure of the reduced models, that makes them more efficient than the dense models produced by existing techniques. Moreover, the model structure allows for an insightful interpretation of the reduction process in terms of system theory. Numerical results show that TurboMOR scales more favourably than existing techniques in terms of reduction time, simulation time and memory consumption.",2015-07-01T13:12:38Z,http://arxiv.org/pdf/1507.00219v1,2024-04-28,
1507.01894v2,"On pore-scale modeling and simulation of reactive transport in 3D geometries","Pore-scale modeling and simulation of reactive flow in porous media has a range of diverse applications, and poses a number of research challenges. It is known that the morphology of a porous medium has significant influence on the local flow rate, which can have a substantial impact on the rate of chemical reactions. While there are a large number of papers and software tools dedicated to simulating either fluid flow in 3D computerized tomography (CT) images or reactive flow using pore-network models, little attention to date has been focused on the pore-scale simulation of sorptive transport in 3D CT images, which is the specific focus of this paper. Here we first present an algorithm for the simulation of such reactive flows directly on images, which is implemented in a sophisticated software package. We then use this software to present numerical results in two resolved geometries, illustrating the importance of pore-scale simulation and the flexibility of our software package.",2015-07-07T17:38:44Z,http://arxiv.org/pdf/1507.01894v2,2024-04-28,
1507.06565v1,"Large scale lattice Boltzmann simulation for the coupling of free and porous media flow","In this work, we investigate the interaction of free and porous media flow by large scale lattice Boltzmann simulations. We study the transport phenomena at the porous interface on multiple scales, i.e., we consider both, computationally generated pore-scale geometries and homogenized models at a macroscopic scale. The pore-scale results are compared to those obtained by using different transmission models. Two-domain approaches with sharp interface conditions, e.g., of Beavers--Joseph--Saffman type, as well as a single-domain approach with a porosity depending viscosity are taken into account. For the pore-scale simulations, we use a highly scalable communication-reducing scheme with a robust second order boundary handling. We comment on computational aspects of the pore-scale simulation and on how to generate pore-scale geometries. The two-domain approaches depend sensitively on the choice of the exact position of the interface, whereas a well-designed single-domain approach can significantly better recover the averaged pore-scale results.",2015-07-23T17:03:20Z,http://arxiv.org/pdf/1507.06565v1,2024-04-28,
1508.03604v1,"MOLNs: A cloud platform for interactive, reproducible and scalable spatial stochastic computational experiments in systems biology using PyURDME","Computational experiments using spatial stochastic simulations have led to important new biological insights, but they require specialized tools, a complex software stack, as well as large and scalable compute and data analysis resources due to the large computational cost associated with Monte Carlo computational workflows. The complexity of setting up and managing a large-scale distributed computation environment to support productive and reproducible modeling can be prohibitive for practitioners in systems biology. This results in a barrier to the adoption of spatial stochastic simulation tools, effectively limiting the type of biological questions addressed by quantitative modeling. In this paper, we present PyURDME, a new, user-friendly spatial modeling and simulation package, and MOLNs, a cloud computing appliance for distributed simulation of stochastic reaction-diffusion models. MOLNs is based on IPython and provides an interactive programming platform for development of sharable and reproducible distributed parallel computational experiments.",2015-08-14T18:49:36Z,http://arxiv.org/pdf/1508.03604v1,2024-04-28,
1508.03882v2,"Quantifying and Visualizing Uncertainties in Molecular Models","Computational molecular modeling and visualization has seen significant progress in recent years with sev- eral molecular modeling and visualization software systems in use today. Nevertheless the molecular biology community lacks techniques and tools for the rigorous analysis, quantification and visualization of the associated errors in molecular structure and its associated properties. This paper attempts at filling this vacuum with the introduction of a systematic statistical framework where each source of structural uncertainty is modeled as a ran- dom variable (RV) with a known distribution, and properties of the molecules are defined as dependent RVs. The framework consists of a theoretical basis, and an empirical implementation where the uncertainty quantification (UQ) analysis is achieved by using Chernoff-like bounds. The framework enables additionally the propagation of input structural data uncertainties, which in the molecular protein world are described as B-factors, saved with almost all X-ray models deposited in the Protein Data Bank (PDB). Our statistical framework is also able and has been applied to quantify and visualize the uncertainties in molecular properties, namely solvation interfaces and solvation free energy estimates. For each of these quantities of interest (QOI) of the molecular models we provide several novel and intuitive visualizations of the input, intermediate, and final propagated uncertainties. These methods should enable the end user achieve a more quantitative and visual evaluation of various molecular PDB models for structural and property correctness, or the lack thereof.",2015-08-17T00:18:08Z,http://arxiv.org/pdf/1508.03882v2,2024-04-28,
1508.04105v1,"PTILE: A framework for the Evaluation of Power Transformer Insulation Life in Electric Power System","In this paper, a framework is developed for power transformer (Generator Step up Unit) insulation life evaluation (PTILE) study on power system Network. Parameters used for studies include real time sample data obtained from power transformer field studies in the South-South Niger Delta region of Nigeria. It is used for performing simulations over varying number of years. Simulation reports shows a polynomial running time complexity and validates the stochastic Hot Spot theory indicating that the transformers in such region should be replaced sooner due to higher hot spots and transformer loading in such regions",2015-08-17T18:37:58Z,http://arxiv.org/pdf/1508.04105v1,2024-04-28,
1508.07435v3,"Subdifferential-based implicit return-mapping operators in Mohr-Coulomb plasticity","The paper is devoted to a constitutive solution, limit load analysis and Newton-like methods in elastoplastic problems containing the Mohr-Coulomb yield criterion. Within the constitutive problem, we introduce a self-contained derivation of the implicit return-mapping solution scheme using a recent subdifferential-based treatment. Unlike conventional techniques based on Koiter's rules, the presented scheme a priori detects a position of the unknown stress tensor on the yield surface even if the constitutive solution cannot be found in closed form. This fact eliminates blind guesswork from the scheme, enables to analyze properties of the constitutive operator, and simplifies construction of the consistent tangent operator which is important for the semismooth Newton method applied on the incremental boundary value elastoplastic problem. The incremental problem in Mohr-Coulomb plasticity is combined with the limit load analysis. Beside a conventional direct method of the incremental limit analysis, a recent indirect one is introduced and its advantages are described. The paper contains 2D and 3D numerical experiments on slope stability with publicly available Matlab implementations.",2015-08-29T11:11:40Z,http://arxiv.org/pdf/1508.07435v3,2024-04-28,
1509.01693v1,"The Economic Dispatch for Integrated Wind Power Systems Using Particle Swarm Optimization","The economic dispatch of wind power units is quite different from that in conventional thermal units, since the adopted model should take into consideration the intermittency nature of wind speed as well. Therefore, this paper uses a model that takes into account the aforementioned consideration in addition to whether the utility owns wind turbines or not. The economic dispatch is solved by using one of the modern optimization algorithms: the particle swarm optimization algorithm. A 6-bus system is used and it includes wind-powered generators besides to thermal generators. The thorough analysis of the results is also provided.",2015-09-05T11:24:36Z,http://arxiv.org/pdf/1509.01693v1,2024-04-28,
1509.01709v2,"Algorithm for estimating swirl angles in multi-intake hydraulic sumps","The paper has been withdrawn effective November 18, 2015.   Hydraulic Pump sumps are designed to provide a swirl free flow to the pump. The degree of swirl is measured in physical model tests using a swirl meter and a quantity known as swirl angle is generally measured. The present paper presents a novel method to compute the bulk swirl angle using the local velocity field obtained from computational fluid dynamics data. The basis for the present method is the conservation of angular momentum conservation. By carrying out both numerical and experimental studies the novel swirl angle calculation method is validated. Further the effect of vortex suppression devices in reducing the swirl angle is also demonstrated.",2015-09-05T15:37:11Z,http://arxiv.org/pdf/1509.01709v2,2024-04-28,
1509.03198v1,"Agent enabled Mining of Distributed Protein Data Banks","Mining biological data is an emergent area at the intersection between bioinformatics and data mining (DM). The intelligent agent based model is a popular approach in constructing Distributed Data Mining (DDM) systems to address scalable mining over large scale distributed data. The nature of associations between different amino acids in proteins has also been a subject of great anxiety. There is a strong need to develop new models and exploit and analyze the available distributed biological data sources. In this study, we have designed and implemented a multi-agent system (MAS) called Agent enriched Quantitative Association Rules Mining for Amino Acids in distributed Protein Data Banks (AeQARM-AAPDB). Such globally strong association rules enhance understanding of protein composition and are desirable for synthesis of artificial proteins. A real protein data bank is used to validate the system.",2015-06-19T07:44:29Z,http://arxiv.org/pdf/1509.03198v1,2024-04-28,
1509.04250v1,"The response of grandstands driven by filtered Gaussian white noise processes","This paper presents a semi-analytical estimate of the response of a grandstand occupied by an active crowd and by a passive crowd. Filtered Gaussian white noise processes are used to approximate the loading terms representing an active crowd. Lumped biodynamic models with a single degree of freedom are included to reflect passive spectators occupying the structure. The response is described in terms of the first two moments, employing the It\^o formula and the state augmentation method for the stationary time domain solution. The quality of the approximation is compared on the basis of three examples of varying complexity using Monte Carlo simulation based on a synthetic generator available in the literature. For comparative purposes, there is also a brief review of frequency domain estimates.",2015-09-14T19:23:06Z,http://arxiv.org/pdf/1509.04250v1,2024-04-28,
1509.05208v1,"Numerical simulation of the stress-strain state of the dental system","We present mathematical models, computational algorithms and software, which can be used for prediction of results of prosthetic treatment. More interest issue is biomechanics of the periodontal complex because any prosthesis is accompanied by a risk of overloading the supporting elements. Such risk can be avoided by the proper load distribution and prediction of stresses that occur during the use of dentures. We developed the mathematical model of the periodontal complex and its software implementation. This model is based on linear elasticity theory and allows to calculate the stress and strain fields in periodontal ligament and jawbone. The input parameters for the developed model can be divided into two groups. The first group of parameters describes the mechanical properties of periodontal ligament, teeth and jawbone (for example, elasticity of periodontal ligament etc.). The second group characterized the geometric properties of objects: the size of the teeth, their spatial coordinates, the size of periodontal ligament etc. The mechanical properties are the same for almost all, but the input of geometrical data is complicated because of their individual characteristics. In this connection, we develop algorithms and software for processing of images obtained by computed tomography (CT) scanner and for constructing individual digital model of the tooth-periodontal ligament-jawbone system of the patient. Integration of models and algorithms described allows to carry out biomechanical analysis on three-dimensional digital model and to select prosthesis design.",2015-09-17T11:19:42Z,http://arxiv.org/pdf/1509.05208v1,2024-04-28,
1509.08357v1,"Skin Effect Modeling in Conductors of Arbitrary Shape Through a Surface Admittance Operator and the Contour Integral Method","An accurate modeling of skin effect inside conductors is of capital importance to solve transmission line and scattering problems. This paper presents a surface-based formulation to model skin effect in conductors of arbitrary cross section, and compute the per-unit-length impedance of a multiconductor transmission line. The proposed formulation is based on the Dirichlet-Neumann operator that relates the longitudinal electric field to the tangential magnetic field on the boundary of a conductor. We demonstrate how the surface operator can be obtained through the contour integral method for conductors of arbitrary shape. The proposed algorithm is simple to implement, efficient, and can handle arbitrary cross-sections, which is a main advantage over the existing approach based on eigenfunctions, which is available only for canonical conductor's shapes. The versatility of the method is illustrated through a diverse set of examples, which includes transmission lines with trapezoidal, curved, and V-shaped conductors. Numerical results demonstrate the accuracy, versatility, and efficiency of the proposed technique.",2015-09-28T15:23:48Z,http://arxiv.org/pdf/1509.08357v1,2024-04-28,
1509.09211v1,"Normalized rotation shape descriptors and lossy compression of molecular shape","There is a common need to search of molecular databases for compounds resembling some shape, what suggests having similar biological activity while searching for new drugs. The large size of the databases requires fast methods for such initial screening, for example based on feature vectors constructed to fulfill the requirement that similar molecules should correspond to close vectors. Ultrafast Shape Recognition (USR) is a popular approach of this type. It uses vectors of 12 real number as 3 first moments of distances from 4 emphasized points. These coordinates might contain unnecessary correlations and does not allow to reconstruct the approximated shape. In contrast, spherical harmonic (SH) decomposition uses orthogonal coordinates, suggesting their independence and so lager informational content of the feature vector. There is usually considered rotationally invariant SH descriptors, what means discarding of some essential information.   This article discusses framework for descriptors with normalized rotation, for example by using principal component analysis (PCA-SH). As one of the most interesting are ligands which have to slide into a protein, we will introduce descriptors optimized for such flat elongated shapes. Bent deformed cylinder (BDC) describes the molecule as a cylinder which was first bent, then deformed such that its cross-sections became ellipses of evolving shape. Legendre polynomials are used to describe the central axis of such bent cylinder. Additional polynomials are used to define evolution of such elliptic cross-section along the main axis. There will be also discussed bent cylindrical harmonics (BCH), which uses cross-sections described by cylindrical harmonics instead of ellipses. All these normalized rotation descriptors allow to reconstruct (decode) the approximated representation of the shape, hence can be also used for lossy compression purposes.",2015-09-30T15:11:33Z,http://arxiv.org/pdf/1509.09211v1,2024-04-28,
1510.03035v1,"Reliability Analysis of Processes with Moving Cracked Material","The reliability of processes with moving elastic and isotropic material containing initial cracks is considered in terms of fracture. The material is modelled as a moving plate which is simply supported from two of its sides and subjected to homogeneous tension acting in the travelling direction. For tension, two models are studied: i) tension is constant with respect to time, and ii) tension varies temporally according to an Ornstein-Uhlenbeck process. Cracks of random length are assumed to occur in the material according to a stochastic counting process. For a general counting process, a representation of the nonfracture probability of the system is obtained that exploits conditional Monte Carlo simulation. Explicit formulae are derived for special cases. To study the reliability of the system with temporally varying tension, a known explicit result for the first passage time of an Ornstein-Uhlenbeck process to a constant boundary is utilized. Numerical examples are provided for printing presses and paper material.",2015-10-11T11:01:30Z,http://arxiv.org/pdf/1510.03035v1,2024-04-28,
1510.03102v3,"Dynamic Robust Transmission Expansion Planning","Recent breakthroughs in Transmission Network Expansion Planning (TNEP) have demonstrated that the use of robust optimization, as opposed to stochastic programming methods, renders the expansion planning problem considering uncertainties computationally tractable for real systems. However, there is still a yet unresolved and challenging problem as regards the resolution of the dynamic TNEP problem (DTNEP), which considers the year-by-year representation of uncertainties and investment decisions in an integrated way. This problem has been considered to be a highly complex and computationally intractable problem, and most research related to this topic focuses on very small case studies or used heuristic methods and has lead most studies about TNEP in the technical literature to take a wide spectrum of simplifying assumptions. In this paper an adaptive robust transmission network expansion planning formulation is proposed for keeping the full dynamic complexity of the problem. The method overcomes the problem size limitations and computational intractability associated with dynamic TNEP for realistic cases. Numerical results from an illustrative example and the IEEE 118-bus system are presented and discussed, demonstrating the benefits of this dynamic TNEP approach with respect to classical methods.",2015-10-11T21:04:27Z,http://arxiv.org/pdf/1510.03102v3,2024-04-28,
1510.06482v2,"Triangular Alignment (TAME): A Tensor-based Approach for Higher-order Network Alignment","Network alignment has extensive applications in comparative interactomics. Traditional approaches aim to simultaneously maximize the number of conserved edges and the underlying similarity of aligned entities. We propose a novel formulation of the network alignment problem that extends topological similarity to higher-order structures and provides a new objective function that maximizes the number of aligned substructures. This objective function corresponds to an integer programming problem, which is NP-hard. Consequently, we identify a closely related surrogate function whose maximization results in a tensor eigenvector problem. Based on this formulation, we present an algorithm called Triangular AlignMEnt (TAME), which attempts to maximize the number of aligned triangles across networks. Using a case study on the NAPAbench dataset, we show that triangular alignment is capable of producing mappings with high node correctness. We further evaluate our method by aligning yeast and human interactomes. Our results indicate that TAME outperforms the state-of-art alignment methods in terms of conserved triangles. In addition, we show that the number of conserved triangles is more significantly correlated, compared to the conserved edge, with node correctness and co-expression of edges. Our formulation and resulting algorithms can be easily extended to arbitrary motifs.",2015-10-22T02:55:53Z,http://arxiv.org/pdf/1510.06482v2,2024-04-28,
1510.07020v2,"An Efficient Polyphase Filter Based Resampling Method for Unifying the PRFs in SAR Data","Variable and higher pulse repetition frequencies (PRFs) are increasingly being used to meet the stricter requirements and complexities of current airborne and spaceborne synthetic aperture radar (SAR) systems associated with higher resolution and wider area products. POLYPHASE, the proposed resampling scheme, downsamples and unifies variable PRFs within a single look complex (SLC) SAR acquisition and across a repeat pass sequence of acquisitions down to an effective lower PRF. A sparsity condition of the received SAR data ensures that the uniformly resampled data approximates the spectral properties of a decimated densely sampled version of the received SAR data. While experiments conducted with both synthetically generated and real airborne SAR data show that POLYPHASE retains comparable performance to the state-of-the-art BLUI scheme in image quality, a polyphase filter-based implementation of POLYPHASE offers significant computational savings for arbitrary (not necessarily periodic) input PRF variations, thus allowing fully on-board, in-place, and real-time implementation.",2015-10-22T12:54:40Z,http://arxiv.org/pdf/1510.07020v2,2024-04-28,
1511.01380v1,"Isogeometric approach for nonlinear bending and post-buckling analysis of functionally graded plates under thermal environment","In this paper, equilibrium and stability equations of functionally graded material (FGM) plate under thermal environment are formulated based on isogeometric analysis (IGA) in combination with higher-order shear deformation theory (HSDT). The FGM plate is made by a mixture of two distinct components, for which material properties not only vary continuously through thickness according to a power-law distribution but also are assumed to be a function of temperature. Temperature field is assumed to be constant in any plane and uniform, linear and nonlinear through plate thickness, respectively. The governing equation is in nonlinear form based on von Karman assumption and thermal effect. A NURBS-based isogeometric finite element formulation is utilized to naturally fulfil the rigorous C1-continuity required by the present plate model. Influences of gradient indices, boundary conditions, temperature distributions, material properties, length-to-thickness ratios on the behaviour of FGM plate are discussed in details. Numerical results demonstrate excellent performance of the present approach.",2015-11-04T16:06:46Z,http://arxiv.org/pdf/1511.01380v1,2024-04-28,
1511.08410v1,"Modeling of anechoich chambers with equivalent materials and equivalent sources","Numerical simulation of anechoic chambers is a hot topic since it can provide useful data about the performance of the EMC site. However, the mathematical nature of the problem, the physical dimensions of the simulated sites and the frequency ranges pose nontrivial challenges to the simulation. Computational requirements in particular will quickly become unmanageable if adequate techniques are not employed. In this work we describe a novel approach, based on equivalent elements, that enables the simulation of large chambers with modest computational resources. The method is then validated against real measurement results.",2015-11-26T15:09:59Z,http://arxiv.org/pdf/1511.08410v1,2024-04-28,
1512.02183v1,"Wavelet Based Load Models from AMI Data","A major challenge of using AMI data in power system analysis is the large size of the data sets. For rapid analysis that addresses historical behavior of systems consisting of a few hundred feeders, all of the AMI load data can be loaded into memory and used in a power flow analysis. However, if a system contains thousands of feeders then the handling of the AMI data in the analysis becomes more challenging. The work here seeks to demonstrate that the information contained in large AMI data sets can be compressed into accurate load models using wavelets. Two types of wavelet based load models are considered, the multi-resolution wavelet load model for each individual customer and the classified wavelet load model for customers that share similar load patterns. The multi-resolution wavelet load model compresses the data, and the classified wavelet load model further compresses the data. The method of grouping customers into classes using the wavelet based classification technique is illustrated.",2015-12-07T19:34:58Z,http://arxiv.org/pdf/1512.02183v1,2024-04-28,
1604.00547v1,"Isogeometric analysis for functionally graded microplates based on modified couple stress theory","Analysis of static bending, free vibration and buckling behaviours of functionally graded microplates is investigated in this study. The main idea is to use the isogeometric analysis in associated with novel four-variable refined plate theory and quasi-3D theory. More importantly, the modified couple stress theory with only one material length scale parameter is employed to effectively capture the size-dependent effects within the microplates. Meanwhile, the quasi-3D theory which is constructed from a novel seventh-order shear deformation refined plate theory with four unknowns is able to consider both shear deformations and thickness stretching effect without requiring shear correction factors. The NURBS-based isogeometric analysis is integrated to exactly describe the geometry and approximately calculate the unknown fields with higher-order derivative and continuity requirements. The convergence and verification show the validity and efficiency of this proposed computational approach in comparison with those existing in the literature. It is further applied to study the static bending, free vibration and buckling responses of rectangular and circular functionally graded microplates with various types of boundary conditions. A number of investigations are also conducted to illustrate the effects of the material length scale, material index, and length-to-thickness ratios on the responses of the microplates.",2016-04-02T19:33:15Z,http://arxiv.org/pdf/1604.00547v1,2024-04-28,
1604.00594v1,"Two Dimensional Angle of Arrival Estimation","We present a new method for the estimation of two dimensional (2D) angles of arrival (AOAs), namely, azimuth and incidence angles of multiple narrowband signals of same frequency in the far field of antenna array.",2016-04-03T05:21:19Z,http://arxiv.org/pdf/1604.00594v1,2024-04-28,
1604.01367v1,"Isogeometric nonlinear bending and buckling analysis of variable-thickness composite plate structures","This paper investigates nonlinear bending and buckling behaviours of composite plates characterized by a thickness variation. Layer interfaces are described as functions of inplane coordinates. Top and bottom surfaces of the plate are symmetric about the midplane and the plate could be considered as a flat surface in analysis along with thickness parameters which vary over the plate. The variable thickness at a certain position in the midplane is modeled by a set of control points (or thickness-parameters) through NURBS (Non-Uniform Rational B-Spline) basic functions. The knot parameter space which is referred in modelling geometry and approximating displacement variables is employed for approximating thickness, simultaneously. The use of quadratic NURBS functions results in C^1 continuity of modeling variable thickness and analyzing solutions. Thin to moderately thick laminates in bound of first-order shear deformation theory (FSDT) are taken into account. Strain-displacement relations in sense of von-Karman theory are employed for large deformation. Riks method is used for geometrically nonlinear analysis. The weak form is approximated numerically by the isogeometric analysis (IGA), which has been found to be a robust, stable and realistic numerical tool. Numerical results confirm the reliability and capacity of the propose method.",2016-04-03T19:45:23Z,http://arxiv.org/pdf/1604.01367v1,2024-04-28,
1604.01651v3,"Representing model inadequacy: A stochastic operator approach","Mathematical models of physical systems are subject to many uncertainties such as measurement errors and uncertain initial and boundary conditions. After accounting for these uncertainties, it is often revealed that discrepancies between the model output and the observations remain; if so, the model is said to be inadequate. In practice, the inadequate model may be the best that is available or tractable, and so despite its inadequacy the model may be used to make predictions of unobserved quantities. In this case, a representation of the inadequacy is necessary, so the impact of the observed discrepancy can be determined. We investigate this problem in the context of chemical kinetics and propose a new technique to account for model inadequacy that is both probabilistic and physically meaningful. A stochastic inadequacy operator $\mathcal{S}$ is introduced which is embedded in the ODEs describing the evolution of chemical species concentrations and which respects certain physical constraints such as conservation laws. The parameters of $\mathcal{S}$ are governed by probability distributions, which in turn are characterized by a set of hyperparameters. The model parameters and hyperparameters are calibrated using high-dimensional hierarchical Bayesian inference. We apply the method to a typical problem in chemical kinetics---the reaction mechanism of hydrogen combustion.",2016-04-06T14:57:56Z,http://arxiv.org/pdf/1604.01651v3,2024-04-28,
1604.03195v2,"Multibody minimum-energy trajectory with applications to protein folding","This work addresses the optimal control of multibody systems being actuated with control forces in order to find a dynamically feasible minimum-energy trajectory of the system. The optimal control problem and its constraints are integrated in a discrete version of the equation of motion allowing the minimization of system energy with respect to a discrete state and control trajectory. The work is centred on a specific type of open-chain multibody system, with strong local propensity, where the overall system kinematics is described essentially by the torsion around the links that connect rigid bodies. The coupling between the rigid body motion, and the optimal conformation is described as an elastic band of replicas of the original system with different conformations. The band forces are used to control system's motion directly, reflecting the influence of the system energy field on its conformation, using for that the Nudged-Elastic Band method. Here the equation of motion of the multibody grid are solved by using the augmented Lagrangean method. In this context, if a feasible minimum-energy trajectory of the original system exists it is a stationary state of the extended system. This approach is applied to the folding of a single chain protein.",2016-04-12T01:26:44Z,http://arxiv.org/pdf/1604.03195v2,2024-04-28,
1604.05015v1,"Using Clustering Method to Understand Indian Stock Market Volatility","In this paper we use Clustering Method to understand whether stock market volatility can be predicted at all, and if so, when it can be predicted. The exercise has been performed for the Indian stock market on daily data for two years. For our analysis we map number of clusters against number of variables. We then test for efficiency of clustering. Our contention is that, given a fixed number of variables, one of them being historic volatility of NIFTY returns, if increase in the number of clusters improves clustering efficiency, then volatility cannot be predicted. Volatility then becomes random as, for a given time period, it gets classified in various clusters. On the other hand, if efficiency falls with increase in the number of clusters, then volatility can be predicted as there is some homogeneity in the data. If we fix the number of clusters and then increase the number of variables, this should have some impact on clustering efficiency. Indeed if we can hit upon, in a sense, an optimum number of variables, then if the number of clusters is reasonably small, we can use these variables to predict volatility. The variables that we consider for our study are volatility of NIFTY returns, volatility of gold returns, India VIX, CBOE VIX, volatility of crude oil returns, volatility of DJIA returns, volatility of DAX returns, volatility of Hang Seng returns and volatility of Nikkei returns. We use three clustering algorithms namely Kernel K-Means, Self Organizing Maps and Mixture of Gaussian models and two internal clustering validity measures, Silhouette Index and Dunn Index, to assess the quality of generated clusters.",2016-04-18T06:58:30Z,http://arxiv.org/pdf/1604.05015v1,2024-04-28,
1604.05122v1,"Numerical solution of a parabolic system in air pollution","An air pollution model is generally described by a system of PDEs on unbounded domain. Transformation of the independent variable is used to convert the problem for nonlinear air pollution on finite computational domain. We investigate the new, degenerated parabolic problem in Sobolev spaces with weights for well-posedness and positivity of the solution. Then we construct a fitted finite volume difference scheme. Some results from computations are presented.",2016-04-18T12:44:39Z,http://arxiv.org/pdf/1604.05122v1,2024-04-28,
1604.06777v1,"Global Sensitivity Analysis with 2D Hydraulic Codes: Application on Uncertainties Related to High-Resolution Topographic Data","Technologies such as aerial photogrammetry allow production of 3D topographic data including complex environments such as urban areas. Therefore, it is possible to create High Resolution (HR) Digital Elevation Models (DEM) incorporating thin above ground elements influencing overland flow paths. Even though this category of big data has a high level of accuracy, there are still errors in measurements and hypothesis under DEM elaboration. Moreover, operators look for optimizing spatial discretization resolution in order to improve flood models computation time. Errors in measurement, errors in DEM generation, and operator choices for inclusion of this data within 2D hydraulic model, might influence results of flood models simulations. These errors and hypothesis may influence significantly flood modelling results variability. The purpose of this study is to investigate uncertainties related to (i) the own error of high resolution topographic data, and (ii) the modeller choices when including topographic data in hydraulic codes. The aim is to perform a Global Sensitivity Analysis (GSA) which goes through a Monte-Carlo uncertainty propagation, to quantify impact of uncertainties, followed by a Sobol' indices computation, to rank influence of identified parameters on result variability. A process using a coupling of an environment for parametric computation (Prom{\'e}th{\'e}e) and a code relying on 2D shallow water equations (FullSWOF 2D) has been developed (P-FS tool). The study has been performed over the lower part of the Var river valley using the estimated hydrograph of 1994 flood event. HR topographic data has been made available for the study area, which is 17.5 km 2 , by Nice municipality. Three uncertain parameters were studied: the measurement error (var. E), the level of details of above-ground element representation in DEM (buildings, sidewalks, etc.) (var. S), and the spatial discretization resolution (grid cell size for regular mesh) (var. R). Parameter var. E follows a probability density function, whereas parameters var. S and var. R. are discrete operator choices. Combining these parameters, a database of 2, 000 simulations has been produced using P-FS tool implemented on a high performance computing structure. In our study case, the output of interest is the maximal",2016-03-24T07:56:43Z,http://arxiv.org/pdf/1604.06777v1,2024-04-28,
1606.00093v1,"ExTASY: Scalable and Flexible Coupling of MD Simulations and Advanced Sampling Techniques","For many macromolecular systems the accurate sampling of the relevant regions on the potential energy surface cannot be obtained by a single, long Molecular Dynamics (MD) trajectory. New approaches are required to promote more efficient sampling. We present the design and implementation of the Extensible Toolkit for Advanced Sampling and analYsis (ExTASY) for building and executing advanced sampling workflows on HPC systems. ExTASY provides Python based ""templated scripts"" that interface to an interoperable and high-performance pilot-based run time system, which abstracts the complexity of managing multiple simulations. ExTASY supports the use of existing highly-optimised parallel MD code and their coupling to analysis tools based upon collective coordinates which do not require a priori knowledge of the system to bias. We describe two workflows which both couple large ""ensembles"" of relatively short MD simulations with analysis tools to automatically analyse the generated trajectories and identify molecular conformational structures that will be used on-the-fly as new starting points for further ""simulation-analysis"" iterations. One of the workflows leverages the Locally Scaled Diffusion Maps technique; the other makes use of Complementary Coordinates techniques to enhance sampling and generate start-points for the next generation of MD simulations. We show that the ExTASY tools have been deployed on a range of HPC systems including ARCHER (Cray CX30), Blue Waters (Cray XE6/XK7), and Stampede (Linux cluster), and that good strong scaling can be obtained up to 1000s of MD simulations, independent of the size of each simulation. We discuss how ExTASY can be easily extended or modified by end-users to build their own workflows, and ongoing work to improve the usability and robustness of ExTASY.",2016-06-01T02:01:25Z,http://arxiv.org/pdf/1606.00093v1,2024-04-28,
1606.01243v1,"BES with FEM: Building Energy Simulation using Finite Element Methods","An overall objective of energy efficiency in the built environment is to improve building and systems performances in terms of durability, comfort and economics. In order to predict, improve and meet a certain set of performance requirements related to the indoor climate of buildings and the associated energy demand, building energy simulation (BES) tools are indispensable. Due to the rapid development of FEM software and the Multiphysics approaches, it should possible to build and simulate full 3D models of buildings regarding the energy demand. The paper presents a methodology for performing building energy simulation with Comsol. The method was applied to an international test box experiment. The results showed an almost perfect agreement between the used BES model and Comsol. These preliminary results confirm the great opportunities to use FEM related software for building energy performance simulation.",2016-06-03T08:20:32Z,http://arxiv.org/pdf/1606.01243v1,2024-04-28,
1606.02422v4,"Bayesian inference for the stochastic identification of elastoplastic material parameters: Introduction, misconceptions and insights","We discuss Bayesian inference (BI) for the probabilistic identification of material parameters. This contribution aims to shed light on the use of BI for the identification of elastoplastic material parameters. For this purpose a single spring is considered, for which the stress-strain curves are artificially created. Besides offering a didactic introduction to BI, this paper proposes an approach to incorporate statistical errors both in the measured stresses, and in the measured strains. It is assumed that the uncertainty is only due to measurement errors and the material is homogeneous. Furthermore, a number of possible misconceptions on BI are highlighted based on the purely elastic case.",2016-06-08T07:03:09Z,http://arxiv.org/pdf/1606.02422v4,2024-04-28,
1606.03855v2,"Mathematical Modeling of Dynamics for Partially Filled Shells of Revolution","In this work we study the dynamic behaviour of compound shells of revolution partially filled with an ideal incompressible fluid based on boundary-value problems. New analytical mathematical model with corresponding discrete scheme for the elastic displacements and the dynamic liquid pressure is developed. The discrete scheme is based on the method of discrete singularities. A code to perform the numerical analysis is developed. Comprehensive benchmarking of the obtained results against other methods is done and good agreement is observed. The convergence of the proposed numerical method is demonstrated. One of the advantages of this new model is that the initial 3D problem is analytically reduced to a 1D integral equation. Moreover, it can handle the behaviour of the pressure in the vicinity of the nodes explicitly and the computational technique used has a quick convergence requiring a negligible amount of CPU time.",2016-06-13T08:27:21Z,http://arxiv.org/pdf/1606.03855v2,2024-04-28,
1606.06975v1,"Bias Correction in Saupe Tensor Estimation","Estimation of the Saupe tensor is central to the determination of molecular structures from residual dipolar couplings (RDC) or chemical shift anisotropies. Assuming a given template structure, the singular value decomposition (SVD) method proposed in Losonczi et al. 1999 has been used traditionally to estimate the Saupe tensor. Despite its simplicity, whenever the template structure has large structural noise, the eigenvalues of the estimated tensor have a magnitude systematically smaller than their actual values. This leads to systematic error when calculating the eigenvalue dependent parameters, magnitude and rhombicity. We propose here a Monte Carlo simulation method to remove such bias. We further demonstrate the effectiveness of our method in the setting when the eigenvalue estimates from multiple template protein fragments are available and their average is used as an improved eigenvalue estimator. For both synthetic and experimental RDC datasets of ubiquitin, when using template fragments corrupted by large noise, the magnitude of our proposed bias-reduced estimator generally reaches at least 90% of the actual value, whereas the magnitude of SVD estimator can be shrunk below 80% of the true value.",2016-06-22T15:05:23Z,http://arxiv.org/pdf/1606.06975v1,2024-04-28,
1606.08761v1,"A Dissipative Systems Theory for FDTD with Application to Stability Analysis and Subgridding","This paper establishes a far-reaching connection between the Finite-Difference Time-Domain method (FDTD) and the theory of dissipative systems. The FDTD equations for a rectangular region are written as a dynamical system having the magnetic and electric fields on the boundary as inputs and outputs. Suitable expressions for the energy stored in the region and the energy absorbed from the boundaries are introduced, and used to show that the FDTD system is dissipative under a generalized Courant-Friedrichs-Lewy condition. Based on the concept of dissipation, a powerful theoretical framework to investigate the stability of FDTD methods is devised. The new method makes FDTD stability proofs simpler, more intuitive, and modular. Stability conditions can indeed be given on the individual components (e.g. boundary conditions, meshes, embedded models) instead of the whole coupled setup. As an example of application, we derive a new subgridding method with material traverse, arbitrary grid refinement, and guaranteed stability. The method is easy to implement and has a straightforward stability proof. Numerical results confirm its stability, low reflections, and ability to handle material traverse.",2016-06-28T15:45:32Z,http://arxiv.org/pdf/1606.08761v1,2024-04-28,
1607.01033v1,"An Application of the EM-algorithm to Approximate Empirical Distributions of Financial Indices with the Gaussian Mixtures","In this study I briefly illustrate application of the Gaussian mixtures to approximate empirical distributions of financial indices (DAX, Dow Jones, Nikkei, RTSI, S&P 500). The resulting distributions illustrate very high quality of approximation as evaluated by Kolmogorov-Smirnov test. This implies further study of application of the Gaussian mixtures to approximate empirical distributions of financial indices.",2016-06-29T12:44:14Z,http://arxiv.org/pdf/1607.01033v1,2024-04-28,
1607.04596v4,"Solving the stochastic Landau-Lifshitz-Gilbert-Slonczewski equation for monodomain nanomagnets : A survey and analysis of numerical techniques","The stochastic Landau-Lifshitz-Gilbert-Slonczewski (s-LLGS) equation is widely used to study the temporal evolution of the macrospin subject to spin torque and thermal noise. The numerical simulation of the s-LLGS equation requires an appropriate choice of stochastic calculus and numerical integration scheme. In this paper, we comprehensively evaluate the accuracy and complexity of various numerical techniques to solve the s-LLGS equation. We focus on implicit midpoint, Heun, and Euler-Heun methods that converge to the Stratonovich solution of the s-LLGS equation. By performing numerical tests for both strong (path-wise) and weak (statistical) convergence, we quantify the accuracy of various numerical schemes used to solve the s-LLGS equation. We demonstrate a new method intended to solve Stochastic Differential Equations (SDEs) with small noise (RK4-Heun), and test its capability to handle the s-LLGS equation. We also discuss the circuit implementation of nanomagnets for large-scale SPICE-based simulations. We evaluate the efficacy of SPICE in handling the stochastic dynamics of the multiplicative noise in the s-LLGS equation. Numerical schemes such as Euler and Gear, typically used by SPICE-based circuit simulators do not yield the expected outcome when solving the Stratonovich s-LLGS equation. While the trapezoidal method in SPICE does solve for the Stratonovich solution, its accuracy is limited by the minimum time step of integration in SPICE. We implement the s-LLGS equation in both its cartesian and spherical coordinates form in SPICE and compare the stability and accuracy of the two implementations. The results in this paper will serve as guidelines for researchers to understand the tradeoffs between accuracy and complexity of various numerical methods and the choice of appropriate calculus to solve the s-LLGS equation.",2016-07-15T17:44:35Z,http://arxiv.org/pdf/1607.04596v4,2024-04-28,
1607.08853v1,"A Unified Approach for Beam-to-Beam Contact","Existing beam contact formulations can be categorized in point contact models that consider a discrete contact force at the closest point of the beams, and line contact models that assume distributed contact forces. In this work, it will be shown that line contact formulations provide accurate and robust mechanical models in the range of small contact angles, whereas the computational efficiency considerably decreases with increasing contact angles. On the other hand, point contact formulations serve as sufficiently accurate and very efficient models in the regime of large contact angles, while they are not applicable for small contact angles as a consequence of non-unique closest point projections. In order to combine the advantages of these basic formulations, a novel all-angle beam contact (ABC) formulation is developed that applies a point contact formulation for large contact angles and a recently developed line contact formulation for small contact angles, the two being smoothly connected by means of a variationally consistent model transition. Based on a stringent analysis, two different transition laws are investigated, optimal algorithmic parameters are suggested and conservation of energy and momentum is shown. All configuration-dependent quantities are consistently linearized, thus allowing for their application within implicit time integration schemes. Furthermore, a step size control of the nonlinear solution scheme that allows for large displacement increments per time step and an efficient two-stage contact search based on dynamically adapted search segments are proposed. A series of numerical test cases is analyzed in order to verify the accuracy and consistency of the proposed contact model transition regarding contact force distributions and conservation properties, but also for quantifying the efficiency gains as compared to standard beam contact formulations.",2016-07-29T15:45:42Z,http://arxiv.org/pdf/1607.08853v1,2024-04-28,
1608.03990v3,"Machine Learning-augmented Predictive Modeling of Turbulent Separated Flows over Airfoils","A modeling paradigm is developed to augment predictive models of turbulence by effectively utilizing limited data generated from physical experiments. The key components of our approach involve inverse modeling to infer the spatial distribution of model discrepancies, and, machine learning to reconstruct discrepancy information from a large number of inverse problems into corrective model forms. We apply the methodology to turbulent flows over airfoils involving flow separation. Model augmentations are developed for the Spalart Allmaras (SA) model using adjoint-based full field inference on experimentally measured lift coefficient data. When these model forms are reconstructed using neural networks (NN) and embedded within a standard solver, we show that much improved predictions in lift can be obtained for geometries and flow conditions that were not used to train the model. The NN-augmented SA model also predicts surface pressures extremely well. Portability of this approach is demonstrated by confirming that predictive improvements are preserved when the augmentation is embedded in a different commercial finite-element solver. The broader vision is that by incorporating data that can reveal the form of the innate model discrepancy, the applicability of data-driven turbulence models can be extended to more general flows.",2016-08-13T15:07:50Z,http://arxiv.org/pdf/1608.03990v3,2024-04-28,
1608.04998v1,"A Unified Finite Element Method for Fluid-Structure Interaction","In this article, we present a new unified finite element method (UFEM) for simulation of general Fluid-Structure interaction (FSI) which has the same generality and robustness as monolithic methods but is significantly more computationally efficient and easier to implement. Our proposed approach has similarities with classical immersed finite element methods (IFEMs), by approximating a single velocity and pressure field in the entire domain (i.e. occupied by fluid and solid) on a single mesh, but differs by treating the corrections due to the solid deformation on the left-hand side of the modified fluid flow equations (i.e. implicitly). The method is described in detail, followed by the presentation of multiple computational examples in order to validate it across a wide range of fluid and solid parameters and interactions.",2016-08-15T22:56:50Z,http://arxiv.org/pdf/1608.04998v1,2024-04-28,
1608.06654v1,"Stress-constrained continuum topology optimization: a new approach based on elasto-plasticity","A new approach for generating stress-constrained topological designs in continua is presented. The main novelty is in the use of elasto-plastic modeling and in optimizing the design such that it will exhibit a linear-elastic response. This is achieved by imposing a single global constraint on the total sum of equivalent plastic strains, providing accurate control over all local stress violations. The single constraint essentially replaces a large number of local stress constraints or an approximate aggregation of them--two common approaches in the literature. A classical rate-independent plasticity model is utilized, for which analytical adjoint sensitivity analysis is derived and verified. Several examples demonstrate the capability of the computational procedure to generate designs that challenge results from the literature, in terms of the obtained stiffness-strength-weight trade-offs. A full elasto-plastic analysis of the optimized designs shows that prior to the initial yielding, these designs can sustain significantly higher loads than minimum compliance topological layouts, with only a minor compromise on stiffness.",2016-08-23T21:06:28Z,http://arxiv.org/pdf/1608.06654v1,2024-04-28,
1610.00004v1,"Energy consistent framework for continuously evolving 3D crack propagation","This paper presents a formulation for brittle fracture in 3D elastic solids within the context of configurational mechanics. The local form of the first law of thermodynamics provides a condition for equilibrium of the crack front. The direction of the crack propagation is shown to be given by the direction of the configurational forces on the crack front that maximise the local dissipation. The evolving crack front is continuously resolved by the finite element mesh, without the need for face splitting or the use of enrichment techniques. A monolithic solution strategy is adopted, solving simultaneously for both the material displacements (i.e. crack extension) and the spatial displacements, is adopted. In order to trace the dissipative loading path, an arc-length procedure is developed that controls the incremental crack area growth. In order to maintain mesh quality, smoothing of the mesh is undertaken as a continuous process, together with face flipping, node merging and edge splitting where necessary. Hierarchical basis functions of arbitrary polynomial order are adopted to increase the order of approximation without the need to change the finite element mesh. Performance of the formulation is demonstrated by means of three representative numerical simulations, demonstrating both accuracy and robustness.",2016-09-30T11:16:35Z,http://arxiv.org/pdf/1610.00004v1,2024-04-28,
1610.01265v1,"Advancing parabolic operators in thermodynamic MHD models: Explicit super time-stepping versus implicit schemes with Krylov solvers","We explore the performance and advantages/disadvantages of using unconditionally stable explicit super time-stepping (STS) algorithms versus implicit schemes with Krylov solvers for integrating parabolic operators in thermodynamic MHD models of the solar corona. Specifically, we compare the second-order Runge-Kutta Legendre (RKL2) STS method with the implicit backward Euler scheme computed using the preconditioned conjugate gradient (PCG) solver with both a point-Jacobi and a non-overlapping domain decomposition ILU0 preconditioner. The algorithms are used to integrate anisotropic Spitzer thermal conduction and artificial kinematic viscosity at time-steps much larger than classic explicit stability criteria allow. A key component of the comparison is the use of an established MHD model (MAS) to compute a real-world simulation on a large HPC cluster. Special attention is placed on the parallel scaling of the algorithms. It is shown that, for a specific problem and model, the RKL2 method is comparable or surpasses the implicit method with PCG solvers in performance and scaling, but suffers from some accuracy limitations. These limitations, and the applicability of RKL methods are briefly discussed.",2016-10-05T03:32:11Z,http://arxiv.org/pdf/1610.01265v1,2024-04-28,
1610.02277v2,"Multi-mesh multi-objective optimization with application to a model problem in urban design","We present an application of multi-mesh finite element methods as part of a methodology for optimizing settlement layouts. By formulating a multi-objective optimization problem, we demonstrate how a given number of buildings may be optimally placed on a given piece of land with respect to both wind conditions and the view experienced from the buildings. The wind flow is modeled by a multi-mesh (cut finite element) method. This allows each building to be embedded in a boundary-fitted mesh which can be moved freely on top of a fixed background mesh. This approach enables a multitude of settlement layouts to be evaluated without the need for costly mesh generation when changing the configuration of buildings. The view is modeled by a measure that takes into account the totality of unobstructed view from the collection of buildings, and is efficiently computed by rasterization.",2016-10-07T13:33:50Z,http://arxiv.org/pdf/1610.02277v2,2024-04-28,
1610.03484v2,"Multi-scale computational homogenisation to predict the long-term durability of composite structures","A coupled hygro-thermo-mechanical computational model is proposed for fibre reinforced polymers, formulated within the framework of Computational Homogenisation (CH). At each macrostructure Gauss point, constitutive matrices for thermal, moisture transport and mechanical responses are calculated from CH of the underlying representative volume element (RVE). A degradation model, developed from experimental data relating evolution of mechanical properties over time for a given exposure temperature and moisture concentration is also developed and incorporated in the proposed computational model. A unified approach is used to impose the RVE boundary conditions, which allows convenient switching between linear Dirichlet, uniform Neumann and periodic boundary conditions. A plain weave textile composite RVE consisting of yarns embedded in a matrix is considered in this case. Matrix and yarns are considered as isotropic and transversely isotropic materials respectively. Furthermore, the computational framework utilises hierarchic basis functions and designed to take advantage of distributed memory high-performance computing.",2016-10-11T15:47:23Z,http://arxiv.org/pdf/1610.03484v2,2024-04-28,
1610.03932v3,"The Curvature-Augmented Closest Point Method with Vesicle Inextensibility Application","The Closest Point method, initially developed by Ruuth and Merriman, allows for the numerical solution of surface partial differential equations without the need for a parameterization of the surface itself. Surface quantities are embedded into the surrounding domain by assigning each value at a given spatial location to the corresponding value at the closest point on the surface. This embedding allows for surface derivatives to be replaced by their Cartesian counterparts (e.g. $\nabla_s = \nabla$). This equivalence is only valid on the surface, and thus, interpolation is used to enforce what is known as the side condition away from the surface. To improve upon the method, this work derives an operator embedding that incorporates curvature information, making it valid in a neighborhood of the surface. With this, direct enforcement of the side condition is no longer needed. Comparisons in $\mathbb{R}^2$ and $\mathbb{R}^3$ show that the resulting Curvature-Augmented Closest Point method has better accuracy and requires less memory, through increased matrix sparsity, than the Closest Point method, while maintaining similar matrix condition numbers. To demonstrate the utility of the method in a physical application, simulations of inextensible, bi-lipid vesicles evolving toward equilibrium shapes are also included.",2016-10-13T04:08:32Z,http://arxiv.org/pdf/1610.03932v3,2024-04-28,
1610.04303v1,"Electrothermal Simulation of Bonding Wire Degradation under Uncertain Geometries","In this paper, electrothermal field phenomena in electronic components are considered. This coupling is tackled by multiphysical field simulations using the Finite Integration Technique (FIT). In particular, the design of bonding wires with respect to thermal degradation is investigated. Instead of resolving the wires by the computational grid, lumped element representations are introduced as point-to-point connections in the spatially distributed model. Fabrication tolerances lead to uncertainties of the wires' parameters and influence the operation and reliability of the final product. Based on geometric measurements, the resulting variability of the wire temperatures is determined using the stochastic electrothermal field-circuit model.",2016-10-14T00:46:08Z,http://arxiv.org/pdf/1610.04303v1,2024-04-28,
1610.04304v1,"Automatic Generation of Equivalent Electrothermal SPICE Netlists from 3D Electrothermal Field Models","Starting from a 3D electrothermal field problem discretized by the Finite Integration Technique, the equivalence to a circuit description is shown by exploiting the analogy to the Modified Nodal Analysis approach. Using this analogy, an algorithm for the automatic generation of a monolithic SPICE netlist is presented. Joule losses from the electrical circuit are included as heat sources in the thermal circuit. The thermal simulation yields nodal temperatures that influence the electrical conductivity. Apart from the used field discretization, this approach applies no further simplifications. An example 3D chip package is used to validate the algorithm.",2016-10-14T00:47:04Z,http://arxiv.org/pdf/1610.04304v1,2024-04-28,
1610.04610v1,"Three-dimensional nonlinear micro/meso-mechanical response of the fibre-reinforced polymer composites","A three-dimensional multi-scale computational homogenisation framework is developed for the prediction of nonlinear micro/meso-mechanical response of the fibre-reinforced polymer (FRP) composites. Two dominant damage mechanisms, i.e. matrix elasto-plastic response and fibre-matrix decohesion are considered and modelled using a non-associative pressure dependent paraboloidal yield criterion and cohesive interface elements respectively. A linear-elastic transversely isotropic material model is used to model yarns/fibres within the representative volume element (RVE). A unified approach is used to impose the RVE boundary conditions, which allows convenient switching between linear displacement, uniform traction and periodic boundary conditions. The computational model is implemented within the framework of the hierarchic finite element, which permits the use of arbitrary orders of approximation. Furthermore, the computational framework is designed to take advantage of distributed memory high-performance computing. The accuracy and performance of the computational framework are demonstrated with a variety of numerical examples, including unidirectional FRP composite, a composite comprising a multi-fibre and multi-layer RVE, with randomly generated fibres, and a single layered plain weave textile composite. Results are validated against the reference experimental/numerical results from the literature. The computational framework is also used to study the effect of matrix and fibre-matrix interfaces properties on the homogenised stress-strain responses.",2016-10-14T19:43:01Z,http://arxiv.org/pdf/1610.04610v1,2024-04-28,
1610.05472v2,"Simulation of electrical machines - A FEM-BEM coupling scheme","Electrical machines commonly consist of moving and stationary parts. The field simulation of such devices can be very demanding if the underlying numerical scheme is solely based on a domain discretization, such as in case of the Finite Element Method (FEM). Here, a coupling scheme based on FEM together with Boundary Element Methods (BEM) is presented that neither hinges on re-meshing techniques nor deals with a special treatment of sliding interfaces. While the numerics are certainly more involved the reward is obvious: The modeling costs decrease and the application engineer is provided with an easy-to-use, versatile, and accurate simulation tool.",2016-10-18T08:09:36Z,http://arxiv.org/pdf/1610.05472v2,2024-04-28,
1610.06958v1,"Scale-Dependent Pedotransfer Functions Reliability for Estimating Saturated Hydraulic Conductivity","Saturated hydraulic conductivity Ksat is a fundamental characteristic in modeling flow and contaminant transport in soils and sediments. Therefore, many models have been developed to estimate Ksat from easily measureable parameters, such as textural properties, bulk density, etc. However, Ksat is not only affected by textural and structural characteristics, but also by scale e.g., internal diameter and height. Using the UNSODA database and the contrast pattern aided regression (CPXR) method, we recently developed scale-dependent pedotransfer functions to estimate Ksat from textural data, bulk density, and sample dimensions. The main objectives of this study were evaluating the proposed pedotransfer functions using a larger database, and comparing them with seven other models. For this purpose, we selected more than nineteen thousands soil samples from all around the United States. Results showed that the scale-dependent pedotransfer functions estimated Ksat more accurately than seven other models frequently used in the literature.",2016-10-21T21:29:06Z,http://arxiv.org/pdf/1610.06958v1,2024-04-28,
1610.09902v2,"A Quadratic Manifold for Model Order Reduction of Nonlinear Structural Dynamics","This paper describes the use of a quadratic manifold for the model order reduction of structural dynamics problems featuring geometric nonlinearities. The manifold is tangent to a subspace spanned by the most relevant vibration modes, and its curvature is provided by modal derivatives obtained by sensitivity analysis of the eigenvalue problem, or its static approximation, along the vibration modes. The construction of the quadratic manifold requires minimal computational effort once the vibration modes are known. The reduced order model is then obtained by Galerkin projection, where the configuration-dependent tangent space of the manifold is used to project the discretized equations of motion.",2016-10-31T12:57:21Z,http://arxiv.org/pdf/1610.09902v2,2024-04-28,
1610.09906v1,"Generalization of Quadratic Manifolds for Reduced Order Modeling of Nonlinear Structural Dynamics","In this paper, a generalization of a quadratic manifold approach for the reduction of geometrically nonlinear structural dynamics problems is presented. This generalization is constructed by a linearization of the static force with respect to the generalized coordinates, resulting in a shift of the quadratic behavior from the force to the manifold. In this framework, static derivatives emerge as natural extensions to modal derivatives for displacement fields other than the vibration modes, such as the Krylov subspace vectors. Here the dynamic problem is projected onto the tangent space of the quadratic manifold, allowing for a much less number of generalized coordinates compared to linear basis methods. The potential of the quadratic manifold approach is investigated in a numerical study, where several variations of the approach are compared on different examples, indicating a clear pattern where the proposed approach is applicable.",2016-10-31T13:13:52Z,http://arxiv.org/pdf/1610.09906v1,2024-04-28,
1610.10057v1,"Hydropower optimization: an industrial approach","Nowadays hydroelectric energy is one of the best energy sources: it is cleaner, safer and more programmable than other sources. For this reason, its manage could not be done in an approssimative way, but advance mathematical models must be use. In this article we consider an overview of the problem: we introduce the problem, then we show its simplest but quite exaustive mathematical formulation and in the end we produce numerical results under the ipothesis that all input are deterministic.",2016-10-31T18:34:36Z,http://arxiv.org/pdf/1610.10057v1,2024-04-28,
1611.00531v1,"Modal Analysis of Masonry Structures","This paper presents a new numerical procedure for evaluating the vibration frequencies and mode shapes of masonry buildings in the presence of cracks. The algorithm has been implemented within the NOSA-ITACA code, which models masonry as a nonlinear elastic material with zero tensile strength. Some case studies are reported, and the differences between linear and nonlinear behavior highlighted.",2016-11-02T10:18:13Z,http://arxiv.org/pdf/1611.00531v1,2024-04-28,
1611.00616v2,"Dual Quaternion Variational Integrator for Rigid Body Dynamic Simulation","We introduce a symplectic dual quaternion variational integrator(DQVI) for simulating single rigid body motion in all six degrees of freedom. Dual quaternion is used to represent rigid body kinematics and one-step Lie group variational integrator is used to conserve the geometric structure, energy and momentum of the system during the simulation. The combination of these two becomes the first Lie group variational integrator for rigid body simulation without decoupling translations and rotations. Newton-Raphson method is used to solve the recursive dynamic equation. This method is suitable for real-time rigid body simulations with high precision under large time step. DQVI respects the symplectic structure of the system with excellent long-term conservation of geometry structure, momentum and energy. It also allows the reference point and 6-by-6 inertia matrix to be arbitrarily defined, which is very convenient for a variety of engineering problems.",2016-11-02T14:02:16Z,http://arxiv.org/pdf/1611.00616v2,2024-04-28,
1611.06271v1,"A Novel Single-Source Surface Integral Method to Compute Scattering from Dielectric Objects","Using the traditional surface integral methods, the computation of scattering from a dielectric object requires two equivalent current densities on the boundary of the dielectric. In this paper, we present an approach that requires only a single current density. Our method is based on a surface admittance operator and is applicable to dielectric bodies of arbitrary shape. The formulation results in four times lower memory consumption and up to eight times lower time to solve the linear system than the traditional PMCHWT formulation. Numerical results demonstrate that the proposed technique is as accurate as the PMCHWT formulation.",2016-11-18T23:02:35Z,http://arxiv.org/pdf/1611.06271v1,2024-04-28,
1611.06396v1,"Studying the influence of inclusion characteristics on the characteristic length involved in quasi-brittle materials using the lattice element method","Unlike nonlocal models, there is no need to introduce an internal length in the constitutive law for lattice model at the mesoscopic scale. Actually, the internal length is not explicitly introduced but rather governed by the mesostructure characteristics themselves. The influence of the mesostructure on the width of the fracture process zone which is assumed to be correlated to the characteristic length of the homogenized quasi-brittle material is studied. The influence of the ligament size (a structural parameter) is also investigated. This analysis provides recommendations/warnings when extracting an internal length required for nonlocal damage models from the material mesostructure",2016-11-19T16:17:12Z,http://arxiv.org/pdf/1611.06396v1,2024-04-28,
1611.06868v1,"High-Frequency Modeling and Simulation of a Single-Phase Three-Winding Transformer Including Taps in Regulating Winding","Transformer terminal equivalents obtained via admittance measurements are suitable for simulating high-frequency transient interaction between the transformer and the network. This paper augments the terminal equivalent approach with a measurement-based voltage transfer function model which permits calculation of voltages at internal points in the regulating winding. The approach is demonstrated for a single-phase three-winding transformer in tap position Nom+ with inclusion of three internal points in the regulating winding that represent the mid-point and the two extreme ends. The terminal equivalent modeling makes use of additional common-mode measurements to avoid error magnifications to result from the ungrounded tertiary winding. The final model is used in a time domain simulation where ground-fault initiation results in a resonant voltage build-up in the winding. It is shown that that the peak value of the resonant overvoltage can be higher than during the lightning impulse test, with unfavorable network conditions. Additional measurements show that the selected tap position affects the terminal behavior of the transformer, changing the frequency and peak value of the lower resonance point in the voltage transfer between windings.",2016-11-17T14:27:39Z,http://arxiv.org/pdf/1611.06868v1,2024-04-28,
1611.07368v1,"Discretization of Maxwell's Equations for Non-inertial Observers Using Space-Time Algebra","We employ Maxwell's equations formulated in Space-Time Algebra to perform discretization of moving geometries directly in space-time. All the derivations are carried out without any non-relativistic assumptions, thus the application area of the scheme is not restricted to low velocities. The 4D mesh construction is based on a 3D mesh stemming from a conventional 3D mesh generator. The movement of the system is encoded in the 4D mesh geometry, enabling an easy extension of well-known 3D approaches to the space-time setting. As a research example, we study a manifestation of Sagnac's effect in a rotating ring resonator. In case of constant rotation, the space-time approach enhances the efficiency of the scheme, as the material matrices are constant for every time step, without abandoning the relativistic framework.",2016-11-21T13:11:29Z,http://arxiv.org/pdf/1611.07368v1,2024-04-28,
1611.08266v1,"Researches on Dynamic Load Balancing Algorithms and hp-Adaptivity in 3-D Parallel Adaptive Finite Element Computations","This work is related to PHG (Parallel Hierarchical Grid). PHG is a toolbox for developing parallel adaptive finite element programs, which is under active development at the State Key Laboratory of Scientific and Engineering Computing. The main results of this work are as follows.   1) For the tetrahedral meshes used in PHG, under reasonable assumptions, we proved the existence of through-vertex Hamiltonian paths between arbitrary two vertices, as well as the existence of through-vertex Hamiltonian cycles, and designed an efficient algorithm with linear complexity for constructing through-vertex Hamiltonian paths. The resulting algorithm has been implemented in PHG, and is used for ordering elements in the coarsest mesh for the refinement tree mesh partitioning algorithm.   2) We designed encoding and decoding algorithms for high dimensional Hilbert order. Hilbert order has good locality, and it has wide applications in various fields in computer science, such as memory management, database, and dynamic load balancing.   3) We implemented refinement tree and space filling curve based mesh partitioning algorithms in PHG, and designed the dynamic load balancing module of PHG. The refinement tree based partitioning algorithm was originally proposed by Mitchell, the one implemented in PHG was improved in several aspects. The space filling curve based mesh partitioning function in PHG can use either Hilbert or Morton space filling curve.   4) We studied existing hp-adaptive strategies in the literature, and proposed a new strategy. Numerical experiments show that our new strategy achieves exponential convergence, and is superior, in both precision of the solutions and computation time, to the strategy compared. This part of the work also serves to validate the hp-adaptivity module of PHG.",2016-11-24T17:31:41Z,http://arxiv.org/pdf/1611.08266v1,2024-04-28,
1701.00557v2,"Discrete Optimal Global Convergence of an Evolutionary Algorithm for Clusters under the Potential of Lennard Jones","A review of the properties that bond the particles under Lennard Jones Potential allow to states properties and conditions for building evolutive algorithms using the CB lattice with other different lattices. The new lattice is called CB lattice and it is based on small cubes. A set of propositions states convergence and optimal conditions over the CB lattice for an evolutionary algorithm. The evolutionary algorithm is a reload version of previous genetic algorithms based in phenotypes. The novelty using CB lattice, together with the other lattices, and ad-hoc cluster segmentation and enumeration, is to allow the combination of genotype (DNA coding for cluster using their particle's number) and phenotype (geometrical shapes using particle's coordinates in 3D). A parallel version of an evolutionary algorithm for determining the global optimality is depicted. The results presented are from a standalone program for a personal computer of the evolutionary algorithm, which can estimate all putative optimal Lennard Jones Clusters from 13 to 1612 particles. The novelty are the theoretical results for the evolutionary algorithm's efficiency, the strategies with phenotype or genotype, and the classification of the clusters based in an ad-hoc geometric algorithm for segmenting a cluster into its nucleus and layers. Also, the standalone program is not only capable to replicate the optimal Lennard Jones clusters in The Cambridge Cluster Database (CCD), but to find new ones.",2017-01-03T00:37:08Z,http://arxiv.org/pdf/1701.00557v2,2024-04-28,
1701.03636v2,"Comparison of viscoelastic finite element models for laminated glass beams","Laminated glass elements, which consist of stiff elastic glass layers connected with a compliant viscoelastic polymer foil, exhibit geometrically non-linear and time/temperature-sensitive behavior. In computational modeling, the viscoelastic effects are often neglected or a detailed continuum formulation typically based on the volumetric-deviatoric elastic-viscoelastic split is used for the interlayer. Four layerwise beam theories are introduced in this paper, which differ in the non-linear beam formulation at the layer level (von K\'{a}rm\'{a}n/Reissner) and in constitutive assumptions for the interlayer (a viscoelastic solid with the time-independent bulk modulus/Poisson ratio). We perform detailed verification and validation studies at different temperatures and compare the accuracy of the selected formulation with simplified elastic solutions used in practice. We show that all the four formulations predict very similar responses. Therefore, our suggestion is to use the most straightforward formulation that combines the von K\'{a}rm\'{a}n model with the assumption of time-independent Poisson ratio. The simplified elastic model mostly provides a response in satisfactory agreement with full viscoelastic solutions. However, it can lead to unsafe or inaccurate predictions for rapid changes of loading. These findings provide a suitable basis for extensions towards laminated plates and glass layer fracture, owing to the modular format of layerwise theories.",2017-01-13T11:53:56Z,http://arxiv.org/pdf/1701.03636v2,2024-04-28,
1701.06182v1,"Pseudospectral methods for density functional theory in bounded and unbounded domains","Classical Density Functional Theory (DFT) is a statistical-mechanical framework to analyze fluids, which accounts for nanoscale fluid inhomogeneities and non-local intermolecular interactions. DFT can be applied to a wide range of interfacial phenomena, as well as problems in adsorption, colloidal science and phase transitions in fluids. Typical DFT equations are highly non-linear, stiff and contain several convolution terms. We propose a novel, efficient pseudo-spectral collocation scheme for computing the non-local terms in real space with the help of a specialized Gauss quadrature. Due to the exponential accuracy of the quadrature and a convenient choice of collocation points near interfaces, we can use grids with a significantly lower number of nodes than most other reported methods. We demonstrate the capabilities of our numerical methodology by studying equilibrium and dynamic two-dimensional test cases with single- and multispecies hard-sphere and hard-disc particles modelled with fundamental measure theory, with and without van der Waals attractive forces, in bounded and unbounded physical domains. We show that our results satisfy statistical mechanical sum rules.",2017-01-22T16:23:38Z,http://arxiv.org/pdf/1701.06182v1,2024-04-28,
1701.06254v1,"A Parallel Simulator for Massive Reservoir Models Utilizing Distributed-Memory Parallel Systems","This paper presents our work on developing parallel computational methods for two-phase flow on modern parallel computers, where techniques for linear solvers and nonlinear methods are studied and the standard and inexact Newton methods are investigated. A multi-stage preconditioner for two-phase flow is applied and advanced matrix processing strategies are studied. A local reordering method is developed to speed the solution of linear systems. Numerical experiments show that these computational methods are effective and scalable, and are capable of computing large-scale reservoir simulation problems using thousands of CPU cores on parallel computers. The nonlinear techniques, preconditioner and matrix processing strategies can also be applied to three-phase black oil, compositional and thermal models.",2017-01-23T02:35:21Z,http://arxiv.org/pdf/1701.06254v1,2024-04-28,
1701.06432v2,"On profile reconstruction of Euler-Bernoulli beams by means of an energy based genetic algorithm","This paper studies the inverse problem related to the identification of the flexural stiffness of an Euler Bernoulli beam in order to reconstruct its profile starting from available response data. The proposed identification procedure makes use of energy measurements and is based on the application of a closed form solution for the static displacements of multi-stepped beams. This solution allows to easily calculate the energy related to beams modeled with arbitrary multi-step shapes subjected to a transversal roving force, and to compare it with the correspondent data obtained through direct measurements on real beams. The optimal solution which minimizes the difference between measured and calculated data is then sought by means of genetic algorithms. In the paper several different stepped beams are investigated showing that the proposed procedure allows in many cases to identify the exact beam profile. However it is shown that in some other cases different multi-step profiles may correspond to very similar static responses, and therefore to comparable minima in the optimization problem, thus complicating the profile identification problem.",2017-01-14T08:39:00Z,http://arxiv.org/pdf/1701.06432v2,2024-04-28,
1701.09131v1,"On the choice of homogenization method to achieve effective mechanical properties of composites reinforced by ellipsoidal and spherical particles","In this paper, several rigorous numerical simulations were conducted to examine the relevance of mean-field micromechanical models compared to the Fast Fourier Transform full-field computation by considering spherical or ellipsoidal inclusions. To be more general, the numerical study was extended to a mixture of different kind of microstructures consisting of spheroidal shapes within the same RVE. Although the Fast Fourier Transform full field calculation is sensitive to high contrasts, calculation time, for a combination of complex microstructures, remains reasonable compared with those obtained with mean-field micromechanical models. Moreover, for low volume fractions of inclusions, the results of the mean-field approximations and those of the Fast Fourier Transform-based (FFTb) full-field computation are very close, whatever the inclusions morphology is. For RVEs consisting of ellipsoidal or a mixture of ellipsoidal and spherical inclusions, when the inclusions volume fraction becomes higher, one observes that Lielens' model and the FFTb full-field computation give similar estimates. The accuracy of the computational methods depends on the shape of the inclusions' and their volume fraction.",2017-01-31T17:06:29Z,http://arxiv.org/pdf/1701.09131v1,2024-04-28,
1702.00394v2,"The exponentiated Hencky energy: Anisotropic extension and case studies","In this paper we propose an anisotropic extension of the isotropic exponentiated Hencky energy, based on logarithmic strain invariants. Unlike other elastic formulations, the isotropic exponentiated Hencky elastic energy has been derived solely on differential geometric grounds, involving the geodesic distance of the deformation gradient F to the group of rotations. We formally extend this approach towards anisotropy by defining additional anisotropic logarithmic strain invariants with the help of suitable structural tensors and consider our findings for selected case studies.",2017-02-01T18:57:38Z,http://arxiv.org/pdf/1702.00394v2,2024-04-28,
1702.00695v1,"Adaptive Multiscale Homogenization of the Lattice Discrete Particle Model for the Analysis of Damage and Fracture in Concrete","This paper presents a new adaptive multiscale homogenization scheme for the simulation of damage and fracture in concrete structures. A two-scale homogenization method, coupling meso-scale discrete particle models to macro- scale finite element models, is formulated into an adaptive framework. A continuum multiaxial failure criterion for concrete is calibrated on the basis of fine-scale simulations, and it serves as the adaptive criterion in the multiscale framework. Thus, in this approach, simulations start without assigning any material Representative Volume Element (RVE) to the macro-scale finite elements. The finite elements that meet the adaptive criterion and must be entered into the multiscale homogenization framework are detected on the fly. This leads to a substantial reduction of the computational cost especially for loading conditions leading to damage localization in which only a small portion of the FE mesh is enriched with the homogenized RVE. Several numerical simulations are carried out to investigate the capability of the developed adaptive homogenization method. In addition, a detailed study on the computational cost is performed.",2017-01-31T21:59:47Z,http://arxiv.org/pdf/1702.00695v1,2024-04-28,
1702.01464v2,"Online Voltage Stability Assessment for Load Areas Based on the Holomorphic Embedding Method","This paper proposes an online steady-state voltage stability assessment scheme to evaluate the proximity to voltage collapse at each bus of a load area. Using a non-iterative holomorphic embedding method (HEM) with a proposed physical germ solution, an accurate loading limit at each load bus can be calculated based on online state estimation on the entire load area and a measurement-based equivalent for the external system. The HEM employs a power series to calculate an accurate Power-Voltage (P-V) curve at each load bus and accordingly evaluates the voltage stability margin considering load variations in the next period. An adaptive two-stage Pade approximants method is proposed to improve the convergence of the power series for accurate determination of the nose point on the P-V curve with moderate computational burden. The proposed method is illustrated in detail on a 4-bus test system and then demonstrated on a load area of the Northeast Power Coordinating Council (NPCC) 48-geneartor, 140-bus power system.",2017-02-05T23:01:24Z,http://arxiv.org/pdf/1702.01464v2,2024-04-28,
1702.02046v1,"TensorBeat: Tensor Decomposition for Monitoring Multi-Person Breathing Beats with Commodity WiFi","Breathing signal monitoring can provide important clues for human's physical health problems. Comparing to existing techniques that require wearable devices and special equipment, a more desirable approach is to provide contact-free and long-term breathing rate monitoring by exploiting wireless signals. In this paper, we propose TensorBeat, a system to employ channel state information (CSI) phase difference data to intelligently estimate breathing rates for multiple persons with commodity WiFi devices. The main idea is to leverage the tensor decomposition technique to handle the CSI phase difference data. The proposed TensorBeat scheme first obtains CSI phase difference data between pairs of antennas at the WiFi receiver to create CSI tensor data. Then Canonical Polyadic (CP) decomposition is applied to obtain the desired breathing signals. A stable signal matching algorithm is developed to find the decomposed signal pairs, and a peak detection method is applied to estimate the breathing rates for multiple persons. Our experimental study shows that TensorBeat can achieve high accuracy under different environments for multi-person breathing rate monitoring.",2017-02-06T06:58:22Z,http://arxiv.org/pdf/1702.02046v1,2024-04-28,
1702.05075v2,"High Accuracy Mantle Convection Simulation through Modern Numerical Methods. II: Realistic Models and Problems","Computations have helped elucidate the dynamics of Earth's mantle for several decades already. The numerical methods that underlie these simulations have greatly evolved within this time span, and today include dynamically changing and adaptively refined meshes, sophisticated and efficient solvers, and parallelization to large clusters of computers. At the same time, many of these methods -- discussed in detail in a previous paper in this series -- were developed and tested primarily using model problems that lack many of the complexities that are common to the realistic models our community wants to solve today.   With several years of experience solving complex and realistic models, we here revisit some of the algorithm designs of the earlier paper and discuss the incorporation of more complex physics. In particular, we re-consider time stepping and mesh refinement algorithms, evaluate approaches to incorporate compressibility, and discuss dealing with strongly varying material coefficients, latent heat, and how to track chemical compositions and heterogeneities. Taken together and implemented in a high-performance, massively parallel code, the techniques discussed in this paper then allow for high resolution, 3d, compressible, global mantle convection simulations with phase transitions, strongly temperature dependent viscosity and realistic material properties based on mineral physics data.",2017-02-16T18:29:16Z,http://arxiv.org/pdf/1702.05075v2,2024-04-28,
1702.07284v2,"Efficient Simulation of Temperature Evolution of Overhead Transmission Lines Based on Analytical Solution and NWP","Transmission lines are vital components in power systems. Tripping of transmission lines caused by over-temperature is a major threat to the security of system operations, so it is necessary to efficiently simulate line temperature under both normal operation conditions and foreseen fault conditions. Existing methods based on thermal-steady-state analyses cannot reflect transient temperature evolution, and thus cannot provide timing information needed for taking remedial actions. Moreover, conventional numerical method requires huge computational efforts and barricades system-wide analysis. In this regard, this paper derives an approximate analytical solution of transmission-line temperature evolution enabling efficient analysis on multiple operation states. Considering the uncertainties in environmental parameters, the region of over-temperature is constructed in the environmental parameter space to realize the over-temperature risk assessment in both the planning stage and real-time operations. A test on a typical conductor model verifies the accuracy of the approximate analytical solution. Based on the analytical solution and numerical weather prediction (NWP) data, an efficient simulation method for temperature evolution of transmission systems under multiple operation states is proposed. As demonstrated on an NPCC 140-bus system, it achieves over 1000 times of efficiency enhancement, verifying its potentials in online risk assessment and decision support.",2017-02-23T16:45:31Z,http://arxiv.org/pdf/1702.07284v2,2024-04-28,
1702.07320v1,"Simulation of detecting contact nonlinearity in carbon fibre polymer using ultrasonic nonlinear delayed time reversal","A finite element method simulation of a carbon fibre reinforced polymer block is used to analyse the nonlinearities arising from a contacting delamination gap inside the material. The ultrasonic signal is amplified and nonlinearities are analysed by delayed Time Reversal -- Nonlinear Elastic Wave Spectroscopy signal processing method. This signal processing method allows to focus the wave energy onto the receiving transducer and to modify the focused wave shape, allowing to use several different methods, including pulse inversion, for detecting the nonlinear signature of the damage. It is found that the small crack with contacting acoustic nonlinearity produces a noticeable nonlinear signature when using pulse inversion signal processing, and even higher signature with delayed time reversal, without requiring any baseline information from an undamaged medium.",2017-02-21T15:09:00Z,http://arxiv.org/pdf/1702.07320v1,2024-04-28,
1702.07779v1,"A Stochastic Operator Approach to Model Inadequacy with Applications to Contaminant Transport","The mathematical models used to represent physical phenomena are generally known to be imperfect representations of reality. Model inadequacies arise for numerous reasons, such as incomplete knowledge of the phenomena or computational intractability of more accurate models. In such situations it is impractical or impossible to improve the model, but necessity requires its use to make predictions. With this in mind, it is important to represent the uncertainty that a model's inadequacy causes in its predictions, as neglecting to do so can cause overconfidence in its accuracy. A powerful approach to addressing model inadequacy leverages the composite nature of physical models by enriching a flawed embedded closure model with a stochastic error representation. This work outlines steps in the development of a stochastic operator as an inadequacy representation by establishing the framework for inferring an infinite-dimensional operator and by introducing a novel method for interrogating available high-fidelity models to learn about modeling error.",2017-02-24T21:57:02Z,http://arxiv.org/pdf/1702.07779v1,2024-04-28,
1710.00882v1,"The Tersoff many-body potential: Sustainable performance through vectorization","Molecular dynamics models materials by simulating each individual particle's trajectory. Many-body potentials lead to a more accurate trajectory simulation, and are used in materials science and computational chemistry. We present optimization results for one multi-body potential on a range of vector instruction sets, targeting both CPUs and accelerators like the Intel Xeon Phi. Parallelization of MD simulations is well-studied; by contrast, vectorization is relatively unexplored. Given the prevalence and power of modern vector units, exploiting them is imperative for high performance software. When running on a highly parallel machine, any improvement to the scalar performance is paid back in hundreds or thousands of saved core hours. Vectorization is already commonly used in the optimization or pair potentials; multi-body potentials pose new, unique challenges. Indeed, their optimization pushes the boundaries of current compilers, forcing us to use explicit vectorization techniques for now. In this study, we add an optimized implementation of Tersoff potential to the LAMMPS molecular dynamics simulation package. To reduce the burden of explicit vectorization, we abstract from the specific vector instruction set and desired precision: From one algorithm, we get optimized implementations for many platforms, from SSE4.2 to AVX512, and the Intel Xeon Phi. We compare the kernels across different architectures, and determine suitable architecture-dependent parameters. Our optimizations benefit any architecture, but have a disproportionate effect on the Intel Xeon Phi, which beats the CPU (2xE5-2650) after optimization.",2017-10-02T19:45:52Z,http://arxiv.org/pdf/1710.00882v1,2024-04-28,
1710.02620v1,"A scalable variational inequality approach for flow through porous media models with pressure-dependent viscosity","Mathematical models for flow through porous media typically enjoy the so-called maximum principles, which place bounds on the pressure field. It is highly desirable to preserve these bounds on the pressure field in predictive numerical simulations, that is, one needs to satisfy discrete maximum principles (DMP). Unfortunately, many of the existing formulations for flow through porous media models do not satisfy DMP. This paper presents a robust, scalable numerical formulation based on variational inequalities (VI), to model non-linear flows through heterogeneous, anisotropic porous media without violating DMP. VI is an optimization technique that places bounds on the numerical solutions of partial differential equations. To crystallize the ideas, a modification to Darcy equations by taking into account pressure-dependent viscosity will be discretized using the lowest-order Raviart-Thomas (RT0) and Variational Multi-scale (VMS) finite element formulations. It will be shown that these formulations violate DMP, and, in fact, these violations increase with an increase in anisotropy. It will be shown that the proposed VI-based formulation provides a viable route to enforce DMP. Moreover, it will be shown that the proposed formulation is scalable, and can work with any numerical discretization and weak form. Parallel scalability on modern computational platforms will be illustrated through strong-scaling studies, which will prove the efficiency of the proposed formulation in a parallel setting. Algorithmic scalability as the problem size is scaled up will be demonstrated through novel static-scaling studies. The performed static-scaling studies can serve as a guide for users to be able to select an appropriate discretization for a given problem size.",2017-10-07T02:20:32Z,http://arxiv.org/pdf/1710.02620v1,2024-04-28,
1710.05160v2,"Hyper-reduction over nonlinear manifolds for large nonlinear mechanical systems","Common trends in model order reduction of large nonlinear finite-element-discretized systems involve the introduction of a linear mapping into a reduced set of unknowns, followed by Galerkin projection of the governing equations onto a constant reduction basis. Though this reduces the number of unknowns in the system, the computational cost for obtaining the solution could still be high due to the prohibitive computational costs involved in the evaluation of nonlinear terms. Hyper-reduction methods are then seen as a fast way of approximating the nonlinearity in the system of equations. In the finite element context, the energy conserving sampling and weighing (ECSW) method has emerged as a stability and structure-preserving method for hyper-reduction. Classical hyper-reduction techniques, however, are applicable only in the context of linear mappings into the reduction subspace. In this work, we extend the concept of hyper-reduction using ECSW to general nonlinear mappings, while retaining its desirable stability and structure-preserving properties. As a proof of concept, the proposed hyper-reduction technique is demonstrated over models of a flat plate and a realistic wing structure, whose dynamics has been shown to evolve over a nonlinear (quadratic) manifold. An online speed-up of over one thousand times relative to the full system has been obtained for the wing structure using the proposed method, which is higher than its linear counterpart using the ECSW.",2017-10-14T10:23:23Z,http://arxiv.org/pdf/1710.05160v2,2024-04-28,
1710.05781v1,"A Fast Tree Algorithm for Electric Field Calculation in Electrical Discharge Simulations","The simulation of electrical discharges has been attracting a great deal of attention. In such simulations, the electric field computation dominates the computational time. In this paper, we propose a fast tree algorithm that helps to reduce the time complexity from $O(N^2)$ (from using direct summation) to $O(N\log N)$. The implementation details are discussed and the time complexity is analyzed. A rigorous error estimation shows the error of the tree algorithm decays exponentially with the number of truncation terms and can be controlled adaptively. Numerical examples are presented to validate the accuracy and efficiency of the algorithm.",2017-10-16T15:24:12Z,http://arxiv.org/pdf/1710.05781v1,2024-04-28,
1710.05916v2,"Using Neural Networks to Detect Line Outages from PMU Data","We propose an approach based on neural networks and the AC power flow equations to identify single- and double-line outages in a power grid using the information from phasor measurement unit sensors (PMUs) placed on only a subset of the buses. Rather than inferring the outage from the sensor data by inverting the physical model, our approach uses the AC model to simulate sensor responses to all outages of interest under multiple demand and seasonal conditions, and uses the resulting data to train a neural network classifier to recognize and discriminate between different outage events directly from sensor data. After training, real-time deployment of the classifier requires just a few matrix-vector products and simple vector operations. These operations can be executed much more rapidly than inversion of a model based on AC power flow, which consists of nonlinear equations and possibly integer / binary variables representing line outages, as well as the variables representing voltages and power flows. We are motivated to use neural network by its successful application to such areas as computer vision and natural language processing. Neural networks automatically find nonlinear transformations of the raw data that highlight useful features that make the classification task easier. We describe a principled way to choose sensor locations and show that accurate classification of line outages can be achieved from a restricted set of measurements, even over a wide range of demand profiles.",2017-10-16T07:10:57Z,http://arxiv.org/pdf/1710.05916v2,2024-04-28,
1710.08180v1,"POD-based reduced-order model of an eddy-current levitation problem","The accurate and efficient treatment of eddy-current problems with movement is still a challenge. Very few works applying reduced-order models are available in the literature. In this paper, we propose a proper-orthogonal-decomposition reduced-order model to handle these kind of motional problems. A classical magnetodynamic finite element formulation based on the magnetic vector potential is used as reference and to build up the reduced models. Two approaches are proposed. The TEAM workshop problem 28 is chosen as a test case for validation. Results are compared in terms of accuracy and computational cost.",2017-10-23T10:14:53Z,http://arxiv.org/pdf/1710.08180v1,2024-04-28,
1710.10940v1,"Combining Neural Networks and Signed Particles to Simulate Quantum Systems More Efficiently","Recently a new formulation of quantum mechanics has been suggested which describes systems by means of ensembles of classical particles provided with a sign. This novel approach mainly consists of two steps: the computation of the Wigner kernel, a multi-dimensional function describing the effects of the potential over the system, and the field-less evolution of the particles which eventually create new signed particles in the process. Although this method has proved to be extremely advantageous in terms of computational resources - as a matter of fact it is able to simulate in a time-dependent fashion many- body systems on relatively small machines - the Wigner kernel can represent the bottleneck of simulations of certain systems. Moreover, storing the kernel can be another issue as the amount of memory needed is cursed by the dimensionality of the system. In this work, we introduce a new technique which drastically reduces the computation time and memory requirement to simulate time-dependent quantum systems which is based on the use of an appropriately tailored neural network combined with the signed particle formalism. In particular, the suggested neural network is able to compute efficiently and reliably the Wigner kernel without any training as its entire set of weights and biases is specified by analytical formulas. As a consequence, the amount of memory for quantum simulations radically drops since the kernel does not need to be stored anymore as it is now computed by the neural network itself, only on the cells of the (discretized) phase-space which are occupied by particles. As its is clearly shown in the final part of this paper, not only this novel approach drastically reduces the computational time, it also remains accurate. The author believes this work opens the way towards effective design of quantum devices, with incredible practical implications.",2017-10-19T18:52:07Z,http://arxiv.org/pdf/1710.10940v1,2024-04-28,
1711.00110v1,"Improved Algorithm for Reconstructing Singular Connection in Multi-Block CFD Applications","An improved algorithm is proposed for the reconstruction of singular connectivity from the available pairwise connections during preprocessing phase. To evaluate the performance of the algorithm, an in-house computational fluid dynamics (CFD) code is used in which high-order finite-difference method for spatial discretization running on the Tianhe-1A supercomputer is employed. Test cases with a varied amount of mesh points are chosen, and the test results indicate that the improved singular connection reconstruction algorithm can achieve a speedup factor of 1000X or more when compared with the naive search method adopted in the former version of our code. Moreover, the parallel efficiency can benefit from the strategy of local communication based on the algorithm.",2017-10-29T15:07:08Z,http://arxiv.org/pdf/1711.00110v1,2024-04-28,
1711.02661v1,"e-Fair: Aggregation in e-Commerce for Exploiting Economies of Scale","In recent years, many new and interesting models of successful online business have been developed, including competitive models such as auctions, where the product price tends to rise, and group-buying, where users cooperate obtaining a dynamic price that tends to go down. We propose the e-fair as a business model for social commerce, where both sellers and buyers are grouped to maximize benefits. e-Fairs extend the group-buying model aggregating demand and supply for price optimization as well as consolidating shipments and optimize withdrawals for guaranteeing additional savings. e-Fairs work upon multiple dimensions: time to aggregate buyers, their geographical distribution, price/quantity curves provided by sellers, and location of withdrawal points. We provide an analytical model for time and spatial optimization and simulate realistic scenarios using both real purchase data from an Italian marketplace and simulated ones. Experimental results demonstrate the potentials offered by e-fairs and show benefits for all the involved actors.",2017-11-07T18:58:50Z,http://arxiv.org/pdf/1711.02661v1,2024-04-28,
1711.03589v1,"Approaches to Stochastic Modeling of Wind Turbines","Background. This paper study statistical data gathered from wind turbines located on the territory of the Republic of Poland. The research is aimed to construct the stochastic model that predicts the change of wind speed with time. Purpose. The purpose of this work is to find the optimal distribution for the approximation of available statistical data on wind speed. Methods. We consider four distributions of a random variable: Log-Normal, Weibull, Gamma and Beta. In order to evaluate the parameters of distributions we use method of maximum likelihood. To assess the the results of approximation we use a quantile-quantile plot. Results. All the considered distributions properly approximate the available data. The Weibull distribution shows the best results for the extreme values of the wind speed. Conclusions. The results of the analysis are consistent with the common practice of using the Weibull distribution for wind speed modeling. In the future we plan to compare the results obtained with a much larger data set as well as to build a stochastic model of the evolution of the wind speed depending on time.",2017-11-08T06:38:50Z,http://arxiv.org/pdf/1711.03589v1,2024-04-28,
1711.09307v1,"Spectral Element Methods for Liquid Metal Reactors Applications","Funded by the U.S. Department of Energy, the Nuclear Energy Advanced Modeling and Simulation (NEAMS) program aims to develop an integrated multiphysics simulation capability for the design and analysis of future generations of nuclear power plants. NEAMS embraces a multiresolution hierarchy designing the code suite structure to ultimately span the full range of length and time scales present in relevant reactor design and safety analyses. Advanced reactors, such as liquid metal reactors, rely on innovative component designs to meet cost and safety targets. In order to span a wider design range, advanced modeling and simulation capabilities that rely on minimal assumptions play an important role in optimizing the design. Over the past several years the NEAMS program has developed the integrated multiphysics code suite (thermal-hydraulics, structural analysis and neutronics) SHARP aimed at streamlining the prototyping of such components. For the simulation of fluid flow and heat transfer, SHARP focuses on the high-fidelity end, aiming primarily at turbulence-resolving techniques such large eddy simulation (LES) and direct numerical simulation (DNS). The computational fluid dynamics code (CFD) selected for SHARP is Nek5000, a state-of-the-art highly scalable tool employing the spectral element method (SEM). In this manuscript, to be published in a Von karman institute lecture series monograph on liquid metal reactors, we review the method and its implementation in Nek5000. We also examine several applications. We note that Nek5000 is also regularly employed for intermediate-fidelity approaches such as Reynolds-averaged Navier-Stokes (RANS) and for reduced-order models employing momentum sources or porous media, especially when coupled to neutronics modeling.",2017-11-25T23:29:43Z,http://arxiv.org/pdf/1711.09307v1,2024-04-28,
1711.10782v1,"Computational Aided Design for Generating a Modular, Lightweight Car Concept","Developing an appropriate design process for a conceptual model is a stepping stone toward designing car bodies. This paper presents a methodology to design a lightweight and modular space frame chassis for a sedan electric car. The dual phase high strength steel with improved mechanical properties is employed to reduce the weight of the car body. Utilizing the finite element analysis yields two models in order to predict the performance of each component. The first model is a beam structure with a rapid response in structural stiffness simulation. This model is used for performing the static tests including modal frequency, bending stiffens and torsional stiffness evaluation. Whereas the second model, i.e., a shell model, is proposed to illustrate every module's mechanical behavior as well as its crashworthiness efficiency. In order to perform the crashworthiness analysis, the explicit nonlinear dynamic solver provided by ABAQUS, a commercial finite element software, is used. The results of finite element beam and shell models are in line with the concept design specifications. Implementation of this procedure leads to generate a lightweight and modular concept for an electric car.",2017-11-29T11:30:50Z,http://arxiv.org/pdf/1711.10782v1,2024-04-28,
1711.11230v7,"Route-cost-assignment with joint user and operator behavior as a many-to-one stable matching assignment game","We propose a generalized market equilibrium model using assignment game criteria for evaluating transportation systems that consist of both operators' and users' decisions. The model finds stable pricing, in terms of generalized costs, and matches between user populations in a network to set of routes with line capacities. The proposed model gives a set of stable outcomes instead of single point pricing that allows operators to design ticket pricing, routes/schedules that impact access/egress, shared policies that impact wait/transfer costs, etc., based on a desired mechanism or policy. The set of stable outcomes is proven to be convex from which assignment-dependent unique user-optimal and operator-optimal outcomes can be obtained. Different user groups can benefit from using this model in a prescriptive manner or within a sequential design process. We look at several different examples to test our model: small examples of fixed transit routes and a case study using a small subset of taxi data in NYC. The case study illustrates how one can use the model to evaluate a policy that can require passengers to walk up to 1 block away to meet with a shared taxi without turning away passengers.",2017-11-30T05:06:51Z,http://arxiv.org/pdf/1711.11230v7,2024-04-28,
1711.11519v2,"A Deep Learning Framework for Short-term Power Load Forecasting","The scheduling and operation of power system becomes prominently complex and uncertain, especially with the penetration of distributed power. Load forecasting matters to the effective operation of power system. This paper proposes a novel deep learning framework to forecast the short-term grid load. First, the load data is processed by Box-Cox transformation, and two parameters (electricity price and temperature) are investigated. Then, to quantify the tail-dependence of power load on the two parameters, parametric Copula models are fitted and the threshold of peak load are computed. Next, a deep belief network is built to forecast the hourly load of the power grid. One year grid load data collected from an urbanized area in Texas, United States is utilized in the case studies. Short-term load forecasting are examined in four seasons independently. Day-ahead and week-ahead load forecasting experiments are conducted in each season using the proposed framework. The proposed framework is compared with classical neural networks, support vector regression machine, extreme learning machine, and classical deep belief networks. The load forecasting performances are assessed by mean absolute percentage error, root mean square error, and hit rate. Computational results confirm the effectiveness of the proposed data-driven deep learning framework. The prediction accuracies of both day-ahead forecasting and week-ahead forecasting demonstrate that the proposed framework outperforms the tested algorithms.",2017-11-30T17:12:54Z,http://arxiv.org/pdf/1711.11519v2,2024-04-28,
1311.0438v2,"Modeling Vanilla Option prices: A simulation study by an implicit method","Option contracts can be valued by using the Black-Scholes equation, a partial differential equation with initial conditions. An exact solution for European style options is known. The computation time and the error need to be minimized simultaneously. In this paper, the authors have solved the Black-Scholes equation by employing a reasonably accurate implicit method. Options with known analytic solutions have been evaluated. Furthermore, an overall second order accurate space and time discretization is proposed in this paper Keywords: Computational finance, implicit methods, finite differences, call/put options.",2013-11-03T08:26:38Z,http://arxiv.org/pdf/1311.0438v2,2024-04-28,
1311.0534v1,"Accurate curve fits of IAPWS data for high-pressure, high-temperature single-phase liquid water based on the stiffened gas equation of state","We present a series of optimal (in the sense of least-squares) curve fits for the stiffened gas equation of state for single-phase liquid water. At high pressures and (subcritical) temperatures, the parameters produced by these curve fits are found to have very small relative errors: less than $1\%$ in the pressure model, and less than $2\%$ in the temperature model. At low pressures and temperatures, especially near the liquid-vapor transition line, the error in the curve fits increases rapidly. The smallest pressure value for which curve fits are reported in the present work is 25 MPa, high enough to ensure that the fluid remains a single-phase liquid up to the maximum subcritical temperature of approximately 647K.",2013-11-03T21:42:26Z,http://arxiv.org/pdf/1311.0534v1,2024-04-28,
1311.0790v2,"A Discontinuous Galerkin Time Domain Framework for Periodic Structures Subject To Oblique Excitation","A nodal Discontinuous Galerkin (DG) method is derived for the analysis of time-domain (TD) scattering from doubly periodic PEC/dielectric structures under oblique interrogation. Field transformations are employed to elaborate a formalism that is free from any issues with causality that are common when applying spatial periodic boundary conditions simultaneously with incident fields at arbitrary angles of incidence. An upwind numerical flux is derived for the transformed variables, which retains the same form as it does in the original Maxwell problem for domains without explicitly imposed periodicity. This, in conjunction with the amenability of the DG framework to non-conformal meshes, provides a natural means of accurately solving the first order TD Maxwell equations for a number of periodic systems of engineering interest. Results are presented that substantiate the accuracy and utility of our method.",2013-11-04T17:48:27Z,http://arxiv.org/pdf/1311.0790v2,2024-04-28,
1311.2167v2,"SPH Entropy Errors and the Pressure Blip","The spurious pressure jump at a contact discontinuity, in SPH simulations of the compressible Euler equations is investigated. From the spatiotemporal behaviour of the error, the SPH pressure jump is likened to entropy errors observed for artificial viscosity based finite difference/volume schemes. The error is observed to be generated at start-up and dissipation is the only recourse to mitigate it's effect. We show that similar errors are generated for the Lagrangian plus remap version of the Piecewise Parabolic Method (PPM) finite volume code (PPMLR). Through a comparison with the direct Eulerian version of the PPM code (PPMDE), we argue that a lack of diffusion across the material wave (contact discontinuity) is responsible for the error in PPMLR. We verify this hypothesis by constructing a more dissipative version of the remap code using a piecewise constant reconstruction. As an application to SPH, we propose a hybrid GSPH scheme that adds the requisite dissipation by utilizing a more dissipative Riemann solver for the energy equation. The proposed modification to the GSPH scheme, and it's improved treatment of the anomaly is verified for flows with strong shocks in one and two dimensions. The result that dissipation must act across the density and energy equations provides a consistent explanation for many of the hitherto proposed ""cures"" or ""fixes"" for the problem.",2013-11-09T12:59:38Z,http://arxiv.org/pdf/1311.2167v2,2024-04-28,
1311.3837v1,"SBML for optimizing decision support's tools","Many theoretical works and tools on epidemiological field reflect the emphasis on decision-making Tools by both public health and the scientific community, which continues to increase. Indeed, in the epidemiological field, modeling tools are proving a very important way in helping to make decision. However, the variety, the large volume of data and the nature of epidemics lead us to seek solutions to alleviate the heavy burden imposed on both experts and developers. In this paper, we present a new approach: the passage of an epidemic model realized in Bio-PEPA to a narrative language using the basics of SBML language. Our goal is to allow on one hand, epidemiologists to verify and validate the model, and the other hand, developers to optimize the model in order to achieve a better model of decision making. We also present some preliminary results and some suggestions to improve the simulated model.",2013-11-15T12:58:28Z,http://arxiv.org/pdf/1311.3837v1,2024-04-28,
1311.4570v1,"Numerical modeling of friction stir welding process: a literature review","This survey presents a literature review on friction stir welding (FSW) modeling with a special focus on the heat generation due to the contact conditions between the FSW tool and the workpiece. The physical process is described and the main process parameters that are relevant to its modeling are highlighted. The contact conditions (sliding/sticking) are presented as well as an analytical model that allows estimating the associated heat generation. The modeling of the FSW process requires the knowledge of the heat loss mechanisms, which are discussed mainly considering the more commonly adopted formulations. Different approaches that have been used to investigate the material flow are presented and their advantages/drawbacks are discussed. A reliable FSW process modeling depends on the fine tuning of some process and material parameters. Usually, these parameters are achieved with base on experimental data. The numerical modeling of the FSW process can help to achieve such parameters with less effort and with economic advantages.",2013-11-18T22:10:14Z,http://arxiv.org/pdf/1311.4570v1,2024-04-28,
1311.6215v1,"Using virtual parts to optimize the metrology process","In the measurement process, there are many parameters affecting the measurement results: the influence of the probe system, material stiffness of measured workpiece, the calibration of the probe with a reference sphere, the thermal effects. We want to obtain the limits of a measurement methodology to be able to validate a result. The study is applied to a simple part. We observe the dispersion of the position of different drilled holes (XYZ values in a coordinate system) when we change the quality of the part and the method of calculation. We use the Design of Experiment (Taguchi method) to realize our study. We study the influence of the part quality on a measurement results. We consider two parameters to define the part quality (flatness and perpendicularity). We will also study the influence of different methods of calculation to determine the coordinate system. We can use two options in Metrolog XG software (tangent plane with or without orientation constraint). The originality of this paper is that we present a method for the design of experiment that uses CATIA (CAD system) to generate the measured parts. In this way we can realize a design of experiment with a largest number of experimental results. This is a positive point for a statistical analysis. We are also free to define the parts we want to study without manufacturing difficulties.",2013-11-25T05:57:40Z,http://arxiv.org/pdf/1311.6215v1,2024-04-28,
1311.6460v1,"Wavelet Transform-Based Analysis of QRS complex in ECG Signals","In the present paper we have reported a wavelet based time-frequency multiresolution analysis of an ECG signal. The ECG (electrocardiogram), which records hearts electrical activity, is able to provide with useful information about the type of Cardiac disorders suffered by the patient depending upon the deviations from normal ECG signal pattern. We have plotted the coefficients of continuous wavelet transform using Morlet wavelet. We used different ECG signal available at MIT-BIH database and performed a comparative study. We demonstrated that the coefficient at a particular scale represents the presence of QRS signal very efficiently irrespective of the type or intensity of noise, presence of unusually high amplitude of peaks other than QRS peaks and Base line drift errors. We believe that the current studies can enlighten the path towards development of very lucid and time efficient algorithms for identifying and representing the QRS complexes that can be done with normal computers and processors.",2013-11-25T20:59:34Z,http://arxiv.org/pdf/1311.6460v1,2024-04-28,
1609.00119v1,"Geometrically Exact Finite Element Formulations for Curved Slender Beams: Kirchhoff-Love Theory vs. Simo-Reissner Theory","The present work focuses on geometrically exact finite elements for highly slender beams. It aims at the proposal of novel formulations of Kirchhoff-Love type, a detailed review of existing formulations of Kirchhoff-Love and Simo-Reissner type as well as a careful evaluation and comparison of the proposed and existing formulations. Two different rotation interpolation schemes with strong or weak Kirchhoff constraint enforcement, respectively, as well as two different choices of nodal triad parametrizations in terms of rotation or tangent vectors are proposed. The combination of these schemes leads to four novel finite element variants, all of them based on a C1-continuous Hermite interpolation of the beam centerline. Essential requirements such as representability of general 3D, large-deformation, dynamic problems involving slender beams with arbitrary initial curvatures and anisotropic cross-section shapes or preservation of objectivity and path-independence will be investigated analytically and verified numerically for the different formulations. It will be shown that the geometrically exact Kirchhoff-Love beam elements proposed in this work are the first ones of this type that fulfill all the considered requirements. On the contrary, Simo-Reissner type formulations fulfilling these requirements can be found in the literature very well. However, it will be argued that the shear-free Kirchhoff-Love formulations can provide considerable numerical advantages when applied to highly slender beams. Concretely, several representative numerical test cases confirm that the proposed Kirchhoff-Love formulations exhibit a lower discretization error level as well as a considerably improved nonlinear solver performance in the range of high beam slenderness ratios as compared to two representative Simo-Reissner element formulations from the literature.",2016-09-01T06:11:46Z,http://arxiv.org/pdf/1609.00119v1,2024-04-28,
1609.05502v3,"Inverse Problems with Invariant Multiscale Statistics","We propose a new approach to linear ill-posed inverse problems. Our algorithm alternates between enforcing two constraints: the measurements and the statistical correlation structure in some transformed space. We use a non-linear multiscale scattering transform which discards the phase and thus exposes strong spectral correlations otherwise hidden beneath the phase fluctuations. As a result, both constraints may be put into effect by linear projections in their respective spaces. We apply the algorithm to super-resolution and tomography and show that it outperforms ad hoc convex regularizers and stably recovers the missing spectrum.",2016-09-18T15:44:04Z,http://arxiv.org/pdf/1609.05502v3,2024-04-28,
1609.06153v1,"Uncertainty Analysis of Simple Macroeconomic Models Using Angel-Daemon Games","We propose the use of an angel-daemon framework to perform an uncertainty analysis of short-term macroeconomic models with exogenous components. An uncertainty profile $\mathcal U$ is a short and macroscopic description of a potentially perturbed situation. The angel-daemon framework uses $\mathcal U$ to define a strategic game where two agents, the angel and the daemon, act selfishly having different goals. The Nash equilibria of those games provide the stable strategies in perturbed situations, giving a natural estimation of uncertainty.   In this initial work we apply the framework in order to get an uncertainty analysis of linear versions of the IS-LM and the IS-MP models. In those models, by considering uncertainty profiles, we can capture different economical situations. Some of them can be described in terms of macroeconomic policy coordination. In other cases we just analyse the results of the system under some possible perturbation level. Besides providing examples of application we analyse the structure of the Nash equilibria in some particular cases of interest.",2016-09-19T12:48:52Z,http://arxiv.org/pdf/1609.06153v1,2024-04-28,
1609.06187v2,"Determination of Bond Wire Failure Probabilities in Microelectronic Packages","This work deals with the computation of industry-relevant bond wire failure probabilities in microelectronic packages. Under operating conditions, a package is subject to Joule heating that can lead to electrothermally induced failures. Manufacturing tolerances result, e.g., in uncertain bond wire geometries that often induce very small failure probabilities requiring a high number of Monte Carlo (MC) samples to be computed. Therefore, a hybrid MC sampling scheme that combines the use of an expensive computer model with a cheap surrogate is used. The fraction of surrogate evaluations is maximized using an iterative procedure, yielding accurate results at reduced cost. Moreover, the scheme is non-intrusive, i.e., existing code can be reused. The algorithm is used to compute the failure probability for an example package and the computational savings are assessed by performing a surrogate efficiency study.",2016-09-18T16:50:47Z,http://arxiv.org/pdf/1609.06187v2,2024-04-28,
1609.07114v2,"A Stable FDTD Method with Embedded Reduced-Order Models","The computational efficiency of the Finite-Difference Time-Domain (FDTD) method can be significantly reduced by the presence of complex objects with fine features. Small geometrical details impose a fine mesh and a reduced time step, significantly increasing computational cost. Model order reduction has been proposed as a systematic way to generate compact models for complex objects, that one can then instantiate into a main FDTD mesh. However, the stability of FDTD with embedded reduced models remains an open problem. We propose a systematic method to generate reduced models for FDTD domains, and embed them into a main FDTD mesh with guaranteed stability up to the Courant-Friedrichs-Lewy (CFL) limit of the fine mesh. With a simple perturbation technique, the CFL of the whole scheme can be further extended beyond the fine grid's CFL limit. Reduced models can be created for arbitrary domains containing inhomogeneous and lossy materials. Numerical tests confirm the stability of the proposed method, and its potential to accelerate multiscale FDTD simulations.",2016-09-22T19:19:21Z,http://arxiv.org/pdf/1609.07114v2,2024-04-28,
cs/9808008v1,"Computational Geometry Column 34","Problems presented at the open-problem session of the 14th Annual ACM Symposium on Computational Geometry are listed.",1998-08-31T19:04:14Z,http://arxiv.org/pdf/cs/9808008v1,2024-04-28,
cs/9809038v2,"Incremental and Decremental Maintenance of Planar Width","We present an algorithm for maintaining the width of a planar point set dynamically, as points are inserted or deleted. Our algorithm takes time O(kn^epsilon) per update, where k is the amount of change the update causes in the convex hull, n is the number of points in the set, and epsilon is any arbitrarily small constant. For incremental or decremental update sequences, the amortized time per update is O(n^epsilon).",1998-09-21T23:31:16Z,http://arxiv.org/pdf/cs/9809038v2,2024-04-28,
cs/9809081v1,"Optimal Point Placement for Mesh Smoothing","We study the problem of moving a vertex in an unstructured mesh of triangular, quadrilateral, or tetrahedral elements to optimize the shapes of adjacent elements. We show that many such problems can be solved in linear time using generalized linear programming. We also give efficient algorithms for some mesh smoothing problems that do not fit into the generalized linear programming paradigm.",1998-09-24T21:02:45Z,http://arxiv.org/pdf/cs/9809081v1,2024-04-28,
cs/9809109v1,"Linear Complexity Hexahedral Mesh Generation","We show that any polyhedron forming a topological ball with an even number of quadrilateral sides can be partitioned into O(n) topological cubes, meeting face to face. The result generalizes to non-simply-connected polyhedra satisfying an additional bipartiteness condition. The same techniques can also be used to reduce the geometric version of the hexahedral mesh generation problem to a finite case analysis amenable to machine solution.",1998-09-26T23:44:27Z,http://arxiv.org/pdf/cs/9809109v1,2024-04-28,
cs/9810007v2,"Randomization yields simple O(n log star n) algorithms for difficult Omega(n) problems","We use here the results on the influence graph by Boissonnat et al. to adapt them for particular cases where additional information is available. In some cases, it is possible to improve the expected randomized complexity of algorithms from O(n log n) to O(n log star n).   This technique applies in the following applications: triangulation of a simple polygon, skeleton of a simple polygon, Delaunay triangulation of points knowing the EMST (euclidean minimum spanning tree).",1998-10-07T08:03:32Z,http://arxiv.org/pdf/cs/9810007v2,2024-04-28,
cs/9901013v1,"Analysis of approximate nearest neighbor searching with clustered point sets","We present an empirical analysis of data structures for approximate nearest neighbor searching. We compare the well-known optimized kd-tree splitting method against two alternative splitting methods. The first, called the sliding-midpoint method, which attempts to balance the goals of producing subdivision cells of bounded aspect ratio, while not producing any empty cells. The second, called the minimum-ambiguity method is a query-based approach. In addition to the data points, it is also given a training set of query points for preprocessing. It employs a simple greedy algorithm to select the splitting plane that minimizes the average amount of ambiguity in the choice of the nearest neighbor for the training points. We provide an empirical analysis comparing these two methods against the optimized kd-tree construction for a number of synthetically generated data and query sets. We demonstrate that for clustered data and query sets, these algorithms can provide significant improvements over the standard kd-tree construction for approximate nearest neighbor searching.",1999-01-26T21:23:37Z,http://arxiv.org/pdf/cs/9901013v1,2024-04-28,
cs/9906023v1,"Computational Geometry Column 35","The subquadratic algorithm of Kapoor for finding shortest paths on a polyhedron is described.",1999-06-22T20:40:51Z,http://arxiv.org/pdf/cs/9906023v1,2024-04-28,
cs/9907023v1,"On Deletion in Delaunay Triangulation","This paper presents how the space of spheres and shelling may be used to delete a point from a $d$-dimensional triangulation efficiently. In dimension two, if k is the degree of the deleted vertex, the complexity is O(k log k), but we notice that this number only applies to low cost operations, while time consuming computations are only done a linear number of times.   This algorithm may be viewed as a variation of Heller's algorithm, which is popular in the geographic information system community. Unfortunately, Heller algorithm is false, as explained in this paper.",1999-07-16T13:25:04Z,http://arxiv.org/pdf/cs/9907023v1,2024-04-28,
cs/9907024v1,"Improved Incremental Randomized Delaunay Triangulation","We propose a new data structure to compute the Delaunay triangulation of a set of points in the plane. It combines good worst case complexity, fast behavior on real data, and small memory occupation.   The location structure is organized into several levels. The lowest level just consists of the triangulation, then each level contains the triangulation of a small sample of the levels below. Point location is done by marching in a triangulation to determine the nearest neighbor of the query at that level, then the march restarts from that neighbor at the level below. Using a small sample (3%) allows a small memory occupation; the march and the use of the nearest neighbor to change levels quickly locate the query.",1999-07-16T12:44:07Z,http://arxiv.org/pdf/cs/9907024v1,2024-04-28,
cs/9907025v1,"The union of unit balls has quadratic complexity, even if they all contain the origin","We provide a lower bound construction showing that the union of unit balls in three-dimensional space has quadratic complexity, even if they all contain the origin. This settles a conjecture of Sharir.",1999-07-16T15:20:14Z,http://arxiv.org/pdf/cs/9907025v1,2024-04-28,
cs/9907028v1,"Further Results on Arithmetic Filters for Geometric Predicates","An efficient technique to solve precision problems consists in using exact computations. For geometric predicates, using systematically expensive exact computations can be avoided by the use of filters. The predicate is first evaluated using rounding computations, and an error estimation gives a certificate of the validity of the result. In this note, we studies the statistical efficiency of filters for cosphericity predicate with an assumption of regular distribution of the points. We prove that the expected value of the polynomial corresponding to the in sphere test is greater than epsilon with probability O(epsilon log 1/epsilon) improving the results of a previous paper by the same authors.",1999-07-19T15:09:50Z,http://arxiv.org/pdf/cs/9907028v1,2024-04-28,
cs/9907029v2,"A Probabilistic Analysis of the Power of Arithmetic Filters","The assumption of real-number arithmetic, which is at the basis of conventional geometric algorithms, has been seriously challenged in recent years, since digital computers do not exhibit such capability.   A geometric predicate usually consists of evaluating the sign of some algebraic expression. In most cases, rounded computations yield a reliable result, but sometimes rounded arithmetic introduces errors which may invalidate the algorithms. The rounded arithmetic may produce an incorrect result only if the exact absolute value of the algebraic expression is smaller than some (small) varepsilon, which represents the largest error that may arise in the evaluation of the expression. The threshold varepsilon depends on the structure of the expression and on the adopted computer arithmetic, assuming that the input operands are error-free.   A pair (arithmetic engine,threshold) is an ""arithmetic filter"". In this paper we develop a general technique for assessing the efficacy of an arithmetic filter. The analysis consists of evaluating both the threshold and the probability of failure of the filter.   To exemplify the approach, under the assumption that the input points be chosen randomly in a unit ball or unit cube with uniform density, we analyze the two important predicates ""which-side"" and ""insphere"". We show that the probability that the absolute values of the corresponding determinants be no larger than some positive value V, with emphasis on small V, is Theta(V) for the which-side predicate, while for the insphere predicate it is Theta(V^(2/3)) in dimension 1, O(sqrt(V)) in dimension 2, and O(sqrt(V) ln(1/V)) in higher dimensions. Constants are small, and are given in the paper.",1999-07-19T15:22:19Z,http://arxiv.org/pdf/cs/9907029v2,2024-04-28,
cs/9907030v1,"Algorithms for Coloring Quadtrees","We describe simple linear time algorithms for coloring the squares of balanced and unbalanced quadtrees so that no two adjacent squares are given the same color. If squares sharing sides are defined as adjacent, we color balanced quadtrees with three colors, and unbalanced quadtrees with four colors; these results are both tight, as some quadtrees require this many colors. If squares sharing corners are defined as adjacent, we color balanced or unbalanced quadtrees with six colors; for some quadtrees, at least five colors are required.",1999-07-19T20:55:39Z,http://arxiv.org/pdf/cs/9907030v1,2024-04-28,
cs/9908006v2,"Computational Geometry Column 36","Two results in ""computational origami"" are illustrated.",1999-08-11T17:14:00Z,http://arxiv.org/pdf/cs/9908006v2,2024-04-28,
cs/9908007v1,"Computational Geometry Column 37","Open problems from the 15th Annual ACM Symposium on Computational Geometry.",1999-08-11T17:19:00Z,http://arxiv.org/pdf/cs/9908007v1,2024-04-28,
cs/9908016v1,"Quadrilateral Meshing by Circle Packing","We use circle-packing methods to generate quadrilateral meshes for polygonal domains, with guaranteed bounds both on the quality and the number of elements. We show that these methods can generate meshes of several types: (1) the elements form the cells of a Voronoi diagram, (2) all elements have two opposite right angles, (3) all elements are kites, or (4) all angles are at most 120 degrees. In each case the total number of elements is O(n), where n is the number of input vertices.",1999-08-19T20:40:36Z,http://arxiv.org/pdf/cs/9908016v1,2024-04-28,
cs/9909004v1,"Convex Tours of Bounded Curvature","We consider the motion planning problem for a point constrained to move along a smooth closed convex path of bounded curvature. The workspace of the moving point is bounded by a convex polygon with m vertices, containing an obstacle in a form of a simple polygon with $n$ vertices. We present an O(m+n) time algorithm finding the path, going around the obstacle, whose curvature is the smallest possible.",1999-09-03T15:09:50Z,http://arxiv.org/pdf/cs/9909004v1,2024-04-28,
cs/9909005v1,"Computing largest circles separating two sets of segments","A circle $C$ separates two planar sets if it encloses one of the sets and its open interior disk does not meet the other set. A separating circle is a largest one if it cannot be locally increased while still separating the two given sets. An Theta(n log n) optimal algorithm is proposed to find all largest circles separating two given sets of line segments when line segments are allowed to meet only at their endpoints. In the general case, when line segments may intersect $\Omega(n^2)$ times, our algorithm can be adapted to work in O(n alpha(n) log n) time and O(n \alpha(n)) space, where alpha(n) represents the extremely slowly growing inverse of the Ackermann function.",1999-09-03T15:29:28Z,http://arxiv.org/pdf/cs/9909005v1,2024-04-28,
cs/9909006v1,"Motion Planning of Legged Robots","We study the problem of computing the free space F of a simple legged robot called the spider robot. The body of this robot is a single point and the legs are attached to the body. The robot is subject to two constraints: each leg has a maximal extension R (accessibility constraint) and the body of the robot must lie above the convex hull of its feet (stability constraint). Moreover, the robot can only put its feet on some regions, called the foothold regions. The free space F is the set of positions of the body of the robot such that there exists a set of accessible footholds for which the robot is stable. We present an efficient algorithm that computes F in O(n2 log n) time using O(n2 alpha(n)) space for n discrete point footholds where alpha(n) is an extremely slowly growing function (alpha(n) <= 3 for any practical value of n). We also present an algorithm for computing F when the foothold regions are pairwise disjoint polygons with n edges in total. This algorithm computes F in O(n2 alpha8(n) log n) time using O(n2 alpha8(n)) space (alpha8(n) is also an extremely slowly growing function). These results are close to optimal since Omega(n2) is a lower bound for the size of F.",1999-09-03T16:18:49Z,http://arxiv.org/pdf/cs/9909006v1,2024-04-28,
cs/9909007v1,"Circular Separability of Polygons","Two planar sets are circularly separable if there exists a circle enclosing one of the sets and whose open interior disk does not intersect the other set.   This paper studies two problems related to circular separability. A linear-time algorithm is proposed to decide if two polygons are circularly separable. The algorithm outputs the smallest separating circle. The second problem asks for the largest circle included in a preprocessed, convex polygon, under some point and/or line constraints. The resulting circle must contain the query points and it must lie in the halfplanes delimited by the query lines.",1999-09-03T16:51:23Z,http://arxiv.org/pdf/cs/9909007v1,2024-04-28,
cs/9909017v1,"Finding an ordinary conic and an ordinary hyperplane","Given a finite set of non-collinear points in the plane, there exists a line that passes through exactly two points. Such a line is called an ordinary line. An efficient algorithm for computing such a line was proposed by Mukhopadhyay et al. In this note we extend this result in two directions. We first show how to use this algorithm to compute an ordinary conic, that is, a conic passing through exactly five points, assuming that all the points do not lie on the same conic. Both our proofs of existence and the consequent algorithms are simpler than previous ones. We next show how to compute an ordinary hyperplane in three and higher dimensions.",1999-09-27T11:55:29Z,http://arxiv.org/pdf/cs/9909017v1,2024-04-28,
cs/0009013v2,"Pattern Matching for sets of segments","In this paper we present algorithms for a number of problems in geometric pattern matching where the input consist of a collections of segments in the plane. Our work consists of two main parts. In the first, we address problems and measures that relate to collections of orthogonal line segments in the plane. Such collections arise naturally from problems in mapping buildings and robot exploration.   We propose a new measure of segment similarity called a \emph{coverage measure}, and present efficient algorithms for maximising this measure between sets of axis-parallel segments under translations. Our algorithms run in time $O(n^3\polylog n)$ in the general case, and run in time $O(n^2\polylog n)$ for the case when all segments are horizontal. In addition, we show that when restricted to translations that are only vertical, the Hausdorff distance between two sets of horizontal segments can be computed in time roughly $O(n^{3/2}{\sl polylog}n)$. These algorithms form significant improvements over the general algorithm of Chew et al. that takes time $O(n^4 \log^2 n)$. In the second part of this paper we address the problem of matching polygonal chains. We study the well known \Frd, and present the first algorithm for computing the \Frd under general translations. Our methods also yield algorithms for computing a generalization of the \Fr distance, and we also present a simple approximation algorithm for the \Frd that runs in time $O(n^2\polylog n)$.",2000-09-19T20:09:39Z,http://arxiv.org/pdf/cs/0009013v2,2024-04-28,
cs/0009024v1,"Computing the Depth of a Flat","We give algorithms for computing the regression depth of a k-flat for a set of n points in R^d. The running time is O(n^(d-2) + n log n) when 0 < k < d-1, faster than the best time bound for hyperplane regression or for data depth.",2000-09-25T19:28:49Z,http://arxiv.org/pdf/cs/0009024v1,2024-04-28,
cs/0010039v1,"Computational Geometry Column 40","It has recently been established by Below, De Loera, and Richter-Gebert that finding a minimum size (or even just a small) triangulation of a convex polyhedron is NP-complete. Their 3SAT-reduction proof is discussed.",2000-10-31T17:37:33Z,http://arxiv.org/pdf/cs/0010039v1,2024-04-28,
cs/0101006v2,"Optimal Moebius Transformations for Information Visualization and Meshing","We give linear-time quasiconvex programming algorithms for finding a Moebius transformation of a set of spheres in a unit ball or on the surface of a unit sphere that maximizes the minimum size of a transformed sphere. We can also use similar methods to maximize the minimum distance among a set of pairs of input points. We apply these results to vertex separation and symmetry display in spherical graph drawing, viewpoint selection in hyperbolic browsing, element size control in conformal structured mesh generation, and brain flat mapping.",2001-01-11T21:55:53Z,http://arxiv.org/pdf/cs/0101006v2,2024-04-28,
cs/0103017v1,"Nice point sets can have nasty Delaunay triangulations","We consider the complexity of Delaunay triangulations of sets of points in R^3 under certain practical geometric constraints. The spread of a set of points is the ratio between the longest and shortest pairwise distances. We show that in the worst case, the Delaunay triangulation of n points in R^3 with spread D has complexity Omega(min{D^3, nD, n^2}) and O(min{D^4, n^2}). For the case D = Theta(sqrt{n}), our lower bound construction consists of a uniform sample of a smooth convex surface with bounded curvature. We also construct a family of smooth connected surfaces such that the Delaunay triangulation of any good point sample has near-quadratic complexity.",2001-03-23T00:23:11Z,http://arxiv.org/pdf/cs/0103017v1,2024-04-28,
cs/0108020v3,"Flipping Cubical Meshes","We define and examine flip operations for quadrilateral and hexahedral meshes, similar to the flipping transformations previously used in triangular and tetrahedral mesh generation.",2001-08-27T20:24:51Z,http://arxiv.org/pdf/cs/0108020v3,2024-04-28,
cs/0202011v1,"Small Strictly Convex Quadrilateral Meshes of Point Sets","In this paper, we give upper and lower bounds on the number of Steiner points required to construct a strictly convex quadrilateral mesh for a planar point set. In particular, we show that $3{\lfloor\frac{n}{2}\rfloor}$ internal Steiner points are always sufficient for a convex quadrilateral mesh of $n$ points in the plane. Furthermore, for any given $n\geq 4$, there are point sets for which $\lceil\frac{n-3}{2}\rceil-1$ Steiner points are necessary for a convex quadrilateral mesh.",2002-02-12T14:11:25Z,http://arxiv.org/pdf/cs/0202011v1,2024-04-28,
cs/0203025v1,"Sufficiently Fat Polyhedra are not 2-castable","In this note we consider the problem of manufacturing a convex polyhedral object via casting. We consider a generalization of the sand casting process where the object is manufactured by gluing together two identical faces of parts cast with a single piece mold. In this model we show that the class of convex polyhedra which can be enclosed between two concentric spheres of the ratio of their radii less than 1.07 cannot be manufactured using only two cast parts.",2002-03-20T15:14:24Z,http://arxiv.org/pdf/cs/0203025v1,2024-04-28,
cs/0204042v1,"Preprocessing Chains for Fast Dihedral Rotations Is Hard or Even Impossible","We examine a computational geometric problem concerning the structure of polymers. We model a polymer as a polygonal chain in three dimensions. Each edge splits the polymer into two subchains, and a dihedral rotation rotates one of these chains rigidly about this edge. The problem is to determine, given a chain, an edge, and an angle of rotation, if the motion can be performed without causing the chain to self-intersect. An Omega(n log n) lower bound on the time complexity of this problem is known.   We prove that preprocessing a chain of n edges and answering n dihedral rotation queries is 3SUM-hard, giving strong evidence that solving n queries requires Omega(n^2) time in the worst case. For dynamic queries, which also modify the chain if the requested dihedral rotation is feasible, we show that answering n queries is by itself 3SUM-hard, suggesting that sublinear query time is impossible after any amount of preprocessing.",2002-04-19T21:03:35Z,http://arxiv.org/pdf/cs/0204042v1,2024-04-28,
cs/0204050v1,"Computing Homotopic Shortest Paths Efficiently","This paper addresses the problem of finding shortest paths homotopic to a given disjoint set of paths that wind amongst point obstacles in the plane. We present a faster algorithm than previously known.",2002-04-25T20:41:10Z,http://arxiv.org/pdf/cs/0204050v1,2024-04-28,
cs/0206002v1,"Building Space-Time Meshes over Arbitrary Spatial Domains","We present an algorithm to construct meshes suitable for space-time discontinuous Galerkin finite-element methods. Our method generalizes and improves the `Tent Pitcher' algorithm of \""Ung\""or and Sheffer. Given an arbitrary simplicially meshed domain M of any dimension and a time interval [0,T], our algorithm builds a simplicial mesh of the space-time domain Mx[0,T], in constant time per element. Our algorithm avoids the limitations of previous methods by carefully adapting the durations of space-time elements to the local quality and feature size of the underlying space mesh.",2002-06-01T23:20:51Z,http://arxiv.org/pdf/cs/0206002v1,2024-04-28,
cs/0206018v3,"On Simultaneous Graph Embedding","We consider the problem of simultaneous embedding of planar graphs. There are two variants of this problem, one in which the mapping between the vertices of the two graphs is given and another where the mapping is not given. In particular, we show that without mapping, any number of outerplanar graphs can be embedded simultaneously on an $O(n)\times O(n)$ grid, and an outerplanar and general planar graph can be embedded simultaneously on an $O(n^2)\times O(n^3)$ grid. If the mapping is given, we show how to embed two paths on an $n \times n$ grid, a caterpillar and a path on an $n \times 2n$ grid, or two caterpillar graphs on an $O(n^2)\times O(n^3)$ grid. We also show that 5 paths, or 3 caterpillars, or two general planar graphs cannot be simultaneously embedded given the mapping.",2002-06-11T23:53:35Z,http://arxiv.org/pdf/cs/0206018v3,2024-04-28,
cs/0206019v1,"Simultaneous Embedding of a Planar Graph and Its Dual on the Grid","Traditional representations of graphs and their duals suggest the requirement that the dual vertices be placed inside their corresponding primal faces, and the edges of the dual graph cross only their corresponding primal edges. We consider the problem of simultaneously embedding a planar graph and its dual into a small integer grid such that the edges are drawn as straight-line segments and the only crossings are between primal-dual pairs of edges. We provide a linear-time algorithm that simultaneously embeds a 3-connected planar graph and its dual on a (2n-2) by (2n-2) integer grid, where n is the total number of vertices in the graph and its dual. Furthermore our embedding algorithm satisfies the two natural requirements mentioned above.",2002-06-12T02:02:48Z,http://arxiv.org/pdf/cs/0206019v1,2024-04-28,
cs/0207063v1,"Parallel Delaunay Refinement: Algorithms and Analyses","In this paper, we analyze the complexity of natural parallelizations of Delaunay refinement methods for mesh generation. The parallelizations employ a simple strategy: at each iteration, they choose a set of ``independent'' points to insert into the domain, and then update the Delaunay triangulation. We show that such a set of independent points can be constructed efficiently in parallel and that the number of iterations needed is $O(\log^2(L/s))$, where $L$ is the diameter of the domain, and $s$ is the smallest edge in the output mesh. In addition, we show that the insertion of each independent set of points can be realized sequentially by Ruppert's method in two dimensions and Shewchuk's in three dimensions. Therefore, our parallel Delaunay refinement methods provide the same element quality and mesh size guarantees as the sequential algorithms in both two and three dimensions. For quasi-uniform meshes, such as those produced by Chew's method, we show that the number of iterations can be reduced to $O(\log(L/s))$. To the best of our knowledge, these are the first provably polylog$(L/s)$ parallel time Delaunay meshing algorithms that generate well-shaped meshes of size optimal to within a constant.",2002-07-15T20:20:34Z,http://arxiv.org/pdf/cs/0207063v1,2024-04-28,
cs/0207081v1,"Moebius-Invariant Natural Neighbor Interpolation","We propose an interpolation method that is invariant under Moebius transformations; that is, interpolation followed by transformation gives the same result as transformation followed by interpolation. The method uses natural (Delaunay) neighbors, but weights neighbors according to angles formed by Delaunay circles.",2002-07-24T19:00:19Z,http://arxiv.org/pdf/cs/0207081v1,2024-04-28,
cs/0209007v2,"A Survey and a New Competitive Method for the Planar min-# Problem","We survey most of the different types of approximation algorithms which minimize the number of output vertices. We present their main qualities and their inherent drawbacks.",2002-09-04T01:33:58Z,http://arxiv.org/pdf/cs/0209007v2,2024-04-28,
cs/0209027v1,"Remarks on d-Dimensional TSP Optimal Tour Length Behaviour","The well-known $O(n^{1-1/d})$ behaviour of the optimal tour length for TSP in d-dimensional Cartesian space causes breaches of the triangle inequality. Other practical inadequacies of this model are discussed, including its use as basis for approximation of the TSP optimal tour length or bounds derivations, which I attempt to remedy.",2002-09-23T21:53:22Z,http://arxiv.org/pdf/cs/0209027v1,2024-04-28,
cs/0209034v1,"An Algorithmic Study of Manufacturing Paperclips and Other Folded Structures","We study algorithmic aspects of bending wires and sheet metal into a specified structure. Problems of this type are closely related to the question of deciding whether a simple non-self-intersecting wire structure (a carpenter's ruler) can be straightened, a problem that was open for several years and has only recently been solved in the affirmative.   If we impose some of the constraints that are imposed by the manufacturing process, we obtain quite different results. In particular, we study the variant of the carpenter's ruler problem in which there is a restriction that only one joint can be modified at a time. For a linkage that does not self-intersect or self-touch, the recent results of Connelly et al. and Streinu imply that it can always be straightened, modifying one joint at a time. However, we show that for a linkage with even a single vertex degeneracy, it becomes NP-hard to decide if it can be straightened while altering only one joint at a time. If we add the restriction that each joint can be altered at most once, we show that the problem is NP-complete even without vertex degeneracies.   In the special case, arising in wire forming manufacturing, that each joint can be altered at most once, and must be done sequentially from one or both ends of the linkage, we give an efficient algorithm to determine if a linkage can be straightened.",2002-09-30T19:42:57Z,http://arxiv.org/pdf/cs/0209034v1,2024-04-28,
cs/0302031v1,"Relaxed Scheduling in Dynamic Skin Triangulation","We introduce relaxed scheduling as a paradigm for mesh maintenance and demonstrate its applicability to triangulating a skin surface in $\Rspace^3$.",2003-02-20T22:20:50Z,http://arxiv.org/pdf/cs/0302031v1,2024-04-28,
cs/0303001v1,"When Crossings Count - Approximating the Minimum Spanning Tree","In the first part of the paper, we present an (1+\mu)-approximation algorithm to the minimum-spanning tree of points in a planar arrangement of lines, where the metric is the number of crossings between the spanning tree and the lines. The expected running time is O((n/\mu^5) alpha^3(n) log^5 n), where \mu > 0 is a prescribed constant.   In the second part of our paper, we show how to embed such a crossing metric, into high-dimensions, so that the distances are preserved. As a result, we can deploy a large collection of subquadratic approximations algorithms \cite im-anntr-98,giv-rahdg-99 for problems involving points with the crossing metric as a distance function. Applications include matching, clustering, nearest-neighbor, and furthest-neighbor.",2003-03-01T22:09:20Z,http://arxiv.org/pdf/cs/0303001v1,2024-04-28,
cs/0304023v1,"Partitioning Regular Polygons into Circular Pieces I: Convex Partitions","We explore an instance of the question of partitioning a polygon into pieces, each of which is as ``circular'' as possible, in the sense of having an aspect ratio close to 1. The aspect ratio of a polygon is the ratio of the diameters of the smallest circumscribing circle to the largest inscribed disk. The problem is rich even for partitioning regular polygons into convex pieces, the focus of this paper. We show that the optimal (most circular) partition for an equilateral triangle has an infinite number of pieces, with the lower bound approachable to any accuracy desired by a particular finite partition. For pentagons and all regular k-gons, k > 5, the unpartitioned polygon is already optimal. The square presents an interesting intermediate case. Here the one-piece partition is not optimal, but nor is the trivial lower bound approachable. We narrow the optimal ratio to an aspect-ratio gap of 0.01082 with several somewhat intricate partitions.",2003-04-17T12:29:29Z,http://arxiv.org/pdf/cs/0304023v1,2024-04-28,
cs/0304025v1,"Computational Geometry Column 44","The open problem of whether or not every pair of equal-area polygons has a hinged dissection is discussed.",2003-04-18T21:12:35Z,http://arxiv.org/pdf/cs/0304025v1,2024-04-28,
cs/0307023v2,"Testing Bipartiteness of Geometric Intersection Graphs","We show how to test the bipartiteness of an intersection graph of n line segments or simple polygons in the plane, or of balls in R^d, in time O(n log n). More generally we find subquadratic algorithms for connectivity and bipartiteness testing of intersection graphs of a broad class of geometric objects. For unit balls in R^d, connectivity testing has equivalent randomized complexity to construction of Euclidean minimum spanning trees, and hence is unlikely to be solved as efficiently as bipartiteness testing. For line segments or planar disks, testing k-colorability of intersection graphs for k>2 is NP-complete.",2003-07-09T21:29:49Z,http://arxiv.org/pdf/cs/0307023v2,2024-04-28,
cs/0307027v1,"Deterministic Sampling and Range Counting in Geometric Data Streams","We present memory-efficient deterministic algorithms for constructing epsilon-nets and epsilon-approximations of streams of geometric data. Unlike probabilistic approaches, these deterministic samples provide guaranteed bounds on their approximation factors. We show how our deterministic samples can be used to answer approximate online iceberg geometric queries on data streams. We use these techniques to approximate several robust statistics of geometric data streams, including Tukey depth, simplicial depth, regression depth, the Thiel-Sen estimator, and the least median of squares. Our algorithms use only a polylogarithmic amount of memory, provided the desired approximation factors are inverse-polylogarithmic. We also include a lower bound for non-iceberg geometric queries.",2003-07-11T00:43:00Z,http://arxiv.org/pdf/cs/0307027v1,2024-04-28,
cs/0310031v1,"A weak definition of Delaunay triangulation","We show that the traditional criterion for a simplex to belong to the Delaunay triangulation of a point set is equivalent to a criterion which is a priori weaker. The argument is quite general; as well as the classical Euclidean case, it applies to hyperbolic and hemispherical geometries and to Edelsbrunner's weighted Delaunay triangulation. In spherical geometry, we establish a similar theorem under a genericity condition. The weak definition finds natural application in the problem of approximating a point-cloud data set with a simplical complex.",2003-10-16T00:14:55Z,http://arxiv.org/pdf/cs/0310031v1,2024-04-28,
cs/0405089v1,"Convex Hull of Planar H-Polyhedra","Suppose $<A_i, \vec{c}_i>$ are planar (convex) H-polyhedra, that is, $A_i \in \mathbb{R}^{n_i \times 2}$ and $\vec{c}_i \in \mathbb{R}^{n_i}$. Let $P_i = \{\vec{x} \in \mathbb{R}^2 \mid A_i\vec{x} \leq \vec{c}_i \}$ and $n = n_1 + n_2$. We present an $O(n \log n)$ algorithm for calculating an H-polyhedron $<A, \vec{c}>$ with the smallest $P = \{\vec{x} \in \mathbb{R}^2 \mid A\vec{x} \leq \vec{c} \}$ such that $P_1 \cup P_2 \subseteq P$.",2004-05-24T13:19:59Z,http://arxiv.org/pdf/cs/0405089v1,2024-04-28,
cs/0407020v1,"Minimum Enclosing Polytope in High Dimensions","We study the problem of covering a given set of $n$ points in a high, $d$-dimensional space by the minimum enclosing polytope of a given arbitrary shape. We present algorithms that work for a large family of shapes, provided either only translations and no rotations are allowed, or only rotation about a fixed point is allowed; that is, one is allowed to only scale and translate a given shape, or scale and rotate the shape around a fixed point. Our algorithms start with a polytope guessed to be of optimal size and iteratively moves it based on a greedy principle: simply move the current polytope directly towards any outside point till it touches the surface. For computing the minimum enclosing ball, this gives a simple greedy algorithm with running time $O(nd/\eps)$ producing a ball of radius $1+\eps$ times the optimal. This simple principle generalizes to arbitrary convex shape when only translations are allowed, requiring at most $O(1/\eps^2)$ iterations. Our algorithm implies that {\em core-sets} of size $O(1/\eps^2)$ exist not only for minimum enclosing ball but also for any convex shape with a fixed orientation. A {\em Core-Set} is a small subset of $poly(1/\eps)$ points whose minimum enclosing polytope is almost as large as that of the original points. Although we are unable to combine our techniques for translations and rotations for general shapes, for the min-cylinder problem, we give an algorithm similar to the one in \cite{HV03}, but with an improved running time of $2^{O(\frac{1}{\eps^2}\log \frac{1}{\eps})} nd$.",2004-07-08T19:03:32Z,http://arxiv.org/pdf/cs/0407020v1,2024-04-28,
cs/0412025v3,"Minimum Dilation Stars","The dilation of a Euclidean graph is defined as the ratio of distance in the graph divided by distance in R^d. In this paper we consider the problem of positioning the root of a star such that the dilation of the resulting star is minimal. We present a deterministic O(n log n)-time algorithm for evaluating the dilation of a given star; a randomized O(n log n) expected-time algorithm for finding an optimal center in R^d; and for the case d=2, a randomized O(n 2^(alpha(n)) log^2 n) expected-time algorithm for finding an optimal center among the input points.",2004-12-07T01:39:36Z,http://arxiv.org/pdf/cs/0412025v3,2024-04-28,
cs/0412046v1,"Quasiconvex Programming","We define quasiconvex programming, a form of generalized linear programming in which one seeks the point minimizing the pointwise maximum of a collection of quasiconvex functions. We survey algorithms for solving quasiconvex programs either numerically or via generalizations of the dual simplex method from linear programming, and describe varied applications of this geometric optimization technique in meshing, scientific computation, information visualization, automated algorithm analysis, and robust statistics.",2004-12-10T22:50:50Z,http://arxiv.org/pdf/cs/0412046v1,2024-04-28,
cs/0501007v1,"A Time-Optimal Delaunay Refinement Algorithm in Two Dimensions","We propose a new refinement algorithm to generate size-optimal quality-guaranteed Delaunay triangulations in the plane. The algorithm takes $O(n \log n + m)$ time, where $n$ is the input size and $m$ is the output size. This is the first time-optimal Delaunay refinement algorithm.",2005-01-04T22:30:04Z,http://arxiv.org/pdf/cs/0501007v1,2024-04-28,
cs/0505017v1,"Point set stratification and Delaunay depth","In the study of depth functions it is important to decide whether we want such a function to be sensitive to multimodality or not. In this paper we analyze the Delaunay depth function, which is sensitive to multimodality and compare this depth with others, as convex depth and location depth. We study the stratification that Delaunay depth induces in the point set (layers) and in the whole plane (levels), and we develop an algorithm for computing the Delaunay depth contours, associated to a point set in the plane, with running time O(n log^2 n). The depth of a query point p with respect to a data set S in the plane is the depth of p in the union of S and p. When S and p are given in the input the Delaunay depth can be computed in O(n log n), and we prove that this value is optimal.",2005-05-08T16:36:42Z,http://arxiv.org/pdf/cs/0505017v1,2024-04-28,
cs/0505047v1,"A Simple Proof of the F{á}ry-Wagner Theorem","We give a simple proof of the following fundamental result independently due to Fary (1948) and Wagner (1936): Every plane graph has a drawing in which every edge is straight.",2005-05-18T11:27:59Z,http://arxiv.org/pdf/cs/0505047v1,2024-04-28,
cs/0507038v1,"Upper Bound on the Number of Vertices of Polyhedra with $0,1$-Constraint Matrices","In this note we show that the maximum number of vertices in any polyhedron $P=\{x\in \mathbb{R}^d : Ax\leq b\}$ with $0,1$-constraint matrix $A$ and a real vector $b$ is at most $d!$.",2005-07-14T18:45:36Z,http://arxiv.org/pdf/cs/0507038v1,2024-04-28,
cs/0507049v1,"The Skip Quadtree: A Simple Dynamic Data Structure for Multidimensional Data","We present a new multi-dimensional data structure, which we call the skip quadtree (for point data in R^2) or the skip octree (for point data in R^d, with constant d>2). Our data structure combines the best features of two well-known data structures, in that it has the well-defined ""box""-shaped regions of region quadtrees and the logarithmic-height search and update hierarchical structure of skip lists. Indeed, the bottom level of our structure is exactly a region quadtree (or octree for higher dimensional data). We describe efficient algorithms for inserting and deleting points in a skip quadtree, as well as fast methods for performing point location and approximate range queries.",2005-07-19T20:17:12Z,http://arxiv.org/pdf/cs/0507049v1,2024-04-28,
cs/0510004v1,"The Hunting of the Bump: On Maximizing Statistical Discrepancy","Anomaly detection has important applications in biosurveilance and environmental monitoring. When comparing measured data to data drawn from a baseline distribution, merely, finding clusters in the measured data may not actually represent true anomalies. These clusters may likely be the clusters of the baseline distribution. Hence, a discrepancy function is often used to examine how different measured data is to baseline data within a region. An anomalous region is thus defined to be one with high discrepancy.   In this paper, we present algorithms for maximizing statistical discrepancy functions over the space of axis-parallel rectangles. We give provable approximation guarantees, both additive and relative, and our methods apply to any convex discrepancy function. Our algorithms work by connecting statistical discrepancy to combinatorial discrepancy; roughly speaking, we show that in order to maximize a convex discrepancy function over a class of shapes, one needs only maximize a linear discrepancy function over the same set of shapes.   We derive general discrepancy functions for data generated from a one- parameter exponential family. This generalizes the widely-used Kulldorff scan statistic for data from a Poisson distribution. We present an algorithm running in $O(\smash[tb]{\frac{1}{\epsilon} n^2 \log^2 n})$ that computes the maximum discrepancy rectangle to within additive error $\epsilon$, for the Kulldorff scan statistic. Similar results hold for relative error and for discrepancy functions for data coming from Gaussian, Bernoulli, and gamma distributions. Prior to our work, the best known algorithms were exact and ran in time $\smash[t]{O(n^4)}$.",2005-10-03T02:09:14Z,http://arxiv.org/pdf/cs/0510004v1,2024-04-28,
cs/0510024v1,"Delta-confluent Drawings","We generalize the tree-confluent graphs to a broader class of graphs called Delta-confluent graphs. This class of graphs and distance-hereditary graphs, a well-known class of graphs, coincide. Some results about the visualization of Delta-confluent graphs are also given.",2005-10-11T00:47:37Z,http://arxiv.org/pdf/cs/0510024v1,2024-04-28,
cs/0510053v2,"A pair of trees without a simultaneous geometric embedding in the plane","Any planar graph has a crossing-free straight-line drawing in the plane. A simultaneous geometric embedding of two n-vertex graphs is a straight-line drawing of both graphs on a common set of n points, such that the edges withing each individual graph do not cross. We consider simultaneous embeddings of two labeled trees, with predescribed vertex correspondences, and present an instance of such a pair that cannot be embedded. Further we provide an example of a planar graph that cannot be embedded together with a path when vertex correspondences are given.",2005-10-18T18:26:00Z,http://arxiv.org/pdf/cs/0510053v2,2024-04-28,
cs/0510088v2,"Lower bounds on Locality Sensitive Hashing","Given a metric space $(X,d_X)$, $c\ge 1$, $r>0$, and $p,q\in [0,1]$, a distribution over mappings $\h:X\to \mathbb N$ is called a $(r,cr,p,q)$-sensitive hash family if any two points in $X$ at distance at most $r$ are mapped by $\h$ to the same value with probability at least $p$, and any two points at distance greater than $cr$ are mapped by $\h$ to the same value with probability at most $q$. This notion was introduced by Indyk and Motwani in 1998 as the basis for an efficient approximate nearest neighbor search algorithm, and has since been used extensively for this purpose. The performance of these algorithms is governed by the parameter $\rho=\frac{\log(1/p)}{\log(1/q)}$, and constructing hash families with small $\rho$ automatically yields improved nearest neighbor algorithms. Here we show that for $X=\ell_1$ it is impossible to achieve $\rho\le \frac{1}{2c}$. This almost matches the construction of Indyk and Motwani which achieves $\rho\le \frac{1}{c}$.",2005-10-29T15:05:52Z,http://arxiv.org/pdf/cs/0510088v2,2024-04-28,
cs/0510090v1,"A simple effective method for curvatures estimation on triangular meshes","To definite and compute differential invariants, like curvatures, for triangular meshes (or polyhedral surfaces) is a key problem in CAGD and the computer vision. The Gaussian curvature and the mean curvature are determined by the differential of the Gauss map of the underlying surface. The Gauss map assigns to each point in the surface the unit normal vector of the tangent plane to the surface at this point. We follow the ideas developed in Chen and Wu \cite{Chen2}(2004) and Wu, Chen and Chi\cite{Wu}(2005) to describe a new and simple approach to estimate the differential of the Gauss map and curvatures from the viewpoint of the gradient and the centroid weights. This will give us a much better estimation of curvatures than Taubin's algorithm \cite{Taubin} (1995).",2005-10-31T02:41:33Z,http://arxiv.org/pdf/cs/0510090v1,2024-04-28,
cs/0512001v1,"Which n-Venn diagrams can be drawn with convex k-gons?","We establish a new lower bound for the number of sides required for the component curves of simple Venn diagrams made from polygons. Specifically, for any n-Venn diagram of convex k-gons, we prove that k >= (2^n - 2 - n) / (n (n-2)). In the process we prove that Venn diagrams of seven curves, simple or not, cannot be formed from triangles. We then give an example achieving the new lower bound of a (simple, symmetric) Venn diagram of seven quadrilaterals. Previously Grunbaum had constructed a 7-Venn diagram of non-convex 5-gons [``Venn Diagrams II'', Geombinatorics 2:25-31, 1992].",2005-11-30T23:30:19Z,http://arxiv.org/pdf/cs/0512001v1,2024-04-28,
cs/0512064v1,"Computing shortest non-trivial cycles on orientable surfaces of bounded genus in almost linear time","We present an algorithm that computes a shortest non-contractible and a shortest non-separating cycle on an orientable combinatorial surface of bounded genus in O(n \log n) time, where n denotes the complexity of the surface. This solves a central open problem in computational topology, improving upon the current-best O(n^{3/2})-time algorithm by Cabello and Mohar (ESA 2005). Our algorithm uses universal-cover constructions to find short cycles and makes extensive use of existing tools from the field.",2005-12-15T17:02:53Z,http://arxiv.org/pdf/cs/0512064v1,2024-04-28,
cs/0601033v1,"The density of iterated crossing points and a gap result for triangulations of finite point sets","Consider a plane graph G, drawn with straight lines. For every pair a,b of vertices of G, we compare the shortest-path distance between a and b in G (with Euclidean edge lengths) to their actual distance in the plane. The worst-case ratio of these two values, for all pairs of points, is called the dilation of G. All finite plane graphs of dilation 1 have been classified. They are closely related to the following iterative procedure. For a given point set P in R^2, we connect every pair of points in P by a line segment and then add to P all those points where two such line segments cross. Repeating this process infinitely often, yields a limit point set P*.   The main result of this paper is the following gap theorem: For any finite point set P in the plane for which the above P* is infinite, there exists a threshold t > 1 such that P is not contained in the vertex set of any finite plane graph of dilation at most t. We construct a concrete point set Q such that any planar graph that contains this set amongst its vertices must have a dilation larger than 1.0000047.",2006-01-10T01:13:27Z,http://arxiv.org/pdf/cs/0601033v1,2024-04-28,
cs/0601128v2,"On the 3-distortion of a path","We prove that, when a path of length n is embedded in R^2, the 3-distortion is an Omega(n^{1/2}), and that, when embedded in R^d, the 3-distortion is an O(n^{1/d-1}).",2006-01-30T20:58:48Z,http://arxiv.org/pdf/cs/0601128v2,2024-04-28,
cs/0602080v1,"Pants Decomposition of the Punctured Plane","A pants decomposition of an orientable surface S is a collection of simple cycles that partition S into pants, i.e., surfaces of genus zero with three boundary cycles. Given a set P of n points in the plane, we consider the problem of computing a pants decomposition of the surface S which is the plane minus P, of minimum total length. We give a polynomial-time approximation scheme using Mitchell's guillotine rectilinear subdivisions. We give a quartic-time algorithm to compute the shortest pants decomposition of S when the cycles are restricted to be axis-aligned boxes, and a quadratic-time algorithm when all the points lie on a line; both exact algorithms use dynamic programming with Yao's speedup.",2006-02-22T19:08:48Z,http://arxiv.org/pdf/cs/0602080v1,2024-04-28,
cs/0602095v2,"Epsilon-Unfolding Orthogonal Polyhedra","An unfolding of a polyhedron is produced by cutting the surface and flattening to a single, connected, planar piece without overlap (except possibly at boundary points). It is a long unsolved problem to determine whether every polyhedron may be unfolded. Here we prove, via an algorithm, that every orthogonal polyhedron (one whose faces meet at right angles) of genus zero may be unfolded. Our cuts are not necessarily along edges of the polyhedron, but they are always parallel to polyhedron edges. For a polyhedron of n vertices, portions of the unfolding will be rectangular strips which, in the worst case, may need to be as thin as epsilon = 1/2^{Omega(n)}.",2006-02-27T17:03:19Z,http://arxiv.org/pdf/cs/0602095v2,2024-04-28,
cs/0603002v1,"On comparing sums of square roots of small integers","Let $k$ and $n$ be positive integers, $n>k$. Define $r(n,k)$ to be the minimum positive value of $$ |\sqrt{a_1} + ... + \sqrt{a_k} - \sqrt{b_1} - >... -\sqrt{b_k} | $$ where $ a_1, a_2, ..., a_k, b_1, b_2, ..., b_k $ are positive integers no larger than $n$. It is an important problem in computational geometry to determine a good upper bound of $-\log r(n,k)$. In this paper we prove an upper bound of $ 2^{O(n/\log n)} \log n$, which is better than the best known result $O(2^{2k} \log n)$ whenever $ n \leq ck\log k$ for some constant $c$. In particular, our result implies a {\em subexponential} algorithm to compare two sums of square roots of integers of size $o(k\log k)$.",2006-02-28T22:35:37Z,http://arxiv.org/pdf/cs/0603002v1,2024-04-28,
cs/0603057v1,"Guard Placement For Wireless Localization","Motivated by secure wireless networking, we consider the problem of placing fixed localizers that enable mobile communication devices to prove they belong to a secure region that is defined by the interior of a polygon. Each localizer views an infinite wedge of the plane, and a device can prove membership in the secure region if it is inside the wedges for a set of localizers whose common intersection contains no points outside the polygon. This model leads to a broad class of new art gallery type problems, for which we provide upper and lower bounds.",2006-03-14T20:19:36Z,http://arxiv.org/pdf/cs/0603057v1,2024-04-28,
cs/0604022v4,"Locked and Unlocked Chains of Planar Shapes","We extend linkage unfolding results from the well-studied case of polygonal linkages to the more general case of linkages of polygons. More precisely, we consider chains of nonoverlapping rigid planar shapes (Jordan regions) that are hinged together sequentially at rotatable joints. Our goal is to characterize the families of planar shapes that admit locked chains, where some configurations cannot be reached by continuous reconfiguration without self-intersection, and which families of planar shapes guarantee universal foldability, where every chain is guaranteed to have a connected configuration space. Previously, only obtuse triangles were known to admit locked shapes, and only line segments were known to guarantee universal foldability. We show that a surprisingly general family of planar shapes, called slender adornments, guarantees universal foldability: roughly, the distance from each edge along the path along the boundary of the slender adornment to each hinge should be monotone. In contrast, we show that isosceles triangles with any desired apex angle less than 90 degrees admit locked chains, which is precisely the threshold beyond which the inward-normal property no longer holds.",2006-04-06T14:46:27Z,http://arxiv.org/pdf/cs/0604022v4,2024-04-28,
cs/0604034v2,"Squarepants in a Tree: Sum of Subtree Clustering and Hyperbolic Pants Decomposition","We provide efficient constant factor approximation algorithms for the problems of finding a hierarchical clustering of a point set in any metric space, minimizing the sum of minimimum spanning tree lengths within each cluster, and in the hyperbolic or Euclidean planes, minimizing the sum of cluster perimeters. Our algorithms for the hyperbolic and Euclidean planes can also be used to provide a pants decomposition, that is, a set of disjoint simple closed curves partitioning the plane minus the input points into subsets with exactly three boundary components, with approximately minimum total length. In the Euclidean case, these curves are squares; in the hyperbolic case, they combine our Euclidean square pants decomposition with our tree clustering method for general metric spaces.",2006-04-09T01:18:29Z,http://arxiv.org/pdf/cs/0604034v2,2024-04-28,
cs/0604059v1,"Inner and Outer Rounding of Boolean Operations on Lattice Polygonal Regions","Robustness problems due to the substitution of the exact computation on real numbers by the rounded floating point arithmetic are often an obstacle to obtain practical implementation of geometric algorithms. If the adoption of the --exact computation paradigm--[Yap et Dube] gives a satisfactory solution to this kind of problems for purely combinatorial algorithms, this solution does not allow to solve in practice the case of algorithms that cascade the construction of new geometric objects. In this report, we consider the problem of rounding the intersection of two polygonal regions onto the integer lattice with inclusion properties. Namely, given two polygonal regions A and B having their vertices on the integer lattice, the inner and outer rounding modes construct two polygonal regions with integer vertices which respectively is included and contains the true intersection. We also prove interesting results on the Hausdorff distance, the size and the convexity of these polygonal regions.",2006-04-13T14:33:15Z,http://arxiv.org/pdf/cs/0604059v1,2024-04-28,
cs/0605029v1,"Spanners for Geometric Intersection Graphs","Efficient algorithms are presented for constructing spanners in geometric intersection graphs. For a unit ball graph in R^k, a (1+\epsilon)-spanner is obtained using efficient partitioning of the space into hypercubes and solving bichromatic closest pair problems. The spanner construction has almost equivalent complexity to the construction of Euclidean minimum spanning trees. The results are extended to arbitrary ball graphs with a sub-quadratic running time.   For unit ball graphs, the spanners have a small separator decomposition which can be used to obtain efficient algorithms for approximating proximity problems like diameter and distance queries. The results on compressed quadtrees, geometric graph separators, and diameter approximation might be of independent interest.",2006-05-07T23:38:00Z,http://arxiv.org/pdf/cs/0605029v1,2024-04-28,
cs/0606013v1,"Good Illumination of Minimum Range","A point p is 1-well illuminated by a set F of n point lights if p lies in the interior of the convex hull of F. This concept corresponds to triangle-guarding or well-covering. In this paper we consider the illumination range of the light sources as a parameter to be optimized. First, we solve the problem of minimizing the light sources' illumination range to 1-well illuminate a given point p. We also compute a minimal set of light sources that 1-well illuminates p with minimum illumination range. Second, we solve the problem of minimizing the light sources' illumination range to 1-well illuminate all the points of a line segment with an O(n^2) algorithm. Finally, we give an O(n^2 log n) algorithm for preprocessing the data so that one can obtain the illumination range needed to 1-well illuminate a point of a line segment in O(log n) time. These results can be applied to solve problems of 1-well illuminating a trajectory by approaching it to a polygonal path.",2006-06-02T11:24:27Z,http://arxiv.org/pdf/cs/0606013v1,2024-04-28,
cs/0606036v1,"Computational Euclid","We analyse the axioms of Euclidean geometry according to standard object-oriented software development methodology. We find a perfect match: the main undefined concepts of the axioms translate to object classes. The result is a suite of C++ classes that efficiently supports the construction of complex geometric configurations. Although all computations are performed in floating-point arithmetic, they correctly implement as semi-decision algorithms the tests for equality of points, a point being on a line or in a plane, a line being in a plane, parallelness of lines, of a line and a plane, and of planes. That is, in accordance to the fundamental limitations to computability requiring that only negative outcomes are given with certainty, while positive outcomes only imply possibility of these conditions being true.",2006-06-08T15:43:09Z,http://arxiv.org/pdf/cs/0606036v1,2024-04-28,
cs/0606041v1,"Characterization of Pentagons Determined by Two X-rays","This paper contains some results of pentagons which can be determined by two X-rays. The results reveal this problem is more complicated.",2006-06-09T14:13:15Z,http://arxiv.org/pdf/cs/0606041v1,2024-04-28,
cs/0607094v1,"Upright-Quad Drawing of st-Planar Learning Spaces","We consider graph drawing algorithms for learning spaces, a type of st-oriented partial cube derived from antimatroids and used to model states of knowledge of students. We show how to draw any st-planar learning space so all internal faces are convex quadrilaterals with the bottom side horizontal and the left side vertical, with one minimal and one maximal vertex. Conversely, every such drawing represents an st-planar learning space. We also describe connections between these graphs and arrangements of translates of a quadrant.",2006-07-20T04:52:14Z,http://arxiv.org/pdf/cs/0607094v1,2024-04-28,
cs/0607113v1,"Trees with Convex Faces and Optimal Angles","We consider drawings of trees in which all edges incident to leaves can be extended to infinite rays without crossing, partitioning the plane into infinite convex polygons. Among all such drawings we seek the one maximizing the angular resolution of the drawing. We find linear time algorithms for solving this problem, both for plane trees and for trees without a fixed embedding. In any such drawing, the edge lengths may be set independently of the angles, without crossing; we describe multiple strategies for setting these lengths.",2006-07-26T00:44:20Z,http://arxiv.org/pdf/cs/0607113v1,2024-04-28,
cs/0607145v3,"Geometric definition of a new skeletonization concept","The Divider set, as an innovative alternative concept to maximal disks, Voronoi sets and cut loci, is presented with a formal definition based on topology and differential geometry. The relevant mathematical theory by previous authors and a comparison with other medial axis definitions is presented. Appropriate applications are proposed and examined.",2006-07-31T18:17:30Z,http://arxiv.org/pdf/cs/0607145v3,2024-04-28,
cs/0609033v1,"Choosing Colors for Geometric Graphs via Color Space Embeddings","Graph drawing research traditionally focuses on producing geometric embeddings of graphs satisfying various aesthetic constraints. After the geometric embedding is specified, there is an additional step that is often overlooked or ignored: assigning display colors to the graph's vertices. We study the additional aesthetic criterion of assigning distinct colors to vertices of a geometric graph so that the colors assigned to adjacent vertices are as different from one another as possible. We formulate this as a problem involving perceptual metrics in color space and we develop algorithms for solving this problem by embedding the graph in color space. We also present an application of this work to a distributed load-balancing visualization problem.",2006-09-07T20:39:19Z,http://arxiv.org/pdf/cs/0609033v1,2024-04-28,
cs/0609078v2,"A Continuum Theory for Unstructured Mesh Generation in Two Dimensions","A continuum description of unstructured meshes in two dimensions, both for planar and curved surface domains, is proposed. The meshes described are those which, in the limit of an increasingly finer mesh (smaller cells), and away from irregular vertices, have ideally-shaped cells (squares or equilateral triangles), and can therefore be completely described by two local properties: local cell size and local edge directions. The connection between the two properties is derived by defining a Riemannian manifold whose geodesics trace the edges of the mesh. A function $\phi$, proportional to the logarithm of the cell size, is shown to obey the Poisson equation, with localized charges corresponding to irregular vertices. The problem of finding a suitable manifold for a given domain is thus shown to exactly reduce to an Inverse Poisson problem on $\phi$, of finding a distribution of localized charges adhering to the conditions derived for boundary alignment. Possible applications to mesh generation are discussed.",2006-09-14T00:07:39Z,http://arxiv.org/pdf/cs/0609078v2,2024-04-28,
cs/0612026v1,"A disk-covering problem with application in optical interferometry","Given a disk O in the plane called the objective, we want to find n small disks P_1,...,P_n called the pupils such that $\bigcup_{i,j=1}^n P_i \ominus P_j \supseteq O$, where $\ominus$ denotes the Minkowski difference operator, while minimizing the number of pupils, the sum of the radii or the total area of the pupils. This problem is motivated by the construction of very large telescopes from several smaller ones by so-called Optical Aperture Synthesis. In this paper, we provide exact, approximate and heuristic solutions to several variations of the problem.",2006-12-05T15:36:21Z,http://arxiv.org/pdf/cs/0612026v1,2024-04-28,
cs/0701007v1,"On the Complexity of the Circular Chromatic Number","Circular chromatic number, $\chi_c$ is a natural generalization of chromatic number. It is known that it is \NP-hard to determine whether or not an arbitrary graph $G$ satisfies $\chi(G) = \chi_c(G)$. In this paper we prove that this problem is \NP-hard even if the chromatic number of the graph is known. This answers a question of Xuding Zhu. Also we prove that for all positive integers $k \ge 2$ and $n \ge 3$, for a given graph $G$ with $\chi(G)=n$, it is \NP-complete to verify if $\chi_c(G) \le n- \frac{1}{k}$.",2006-12-31T04:48:59Z,http://arxiv.org/pdf/cs/0701007v1,2024-04-28,
cs/0701096v2,"About the domino problem in the hyperbolic plane, a new solution","In this paper we improve the approach of a previous paper about the domino problem in the hyperbolic plane, see arXiv.cs.CG/0603093. This time, we prove that the general problem of the hyperbolic plane with \`a la Wang tiles is undecidable.",2007-01-16T18:19:22Z,http://arxiv.org/pdf/cs/0701096v2,2024-04-28,
cs/0702039v1,"Hadwiger and Helly-type theorems for disjoint unit spheres","We prove Helly-type theorems for line transversals to disjoint unit balls in $\R^{d}$. In particular, we show that a family of $n \geq 2d$ disjoint unit balls in $\R^d$ has a line transversal if, for some ordering $\prec$ of the balls, any subfamily of 2d balls admits a line transversal consistent with $\prec$. We also prove that a family of $n \geq 4d-1$ disjoint unit balls in $\R^d$ admits a line transversal if any subfamily of size $4d-1$ admits a transversal.",2007-02-07T08:29:03Z,http://arxiv.org/pdf/cs/0702039v1,2024-04-28,
cs/0702080v2,"Sparse geometric graphs with small dilation","Given a set S of n points in R^D, and an integer k such that 0 <= k < n, we show that a geometric graph with vertex set S, at most n - 1 + k edges, maximum degree five, and dilation O(n / (k+1)) can be computed in time O(n log n). For any k, we also construct planar n-point sets for which any geometric graph with n-1+k edges has dilation Omega(n/(k+1)); a slightly weaker statement holds if the points of S are required to be in convex position.",2007-02-14T03:46:54Z,http://arxiv.org/pdf/cs/0702080v2,2024-04-28,
cs/0702087v1,"An Upper Bound on the Average Size of Silhouettes","It is a widely observed phenomenon in computer graphics that the size of the silhouette of a polyhedron is much smaller than the size of the whole polyhedron. This paper provides, for the first time, theoretical evidence supporting this for a large class of objects, namely for polyhedra that approximate surfaces in some reasonable way; the surfaces may be non-convex and non-differentiable and they may have boundaries. We prove that such polyhedra have silhouettes of expected size $O(\sqrt{n})$ where the average is taken over all points of view and n is the complexity of the polyhedron.",2007-02-14T13:52:35Z,http://arxiv.org/pdf/cs/0702087v1,2024-04-28,
cs/0702117v3,"On a family of strong geometric spanners that admit local routing strategies","We introduce a family of directed geometric graphs, denoted $\paz$, that depend on two parameters $\lambda$ and $\theta$. For $0\leq \theta<\frac{\pi}{2}$ and ${1/2} < \lambda < 1$, the $\paz$ graph is a strong $t$-spanner, with $t=\frac{1}{(1-\lambda)\cos\theta}$. The out-degree of a node in the $\paz$ graph is at most $\lfloor2\pi/\min(\theta, \arccos\frac{1}{2\lambda})\rfloor$. Moreover, we show that routing can be achieved locally on $\paz$. Next, we show that all strong $t$-spanners are also $t$-spanners of the unit disk graph. Simulations for various values of the parameters $\lambda$ and $\theta$ indicate that for random point sets, the spanning ratio of $\paz$ is better than the proven theoretical bounds.",2007-02-20T20:54:16Z,http://arxiv.org/pdf/cs/0702117v3,2024-04-28,
cs/0703023v1,"Computing a Minimum-Dilation Spanning Tree is NP-hard","In a geometric network G = (S, E), the graph distance between two vertices u, v in S is the length of the shortest path in G connecting u to v. The dilation of G is the maximum factor by which the graph distance of a pair of vertices differs from their Euclidean distance. We show that given a set S of n points with integer coordinates in the plane and a rational dilation delta > 1, it is NP-hard to determine whether a spanning tree of S with dilation at most delta exists.",2007-03-06T01:53:36Z,http://arxiv.org/pdf/cs/0703023v1,2024-04-28,
cs/0703030v1,"An Efficient Local Approach to Convexity Testing of Piecewise-Linear Hypersurfaces","We show that a closed piecewise-linear hypersurface immersed in $R^n$ ($n\ge 3$) is the boundary of a convex body if and only if every point in the interior of each $(n-3)$-face has a neighborhood that lies on the boundary of some convex body; no assumptions about the hypersurface's topology are needed. We derive this criterion from our generalization of Van Heijenoort's (1952) theorem on locally convex hypersurfaces in $R^n$ to spherical spaces. We also give an easy-to-implement convexity testing algorithm, which is based on our criterion. For $R^3$ the number of arithmetic operations used by the algorithm is at most linear in the number of vertices, while in general it is at most linear in the number of incidences between the $(n-2)$-faces and $(n-3)$-faces. When the dimension $n$ is not fixed and only ring arithmetic is allowed, the algorithm still remains polynomial. Our method works in more general situations than the convexity verification algorithms developed by Mehlhorn et al. (1996) and Devillers et al. (1998) -- for example, our method does not require the input surface to be orientable, nor it requires the input data to include normal vectors to the facets that are oriented ""in a coherent way"". For $R^3$ the complexity of our algorithm is the same as that of previous algorithms; for higher dimensions there seems to be no clear winner, but our approach is the only one that easily handles inputs in which the facet normals are not known to be coherently oriented or are not given at all. Furthermore, our method can be extended to piecewise-polynomial surfaces of small degree.",2007-03-07T07:33:02Z,http://arxiv.org/pdf/cs/0703030v1,2024-04-28,
cs/0703037v1,"Constructing Optimal Highways","For two points $p$ and $q$ in the plane, a straight line $h$, called a highway, and a real $v>1$, we define the \emph{travel time} (also known as the \emph{city distance}) from $p$ and $q$ to be the time needed to traverse a quickest path from $p$ to $q$, where the distance is measured with speed $v$ on $h$ and with speed 1 in the underlying metric elsewhere.   Given a set $S$ of $n$ points in the plane and a highway speed $v$, we consider the problem of finding a \emph{highway} that minimizes the maximum travel time over all pairs of points in $S$. If the orientation of the highway is fixed, the optimal highway can be computed in linear time, both for the $L_1$- and the Euclidean metric as the underlying metric. If arbitrary orientations are allowed, then the optimal highway can be computed in $O(n^{2} \log n)$ time. We also consider the problem of computing an optimal pair of highways, one being horizontal, one vertical.",2007-03-08T17:22:21Z,http://arxiv.org/pdf/cs/0703037v1,2024-04-28,
0705.0635v2,"Moving Walkways, Escalators, and Elevators","We study a simple geometric model of transportation facility that consists of two points between which the travel speed is high. This elementary definition can model shuttle services, tunnels, bridges, teleportation devices, escalators or moving walkways. The travel time between a pair of points is defined as a time distance, in such a way that a customer uses the transportation facility only if it is helpful.   We give algorithms for finding the optimal location of such a transportation facility, where optimality is defined with respect to the maximum travel time between two points in a given set.",2007-05-04T14:52:06Z,http://arxiv.org/pdf/0705.0635v2,2024-04-28,
0705.1956v1,"A Branch and Cut Algorithm for the Halfspace Depth Problem","The concept of data depth in non-parametric multivariate descriptive statistics is the generalization of the univariate rank method to multivariate data. Halfspace depth is a measure of data depth. Given a set S of points and a point p, the halfspace depth (or rank) k of p is defined as the minimum number of points of S contained in any closed halfspace with p on its boundary. Computing halfspace depth is NP-hard, and it is equivalent to the Maximum Feasible Subsystem problem. In this thesis a mixed integer program is formulated with the big-M method for the halfspace depth problem. We suggest a branch and cut algorithm. In this algorithm, Chinneck's heuristic algorithm is used to find an upper bound and a related technique based on sensitivity analysis is used for branching. Irreducible Infeasible Subsystem (IIS) hitting set cuts are applied. We also suggest a binary search algorithm which may be more stable numerically. The algorithms are implemented with the BCP framework from the COIN-OR project.",2007-05-14T15:31:22Z,http://arxiv.org/pdf/0705.1956v1,2024-04-28,
0705.3343v1,"Optimal Separable Algorithms to Compute the Reverse Euclidean Distance Transformation and Discrete Medial Axis in Arbitrary Dimension","In binary images, the distance transformation (DT) and the geometrical skeleton extraction are classic tools for shape analysis. In this paper, we present time optimal algorithms to solve the reverse Euclidean distance transformation and the reversible medial axis extraction problems for $d$-dimensional images. We also present a $d$-dimensional medial axis filtering process that allows us to control the quality of the reconstructed shape.",2007-05-23T11:29:52Z,http://arxiv.org/pdf/0705.3343v1,2024-04-28,
0705.4085v1,"The Distance Geometry of Music","We demonstrate relationships between the classic Euclidean algorithm and many other fields of study, particularly in the context of music and distance geometry. Specifically, we show how the structure of the Euclidean algorithm defines a family of rhythms which encompass over forty timelines (\emph{ostinatos}) from traditional world music. We prove that these \emph{Euclidean rhythms} have the mathematical property that their onset patterns are distributed as evenly as possible: they maximize the sum of the Euclidean distances between all pairs of onsets, viewing onsets as points on a circle. Indeed, Euclidean rhythms are the unique rhythms that maximize this notion of \emph{evenness}. We also show that essentially all Euclidean rhythms are \emph{deep}: each distinct distance between onsets occurs with a unique multiplicity, and these multiplicies form an interval $1,2,...,k-1$. Finally, we characterize all deep rhythms, showing that they form a subclass of generated rhythms, which in turn proves a useful property called shelling. All of our results for musical rhythms apply equally well to musical scales. In addition, many of the problems we explore are interesting in their own right as distance geometry problems on the circle; some of the same problems were explored by Erd\H{o}s in the plane.",2007-05-28T18:36:19Z,http://arxiv.org/pdf/0705.4085v1,2024-04-28,
0707.0610v4,"Unfolding Orthogonal Terrains","It is shown that every orthogonal terrain, i.e., an orthogonal (right-angled) polyhedron based on a rectangle that meets every vertical line in a segment, has a grid unfolding: its surface may be unfolded to a single non-overlapping piece by cutting along grid edges defined by coordinate planes through every vertex.",2007-07-04T15:04:33Z,http://arxiv.org/pdf/0707.0610v4,2024-04-28,
0708.0964v1,"Nodally 3-connected planar graphs and convex combination mappings","A convex combination mapping of a planar graph is a plane mapping in which the external vertices are mapped to the corners of a convex polygon and every internal vertex is a proper weighted average of its neighbours. If a planar graph is nodally 3-connected or triangulated then every such mapping is an embedding (Tutte, Floater).   We give a simple characterisation of nodally 3-connected planar graphs, and generalise the above result to any planar graph which admits any convex embedding.",2007-08-07T14:32:38Z,http://arxiv.org/pdf/0708.0964v1,2024-04-28,
0709.1647v1,"Unfolding Restricted Convex Caps","This paper details an algorithm for unfolding a class of convex polyhedra, where each polyhedron in the class consists of a convex cap over a rectangular base, with several restrictions: the cap's faces are quadrilaterals, with vertices over an underlying integer lattice, and such that the cap convexity is ``radially monotone,'' a type of smoothness constraint. Extensions of Cauchy's arm lemma are used in the proof of non-overlap.",2007-09-11T15:01:31Z,http://arxiv.org/pdf/0709.1647v1,2024-04-28,
0709.1941v3,"Speeding up Simplification of Polygonal Curves using Nested Approximations","We develop a multiresolution approach to the problem of polygonal curve approximation. We show theoretically and experimentally that, if the simplification algorithm A used between any two successive levels of resolution satisfies some conditions, the multiresolution algorithm MR will have a complexity lower than the complexity of A. In particular, we show that if A has a O(N2/K) complexity (the complexity of a reduced search dynamic solution approach), where N and K are respectively the initial and the final number of segments, the complexity of MR is in O(N).We experimentally compare the outcomes of MR with those of the optimal ""full search"" dynamic programming solution and of classical merge and split approaches. The experimental evaluations confirm the theoretical derivations and show that the proposed approach evaluated on 2D coastal maps either shows a lower complexity or provides polygonal approximations closer to the initial curves.",2007-09-12T18:27:53Z,http://arxiv.org/pdf/0709.1941v3,2024-04-28,
0709.2196v1,"Bregman Voronoi Diagrams: Properties, Algorithms and Applications","The Voronoi diagram of a finite set of objects is a fundamental geometric structure that subdivides the embedding space into regions, each region consisting of the points that are closer to a given object than to the others. We may define many variants of Voronoi diagrams depending on the class of objects, the distance functions and the embedding space. In this paper, we investigate a framework for defining and building Voronoi diagrams for a broad class of distance functions called Bregman divergences. Bregman divergences include not only the traditional (squared) Euclidean distance but also various divergence measures based on entropic functions. Accordingly, Bregman Voronoi diagrams allow to define information-theoretic Voronoi diagrams in statistical parametric spaces based on the relative entropy of distributions. We define several types of Bregman diagrams, establish correspondences between those diagrams (using the Legendre transformation), and show how to compute them efficiently. We also introduce extensions of these diagrams, e.g. k-order and k-bag Bregman Voronoi diagrams, and introduce Bregman triangulations of a set of points and their connexion with Bregman Voronoi diagrams. We show that these triangulations capture many of the properties of the celebrated Delaunay triangulation. Finally, we give some applications of Bregman Voronoi diagrams which are of interest in the context of computational geometry and machine learning.",2007-09-14T01:31:17Z,http://arxiv.org/pdf/0709.2196v1,2024-04-28,
0709.2831v2,"Triangulating the Real Projective Plane","We consider the problem of computing a triangulation of the real projective plane P2, given a finite point set S={p1, p2,..., pn} as input. We prove that a triangulation of P2 always exists if at least six points in S are in general position, i.e., no three of them are collinear. We also design an algorithm for triangulating P2 if this necessary condition holds. As far as we know, this is the first computational result on the real projective plane.",2007-09-18T15:36:38Z,http://arxiv.org/pdf/0709.2831v2,2024-04-28,
